\documentclass{article}
%Ciao!! questi sono gli appunti di Statistical Modelling For Stochastic Processes. Ho creato il pacchetto "pacco" che contiene istruzioni e pacchetti necessari per lo stile e i colori.
\usepackage{pacco}

\begin{document}
	\title{Statistical modeling for stochastic processes\\ spring 2024}
	\author{SDS}
	\date{}
	\maketitle
	\tableofcontents
	\section{Markov Chains}
	\subsection{Overview}
	Let ($\Omega, \mathscr{F}, \mathbb{P}$) be a probability space. Then, a \enf{stochastic process} is a collection 
	\[\{X(t,\omega), \; t\in T\}\]
	of random variables
	\[X:T\times\mathscr{F} \longrightarrow S\]
	where:
	\begin{itemize}
		\item $T$ is the \sott{index set} (typically interpreted as time); 
		\begin{example}
			$T:= \mathbb{Z}_+=\{0,1,2,\ldots\}$: $T$ is a \textit{discrete time} ($\{X_n, \; n\in\mathbb{Z}_+\}$).
		\end{example}
		\begin{example}
			$T:=\mathbb{R_+}$:$T$ is a \textit{continuous time}.
		\end{example}
		\item $S$ is the \sott{state space}. $S$ can be countable: we can take $S \subseteq \mathbb{Z}$ and denote the elements of $S$ as $i, j, k, \ldots$ or $i_1, i_2, i_3, \ldots$.
	\end{itemize}
	If $X_n=i$ we say that $X$ is in state $i$ or that $X$ visits $i$ at time $n$. Typically we drop the argument $\omega$ in $X(t,\omega)$ and just write $X(t)$. $X(\cdot,\omega)$ for a fixed $\omega$ is a function of $t$ and is called \sott{trajectory} of $X$.
	
	More generally one could have:
	\begin{itemize}
		\item $T \subset \mathbb{R}^d$: \textbf{random fields} (e.g. geographical coordinates: $X$ is a pair of latitude and longitude data and the state is the height data $\rightarrow T \subset \mathbb{R}^2$);
		\item $S \subset \mathbb{R}$;
		\item $S \subset \mathbb{R}^d$: \textbf{multivariate processes};
	\end{itemize}
	
	\begin{definition}
		A \enf{Markov Chain} (MC) is a discrete-time stochastic process $\{X_n,n\in\mathbb{Z}_+\}$ taking values in a countable space $S$, such that:
		\begin{align}\label{markprop}
			&\prob(X_n=i_n|X_0=i_0,\ldots,X_{n-1}=i_{n-1})\\
			=&\prob(X_n=i_n|X_{n-1}=i_{n-1})\qquad\forall n\,\geqslant1,\;\forall\,i_0,\ldots,i_n \in S.
		\end{align}
		This property is called \enf{Markov property.}
	\end{definition}
	Markov property defines a very broad class of Markov process es. Given the current state $X_{n-1}$, the next $X_n$ does not depend on the state previous to $n-1$. We could express these concepts in terms of $\sigma$-algebras and filtrations: if
	\[
	\mathscr{F}^x_n=\sigma(x_k, k\leqslant n)
	\]
	and 
	\[
	\{\mathscr{F}^x_n,n\in \mathbb{Z}_+\}
	\]
	is the natural filtration, the previous reads:
	\[
	\prob(X_n=i_n|\mathscr{F}^x_{n-1})=\prob(X_n=i_n|X_{n-1}).
	\]
	$\mathscr{F}^x_{n-1}$ renders useless the information previous to $n-1$.\bigskip\\
	From what we studied in our earlier courses, the main assumption in classical inference was the fat that the samples were i.i.d. But at a certain point we need a more sophisticated model that gives up this assumption. If the data are not i.i.d. we need to make a choice regarding the dependence between the data. Markov processes are highly studied because they offer a way of mathematically studying the dependence between samples (another common way is recurring to \textit{Bayesian inference}). 
	\begin{remark}
		The Markoviality is one way of departing from i.i.d. assumptions and all results that follow come from this property.
	\end{remark}
	The dyamics of $X$ are specified therefore by the right hand side of the Markov Property equation \ref{markprop}
	\[p_{ij}=\prob(X_{n+1}=j|X_n=i)\]
	which is called \enf{transition probability}.\\
	We will assume that these are \sott{temporary homogeneous}, i.e. they do not depend on time. So we can denote
	\[p_{ij}=\prob(X_{n+1}=j|X_n=i) \]
	for any time $t$. We call $j$ the \textit{arrival state} ad $i$ the \textit{starting state}.
	\begin{remark}
		\sott{Spatial homogeneity} is a much stronger assumption, not made here in general. In this case, $p_{ij}$ only depends on $j$: $p_{ij}=p_{0, j-1}$. For example, if we analyzed the probability of moving between the tiles of a grid, spatial homogeneity would imply that moving from one tile to another is the same regardless to the starting tile.
	\end{remark}
	
	We tipically collect the $p_{ij}$ in a \enf{transition matrix}:
	\[P=(p_{ij}) \; i,j\in S\]
	\begin{example}
		For $S=\{0,1,\ldots,k\}$ we have:\\
		\[\begin{bmatrix} 
			p_{00} & p_{01} & \dots & p_{0k} \\
			\vdots & \ddots&  & \vdots \\
			\vdots &  & \ddots & \vdots \\
			p_{k0} & p_{k1} & \dots & p_{kk}
		\end{bmatrix}\]
	\end{example}
	P is a \sott{stochastic matrix} since:
	\begin{itemize}
		\item $p_{ij}\geqslant 0\qquad \forall\; i,j \in S$;
		\item $\forall \;i \in S: \qquad \sum_{j\in S}p_{ij}=1$.
	\end{itemize}
	Now $i$ is then the conditional distribution of $X_{n+1}$ given $X_n=i$.\\
	We typically use a graph to represent a MC.
	\begin{example}
		\[
		P=\begin{bmatrix} 
			p_{00} & p_{01} & 0 \\
			p_{10} & 0 & p_{12} & \\
			0 & p_{21} & 0
		\end{bmatrix}
		\]
		
		\begin{figure}[H]
			\centering
			
			\begin{tikzpicture}[every label/.style={align=left}]
	    \begin{tikzpicture}[->,>=stealth',shorten >=2pt, line width=0.5pt, node distance=2cm]
        \node [circle, draw] (zero) {0};
        \node [circle, draw] (one) [right of=zero] {1};
        \node [circle, draw] (two) [right of=one]{2};
        \path (zero) edge [bend left] node [above] {$p_{01}$} (one);
        \path (one) edge [bend left] node [above] {$p_{12}$} (two);
        \path (zero) edge [loop left] node [left] {$p_{00}$} (zero);
        \path (two) edge [bend left] node [below] {$p_{21}$} (one);
        \path (one) edge [bend left] node [below] {$p_{10}$} (zero);
    \end{tikzpicture}
			\end{tikzpicture}
			
			\label{sonoebreo}
		\end{figure}
	\end{example}
	Is the transition matrix enough to derive everything about the Markov process? In other words, does the transition matrix \sott{fully characterize} the distribution and the features of the process? The answer is yes but only if we include the \enf{initial distribution} of the chain.
	
	\begin{definition}
		Define the \enf{initial distribution} of the chain
		\[
		\lambda_i=\prob(X_0=i)\qquad i\in S
		\]
		as the law of the starting state. 
	\end{definition}
	If \[\lambda_i=\delta_{ij}\footnote{Kronecker's delta.} =
	\begin{cases}
		1,& \text{if } i=j\\
		0,              & \text{otherwise}
	\end{cases}\]
	then $X_0=j$ almost surely.
	
	\begin{proposition}
		Let $X$  be a MC on $S$ countable with initial distribution $\lambda=i \in S$ and transition matrix P. Then $\lambda$ and $P$ jointly fully charcterize the law of the chain. 
	\end{proposition}
	\begin{proof2}
		What do we need to prove? We need to examine the joint distribution of a Markov Chain, which is an infinite sequence of random variables that are not i.i.d.: we can't therefore write the joint distribution as the product of the single distributions.
		
		We need to show that $(\lambda, P$) allows us to compute all joint distributions for the chain, i.e. $\forall$ choices of $0\leqslant j_1 < j_2 < \ldots < j_k$ the law $\prob(X_{j_1}=i_{j_1}, \ldots, X_{j_k}=i_{j_k})$\footnote{This is called a \sott{projection}: we are projecting an infinite sequence on a finite subset.}, which has $k$ states, can be computed with $(\lambda, P$) only. We can use the previous as the marginal of the joint distribution for times $0, \ldots, j_k$:
		\[
		\underbrace{0,1,\dots,j_{1-1}}_{\mathclap{\text{marginalize}}},\textcolor{RedViolet}{j_1},\underbrace{j_{1+1},\ldots}_{\mathclap{\text{marginalize}}},\textcolor{RedViolet}{j_2},\underbrace{\ldots}_{\mathclap{\text{marginalize}}},\textcolor{RedViolet}{j_k}
		\]
		i.e. it is
		\[
		\sum_{\underbrace{i_h\in S | h \neq j_1,\ldots,j_k}_\text{indices of times not chosen: $h \in \{o, \ldots, j_k\}$}} \prob(\underbrace{X_0=i_0, X_1=i_1,\ldots,X_{j_1}=i_{j_k}}_\text{$j_k + 1$ states})
		\]
		So it is enough to find $\prob(X_0=i_0,\ldots,X_n=i_n)$, but by the chain rule this equals to 
		\[\prob(X_0=i_0,\ldots,X_{n-1}=i_{n-1}) \cdot \prob(X_n=i_n|X_0,\ldots,X_{n-1})\]
		and thanks to Markov Property this is equal to
		\begin{align*}
			&\prob(X_0=i_0,\ldots,X_{n-1}=i_{n-1}) \cdot p_{i_{n-1},i_n}\\
			=&\ldots=\underbrace{\prob(X_0=i_0)}_{\lambda_{i_0}}\cdot       p_{i_0,i_1}\cdot\ldots\cdot p_{i_{n-1},i_n}
		\end{align*}
		So we only need $\lambda_{i_0}$ and the $P$ matrix.
	\end{proof2}
	
	We can use a compact notation (took from Norris textbook)
	\[X \sim \text{Markov}(\lambda,P).\]
	
	\begin{definition}
		We define the \enf{$k$-step transition probabilities} as
		\begin{align*}
			&p_{ij}^{(k)} := \prob(X_{n+k}=j|X_n=i), \qquad k \in \mathbb{N}\\
			&p_{ij}^{(0)} := \delta_{ij} = \begin{cases}
				1 & i=j\\
				0 & \text{else}
			\end{cases}
		\end{align*}
	\end{definition}
	
	\begin{proposition}{\enf{Chapman-Kolmogorov equations:}}
		for all $e$, $m \in \mathbb{Z}_+$
		\begin{equation}\label{chapkolm}
			p_{ij}^{(e+m}=\sum_{k<j}p_{ik}^{(e)}\cdot p_{kj}^{(m)}
		\end{equation}
	\end{proposition}
	The proof is left as exercise (use graphical intuition below: marginalize out unwanted states and use Markov property). 
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[-,>=stealth', line width=0.5pt, node distance=0.5cm]
			\node [circle, draw] (zero) {};
			\node [circle, draw] (one) [below of=zero] {};
			\node [circle, draw,circle,fill=black, label=left:$i$] (two) [below of=one]{};
			\node [circle, draw] (three) [below of=two]{};
			\node [circle, draw] (four) [below of=three]{};
			
			\node [circle, draw,circle,fill=black] (a) [right=1cm of zero]{};
			\node [circle, draw,circle,fill=black] (b) [below of=a] {};
			\node [circle, draw,circle,fill=black] (c) [below of=b]{};
			\node [circle, draw,circle,fill=black] (d) [below of=c]{};
			\node [circle, draw,circle,fill=black] (e) [below of=d]{};
			
			\node [circle, draw] (f) [right=1cm of a]{};
			\node [circle, draw] (g) [below of=f] {};
			\node [circle, draw] (h) [below of=g]{};
			\node [circle, draw] (i) [below of=h]{};
			\node [circle, draw, label=right:$j$,circle,fill=black] (j) [below of=i]{};
			
			\path (two) edge node [below] {} (a);
			\path (two) edge node [below] {} (b);
			\path (two) edge node [below] {} (c);
			\path (two) edge node [below] {} (d);
			\path (two) edge node [below] {} (e);
			
			\path (a) edge node [below] {} (j);
			\path (b) edge node [below] {} (j);
			\path (c) edge node [below] {} (j);
			\path (d) edge node [below] {} (j);
			\path (e) edge node [below] {} (j);
		\end{tikzpicture}
		\label{mc graph} %porcoddio sono un cazzo di dragooOOO
	\end{figure}
	
	If you take the transition matrix to the power of $k$, $P^k$ is still a stochastic matrix and the entries are $p_{ij}^(k)$.
	\begin{example}
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[->,>=stealth',shorten >=2pt, line width=0.5pt, node distance=2cm]
				\node [circle, draw] (zero) {0};
				\node [circle, draw] (one) [right of=zero] {1};
				\path (zero) edge [bend left] node [above] {$\alpha$} (one);
				\path (zero) edge [loop left] node [left] {$1-\alpha$} (zero);
				\path (one) edge [loop right] node [right] {$1-\beta$} (zero);
				\path (one) edge [bend left] node [below] {$\beta$} (zero);
			\end{tikzpicture}
			\label{sadasdasf}
		\end{figure}
		\[
		P^2=P\cdot P=\ldots=\begin{bmatrix}
			p_{00}^{(2)} & p_{01}^{(2)}\\
			p_{10}^{(2)} & p_{11}^{(2)}
		\end{bmatrix}
		\]
		In this case, the intermediate states don't matter. For instance we have $p_{00}^{(2)}=p_{00}p_{00}+p_{01}p_{10}$
	\end{example}
	\subsection{Notable Markov processes}
	\subsubsection*{Random Walk}
	Given $X_0$, the simple random walk is defined as
	\[X_n=X_{n-1}+Y_n\]
	$Y_n$s are i.i.d.:
	\[
	Y_n=\begin{cases}
		1 &\text{with probability}\; p\\
		-1 &\text{with probability}\; 1-p
	\end{cases}
	\]
	\[
	\begin{tikzpicture}[scale=0.7]
		\begin{axis}[
			xlabel=0,
			every axis x label/.style={
				at={(ticklabel* cs:-0.01)},
				anchor=east,
			},
			ylabel=\empty,
			xmin=0, xmax=52,
			ymin=0, ymax=100,
			axis y line=left,
			y axis line style={opacity=100},
			ytick=\empty,
			xtick=\empty,
			xtick style={draw=black}, 
			axis x line*=bottom
			]
			\addplot[mark=x,RedViolet] plot coordinates {
				(0,50)
				(2,40)
				(4,50)
				(6,40)
				(8,30)
				(10,20)
				(12,30)
				(14,40)
				(16,50)
				(18,40)
				(20,50)
				(22,40)
				(24,50)
				(26,60)
				(28,70)
				(30,80)
				(32,70)
				(34,60)
				(36,70)
				(38,80)
				(40,90)
				(42,80)
				(44,70)
				(46,60)
				(48,50)
				(50,60)
			};
		\end{axis}
	\end{tikzpicture}
	\]
	if $p=\frac{1}{2}$ the the random walk is \sott{symmetric}.\\
	Random walks have application, for instance, in:
	\begin{itemize}
		\item approximation of Brownian motion and other diffusion processes useful in:
		\begin{itemize}
			\item physics;
			\item math finance;
			\item math biology;
		\end{itemize}
		\item random explorations of space:
		\begin{itemize}
			\item Monte-Carlo integration (Monte-Carlo Markov Chain methods);
			\item stochastic optimization
			\item integral approximation
		\end{itemize}
	\end{itemize}
	Some possible extensions are:
	\begin{itemize}
		\item changing the state space dimension: imagine a symmetric random walk in $\mathbb{Z}^d$, $i,j \in \mathbb{Z}^d$ with $i=(i_1,\ldots,i_d)$:
		\[
		p_{ij}=\begin{cases}
			\frac{1}{2d} & \text{if}\quad \sum_{k=1}^q|i_k-j_k|=1\\
			0 & \text{else;}
		\end{cases}
		\]
		\item change law of $Y_n:\prob(Y_n=1)=a_i, \; \in \mathbb{Z}$ (the case of the distribution being heavy tailed is particularly interesting);
		\item change the topology of $S$:
		\begin{itemize}
			\item Random walk on manifold sphere, on torus...;
			\item Random walk on graphs; for example choose uniformly with probability $q$ a node in the graph (Google PageRank Algorithm).
		\end{itemize}
	\end{itemize}
	\subsubsection*{Birth-and-death chains}
	We define the transition probabilities as
	\[
	p_{ij}=\begin{cases}
		p_i & j=i+1\\
		1-p_i & j=(i-1)^+\\
		0 &\text{else}
	\end{cases}
	\]
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[->,>=stealth',shorten >=2pt, line width=0.5pt, node distance=2cm]
			\node [circle, draw] (zero) {0};
			\node [circle, draw] (one) [right of=zero] {1};
			\node [circle, draw] (two) [right of= one] {2};
			\node [circle, draw] (dots) [right of=two] {$\ldots$};
			\path (zero) edge [bend left] node [above] {$p_0$} (one);
			\path (zero) edge [loop left] node [left] {$1-p_0$} (zero);
			\path (one) edge [bend left] node [below] {$1-p_1$} (zero);
			\path (one) edge [bend left] node [above] {$p_1$} (two);
			\path (two) edge [bend left] node [above] {$p_2$} (dots);
			\path (dots) edge [bend left] node [below] {} (two);
			\path (two) edge [bend left] node [below] {$1-p_2$} (one);
		\end{tikzpicture}
		\label{ahah}
	\end{figure}
	\begin{itemize}
		\item Time-varying size of a population (animal, viruses, numbero of request to a CPU...)
		\item size of a queue at a server
		\item dimension of a multivariate distribution used for estimation
		\item dimension $k$ of a mixture model
		\[
		\text{i.e.} \qquad \sum_{i=k}^k w_i f_i
		\]
		with $\sum_{w_i}=1$ and $f_i$ being a density.
	\end{itemize}
	\subsubsection*{Pòlya urns}
	An urn contains $W_0$ white balls and $B_0$ black a ball. We draw a ball, check its colour, put it back and add to the urn another ball of the same colour: this behaviour is called \sott{reinforcement} and it is the opposite of drawing without placement. Let $W_n$ be the number of white balls after n draws (steps):
	\[
	\prob(W_{n+1}=j|W_0,\ldots,W_n)=\begin{cases}
		\frac{W_n}{Wn+Bn}\rightarrow\text{"draw white"} &j=W_n+1\rightarrow\text{"add white"}\\
		\frac{B_n}{Wn+Bn}\rightarrow\text{"draw black"} &j=W_n\rightarrow\text{"same number of whites"}\\
		0 & \text{else}
	\end{cases}
	\]
	\begin{align*}
		W_n+&B_n=W_0+B_{0+n}\\
		&B_n=W_0+B_{0+n}-W_n
	\end{align*}
	The probability only depends on $W_n$ and the previous $W$s are irrelevant, so the process is a Markov Chain
	These are applied in Bayesian inference, since they generate \sott{exchangeable sequences} ($(X_1, X_2)\stackrel{d}{=}(X_2,X_1)$). A classical result which is often used is:
	\[
	\frac{W_n}{W_n+B_n}\xrightarrow{a.s}\theta\sim Beta(W_0, B_0)
	\]
	\subsubsection*{Branching Processes}
	Branching processes are a simple model for evolving populations with reproduction. The basic formulation (Galton-Watson) is:
	\begin{itemize}
		\item individuals live on period;
		\item individuals generate clones independently (the original motivation was the survival of family names);
		\item the next generation is formed by the offspring of the previous generation.
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[-,>=stealth', line width=0.5pt, node distance=0.5cm]
			
			\node [circle] (zero) {0};
			\node [circle, draw,circle,fill=black] (a) [below=0.4cm of zero]{};
			
			\node [circle] (one)[right=1cm of zero] {1};
			\node [circle, draw,circle,fill=black] (b) [below=0.4cm of one]{};
			\node [circle, draw,circle,fill=black] (c) [below of=b]{};
			
			\node [circle] (two)[right=1cm of one] {2};
			\node [circle, draw,circle,fill=black] (d) [below=0.4cm of two]{};
			\node [circle, draw,circle,fill=black] (e) [below of=d]{};
			\node [circle, draw,circle,fill=black] (f) [below of=e]{};
			
			\node [circle] (three)[right=1cm of two] {3};
			\node [circle, draw,circle,fill=black] (g) [below=0.4cm of three]{};
			\node [circle, draw,circle,fill=black] (h) [below of=g]{};
			
			\node [circle] (four)[right=1cm of three] {4};
			\node [circle, draw,circle,fill=black] (i) [below=0.4cm of four]{};
			\node [circle, draw,circle,fill=black] (j) [below of=i]{};
			\node [circle, draw,circle,fill=black] (k) [below of=j]{};
			\node [circle, draw,circle,fill=black] (l) [below of=k]{};
			
			\path (a) edge node [below] {} (b);
			\path (a) edge node [below] {} (c);
			
			\path (c) edge node [below] {} (d);
			\path (c) edge node [below] {} (e);
			\path (c) edge node [below] {} (f);
			
			\path (d) edge node [below] {} (g);
			\path (d) edge node [below] {} (h);
			
			\path (h) edge node [below] {} (i);
			\path (h) edge node [below] {} (j);
			\path (h) edge node [below] {} (k);
			\path (h) edge node [below] {} (l);
		\end{tikzpicture}
		\label{generatons}
	\end{figure}
	Useful applications:
	\begin{itemize}
		\item epidemiology (virus contagion);
		\item population genetics;
		\item physics (random number of neutron produced at collisions).
	\end{itemize}
	Interesting extensions:
	\begin{itemize}
		\item extend to $k$ types;
		\item add immigration.
	\end{itemize}
	Let $X_n$ be the number of individuals in generation $n$ and $Y_i$ be the number of clones/offspring of individual $i$. We have $Y_i \stackrel{i.i.d.}{\sim} p_Y$ on $\mathbb{Z}_+$.
	\[
	X_n=Y_1+Y_2+\ldots+Y_{X_{n-1}}=\sum_{i=1}^{X_{n-1}}Y_i\independent X_{n-2},X_{n-3},\ldots
	\]
	So the process is a Markov chain. 
	
	\begin{proposition}{\enf{Branching Property:}}
		Denote by $X^{(i)}=\{{X_n}, n \in \mathbb{Z}_+|X_0=1\}$, characterizing the chain by a fixed starting point, the branching process started at $i$. Let $\Tilde{X},\hat{X}$ be independent copies of $X$. Then
		\begin{equation}
			X^{(i+j)}\stackrel{d}{=}\Tilde{X}^{(i)}+\hat{X}^{(j)}
		\end{equation}
		This is called \enf{branching property}.
	\end{proposition}
	\begin{proof2}
		Let 
		\[
		g_{z}(s)=\ev{e^{sZ}}
		\]
		be the moment generating function of $Z$. We are interested in the moment generating function of $X_1|X_0=i$.
		\begin{align*}
			g_{X_1^{(i)}}=g_{\sum_{j=1}^{i}Y_j} =(g_Y)^i
		\end{align*} 
		This moment generating function characterizes the one step transition probability of getting to $X_1$, that is the population size at $n=1$, from the state $X_0=i$. Now
		\begin{align*}
			&g_{\{\Tilde{X}_1^{(i)}+\hat{X}_1^{(j)}\}}=g_{\Tilde{X}_1^{(i)}}\cdot g_{\hat{X}_1^{(j)}}=\\
			=(&g_Y)^i\cdot(g_Y)^j=(g_Y)^{i+j}=g_{X_1^{(i+j)}}
		\end{align*}
		We have now proved for one step that, since $\Tilde{X}_1^{(i)}+\hat{X}_1^{(j)}$ and $X_1^{(i+j)}$ share the same moment generating function, they have the same distribution. To extend this result to every step we can use Markov property, so that $X^{(i+j)}_1\stackrel{d}{=}\Tilde{X}^{(i)}_1+\hat{X}^{(j)}_1$ characterizes the law of the entire chain.
	\end{proof2}
	An interesting extension of the model consists in the behaviour of a population during extinction. This ultimately depends on:
	\[
	\ev{Y}=\begin{cases}
		<1 & X\; \text{is subcritical}\\
		=1 & X\; \text{is critical}\\
		>1 & X\; \text{is supercritical}\\
	\end{cases}
	\]
	An interesting case is given by introducing immigration to the subcritical case. This is called \enf{Galton-Watson} branching process.
	\subsubsection*{Wright-Fisher models}
	These model the time-varying frequency of 2 types (in general k or $\infty$ many types) in an evolving population of constant size. The original motivation was modelling the allelic type frequency of a gene locus. These models have the following characteristics:
	\begin{itemize}
		\item individuals live 1 period;
		\item individuals can be of type 0 or 1;
		\item population size is $N$ $\forall n \geqslant \mathbb{Z}_+$;
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[-,>=stealth', line width=0.5pt, node distance=0.5cm]
			\node [circle] (zero) {0};
			\node [circle, draw, shade, shading=ball, circle, ball color=RedViolet!80!white] (a) [below=0.4cm of zero]{};
			\node [circle, draw, shade, shading=ball, circle, ball color=RedViolet!80!white] (b) [below of= a]{};
			\node [circle, draw, shade, shading=ball, circle, ball color=RedViolet!80!white] (c) [below of= b]{};
			\node [circle, draw, shade,shading=ball,circle,ball color=Dandelion!80!white] (d) [below of= c]{};
			
			\node [circle] (one)[right=1cm of zero] {1};
			\node [circle, draw, shade, shading=ball, circle, ball color=RedViolet!80!white] (e) [below=0.4cm of one]{};
			\node [circle, draw, shade, shading=ball, circle, ball color=RedViolet!80!white] (f) [below of= e]{};
			\node [circle, draw, shade,shading=ball,circle,ball color=Dandelion!80!white] (g) [below of= f]{};
			\node [circle, draw, shade,shading=ball,circle,ball color=Dandelion!80!white] (h) [below of= g]{};
			
			\node [circle] (two)[right=1cm of one] {2};
			\node [circle, draw, shade, shading=ball, circle, ball color=RedViolet!80!white] (i) [below=0.4cm of two]{};
			\node [circle, draw, shade, shading=ball, circle, ball color=RedViolet!80!white] (j) [below of= i]{};
			\node [circle, draw, shade, shading=ball, circle, ball color=RedViolet!80!white] (k) [below of= j]{};
			\node [circle, draw, shade,shading=ball,circle,ball color=Dandelion!80!white] (l) [below of= k]{};        
			\node [circle] (three)[right=1cm of two] {3};
			\node [circle, draw, shade, shading=ball, circle, ball color=RedViolet!80!white] (m) [below=0.4cm of three]{};
			\node [circle, draw, shade,shading=ball,circle,ball color=Dandelion!80!white] (n) [below of= m]{};
			\node [circle, draw, shade,shading=ball,circle,ball color=Dandelion!80!white] (o) [below of= n]{};
			\node [circle, draw, shade,shading=ball,circle,ball color=Dandelion!80!white] (p) [below of= o]{};
			
			\path (a) edge node [below] {} (e);
			\path (a) edge node [below] {} (f);
			\path (d) edge node [below] {} (g); 
			\path (d) edge node [below] {} (h); 
			
			\path (e) edge node [below] {} (i); 
			\path (e) edge node [below] {} (j); 
			\path (e) edge node [below] {} (k); 
			\path (h) edge node [below] {} (l); 
			
			\path (j) edge node [below] {} (m); 
			\path (l) edge node [below] {} (n); 
			\path (l) edge node [below] {} (o); 
			\path (l) edge node [below] {} (p); 
		\end{tikzpicture}
		\label{wright models}
	\end{figure}
	We have now constrained the population size, so each parent can generate from 0 to $N$ offspring and therefore birth events are no longer independent. A useful approach to model this behaviour is to imagine that during next generation individuals choose their parent at random from the previous generation, with the condition that it must be of the same type.\\
	Let $X_n$ be the number of type-0 individuals at time $n$. The individuals at generation $n$ are:
	\begin{itemize}
		\item of type-0 with probability $\frac{X_n}{N}$;
		\item of type-1 with probability $1-\frac{X_n}{N}$;
	\end{itemize}
	So every member of a generation chooses its parent by a Bernoulli trial \sott{independently}. Since we have $N$ Bernoulli trial with parameter $p=\frac{X_n}{N}$, we have that:
	\[
	X_n|X_{n-1}\sim Binom(N,\frac{X_{n-1}}{N}).
	\]
	Note that 
	\begin{align*}
		\text{if}\quad &X_{n-1}=0 \implies X_n=0\qquad\text{a.s}\\
		\text{if}\quad &X_{n-1}=N \implies X_n=N\qquad\text{a.s}.
	\end{align*}
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
				xlabel=0,
				every axis x label/.style={
					at={(ticklabel* cs:-0.01)},
					anchor=east,
				},
				ylabel=\empty,
				xmin=0, xmax=52,
				ymin=0, ymax=100,
				axis y line=left,
				y axis line style={opacity=0},
				ytick=\empty,
				xtick={46}, 
				xtick style={draw=black}, 
				xticklabel={fixation event}, 
				axis x line*=bottom
				]
				\addplot[mark=x,RedViolet] plot coordinates {
					(0,50)
					(2,40)
					(4,50)
					(6,40)
					(8,30)
					(10,20)
					(12,30)
					(14,40)
					(16,50)
					(18,40)
					(20,50)
					(22,40)
					(24,30)
					(26,20)
					(28,30)
					(30,40)
					(32,30)
					(34,20)
					(36,10)
					(38,20)
					(40,10)
					(42,20)
					(44,10)
					(46,0)
					(48,0)
					(50,0)
				};
				%\node[below,RedViolet,align=left] at (46,-0.08) {\footnotesize fixation event};
			\end{axis}
			\begin{axis}[
				xlabel=$N$,
				every axis x label/.style={
					at={(ticklabel* cs:-0.01)},
					anchor=east,
				},
				ylabel=\empty,
				xmin=0, xmax=30,
				ymin=0, ymax=100,
				axis y line=left,
				y axis line style={opacity=0},
				ytick=\empty,
				xtick=\empty,
				axis x line*=top]
			\end{axis}
		\end{tikzpicture}
		\label{camminata}
	\end{figure}
	Some useful extension may include:
	\begin{itemize}
		\item mutations;
		\item $k\leqslant\infty$ types;
		\item uncountably many types.
	\end{itemize}
	For example, set mutations such that they only occur to the other type:
	\[
	\alpha=\prob(1\rightarrow0)\qquad\beta=\prob(0\rightarrow1).
	\]
	Mutations take place before reproduction (that is, before the binomial resampling method). We have:
	\begin{itemize}
		\item a $Bernoulli(\beta)$ trial on each of $i$ individual of type 0. On average:
		\begin{itemize}
			\item $i\beta$ individuals become of type 1;
			\item $i(1-\beta)$ individuals remain of type 0;
		\end{itemize}
		\item a $Bernoulli(\alpha)$ trial on each of $N-i$ individual of type 1. On average:
		\begin{itemize}
			\item $(N-i)\alpha$ individuals become of type 0;
			\item $(N-i)(1-\alpha)$ individuals remain of type 1.
		\end{itemize}
	\end{itemize}
	After mutation, the expected proportions are:
	\begin{alignat*}{3}
		&\text{type 0:}&&\qquad \Tilde{p_i}&&=\frac{1}{N}(i(1-\beta)+(N-i)\alpha)=\\
		& && &&=\frac{i}{N}(1-\beta)+(N-i)\alpha=\\
		& && &&=p_i(1-\beta)+(1-p_i)\alpha.\\
		\\
		&\text{type 1:}\qquad &&1-\tilde{p_i}&&=p_i\beta+(1-p_i)(1-\alpha).
	\end{alignat*}
	We can interpret $\tilde{p}_i$ as the \sott{percentage} of type 0 individuals (in expectation) before resampling. The transition probability has become:
	\[=0
	p_{ij}=\prob(X_{n+1}=j|X_n=i)=\binom{N}{j}\Tilde{p}_i^j(i-\Tilde{p}_i)^{N-j}
	\]
	Now, if $i=0\implies p_i=\frac{i}{N}=0\implies\Tilde{p}_i=\alpha\implies\prob(X_{n+1}>0|X_n=0)=1-(1-\alpha)^N>0$.\\
	If $X_n$ is at the boundary $X_n \in \{o,N\}$, now there is positive probability of going back to the interior of the space state.
	\subsubsection*{Transformation of Branching Processes}
	Intuitively, there are two branching processes embedded in the Wright-Fisher trajectory. We can formalize this connection:
	
	\begin{proposition}
		Let $X$ and $W$ be two independent branching processes with the same Poisson offspring distribution. Then, conditional on the total population size being constant and equal to $N$, $X$ and $W=N-X$ are Wright-Fisher chains.
	\end{proposition}
	\begin{proof2}
		\begin{align*}
			X_n&=\text{n° of type 0 individuals}\\
			W_n&=\text{n° of type 1 individuals}\\
		\end{align*}
		With per capita offspring
		\[
		Y\sim Pois(\lambda_j) \qquad j=0,1
		\]
		where $j$ is specific to the first and to the second branching process. Now, since the sum of Poisson variables is still a Poisson variable,
		\begin{align*}
			X_n &= \sum_{i=1}^{X_{n-1}}Y_i \sim Pois(X_{n-1}\lambda_0)\\
			W_n &= \sum_{i=1}^{W_{n-1}}Y_i \sim Pois(W_{n-1}\lambda_1)\\
		\end{align*}
		We know, in general, that
		\[
		Z_j\sim Pois(\gamma_j),\;j=0,1 \implies Z_0|Z_0+Z_1=N\sim Binom\biggl(N,\frac{\gamma_0}{\gamma_0+\gamma_1}\biggr)
		\]
		Let's apply this property:
		\[
		X_n|X_n+W_n=N,X_{n-1},W_{n-1}\sim Binom(N,q_{n-1})
		\]
		What we expect is for the parameter $q_{n-1}$ to only depend on state $n-1$. We know that:
		\[
		q_{n-1}=\frac{X_{n-1}\lambda_0}{X_{n-1}\lambda_0+\underbrace{W_{n-1}}_{\mathclap{N-X_{n-1}\;\text{because we are conditioning on the whole population}}}\lambda_1}=\frac{X_{n-1}\lambda_0}{X_{n-1}(\lambda_0-\lambda_1)+N\lambda_1}
		\]
		But we said that the offspring distribution is the \textit{same} Poisson distribution: so
		\begin{align*}
			&\lambda_0=\lambda_1\implies q_{n-1}=\frac{X_{n-1}}{N}\\
			\implies &X_n|X_{N-1},X_n+W_n=N \sim Binom\biggl(N,\frac{X_{n-1}}{N}\biggr)
		\end{align*}
		which is the transition probability of the Wright-Fisher chains, which characterizes the law of the entire chain. We are therefore claiming that the random variables whose state space is the $\infty$ trajectories of the chain is governed by one law.
	\end{proof2}
	\subsection{Review of Markov Property}
	The property we gave that the beginning can be seen as a regeneration property:
	\[
	X\sim Markov(\lambda,P) \implies \{X_{n+k},k\geqslant 0|X_n=i\}\sim Markov(\delta_i,P).
	\] 
	Here $n$ is fixed and from $n$ onward the chain starts afresh with the same properties. What if $n$ is a random variable $T$, indicating a random time?
	
	\begin{definition}{\enf{strong Markov property:}}
		We are interested in establishing the \enf{strong Markov property}:
		\begin{equation}
			\prob(X_{T+1}=j|X_0,\ldots,X_T=i)=\prob(X_{T+1}=j|X_T=i)
		\end{equation}
	\end{definition}
	Not all random times are suitable: let $\{\mathscr{F}\}_{n\geqslant 0}$ be the \sott{natural filtration} generated by $X$, where $\mathscr{F}_n=\sigma(X_u, 0\leqslant u \leqslant n)$ which is interpreted as the \textit{flow of information} generated by the $X$ trajectory up to time $n$.\\ A random variable $T:\Omega\rightarrow\mathbb{N}\cup\{\infty\}$ is a \enf{stopping time} for $X$ if 
	\[
	\{\omega\in\Omega :T(\omega)=n\in\mathscr{F}_n\}
	\]
	i.e. $\{T=n\}$ is $\mathscr{F}_n$-measurable: we can express $\{T=n\}$ in terms of $X_0,X_1,\ldots,X_n$. If we are interested in other relations:
	\[
	\{T=n\}\in\mathscr{F}_n\implies\begin{cases}
		\{T\leqslant n\}=\bigcup_{i \leqslant n}\{T=n\}\in\mathscr{F}_n\\
		\{T\neq n\}=\{T=n\}^c\in\mathscr{F}_n\\
		\ldots
	\end{cases}
	\]
	
	\begin{example}{\enf{First passage time/first visit to $i$}:}
		\[
		T=\inf\{n\geqslant 1: X_n=i\}.
		\]
		Is $T$ a stopping time? The event $\{T=n\}=\{X_0 \neq i, X_1 \neq i, \ldots, X_{n-1}\neq i, X_n=i\}$ by definition belongs to $\mathscr{F}_n$, so it is a stopping time.
	\end{example}
	\begin{example}{\enf{Last exit time from $\mathbf{A \subset S}$}}
		\[
		T=\sup\{n\geqslant 0: X_n\in A\}.
		\]
		Is $T$ a stopping time? The event depends on $\{X_{n+m},m\geqslant 1\}$. $\{T=m\}$ does not belong to $\mathscr{F}_n$ because it is anticipating using future information: $T$ is \sott{not} a stopping time.
	\end{example}
	
	\begin{theorem}{\enf{Strong Markov Property:}}
		Let $X\sim Markov(\lambda,P)$ and $T$ be a stopping time for $X$. Then, conditional on $T<\infty$ and $X_T=i$, 
		\[
		\{X_{T+n},n\geqslant 0\}\sim Markov(\delta_i,P)
		\]
		is \textbf{independent} on the chain before time $T$.
	\end{theorem}
	If $T$ is a stopping time (meaning that it depends on the past only) the the chain \sott{regenerates} from $T$ with the same properties.
	\begin{example}
		Let $T$ be the first visit to $j$ and $T'=T-1$ the time prior to entering j for the 1st time. $\{T'=n\}$ depends on $X_{n+1}$ which implies that $\{T'=n\}\notin\mathscr{F}_n$. Conditional to $X_{T'}=i$ the process is not Markov:
		\[
		\{X_{T'+n},n\geqslant 0\}\not\sim Markov(\delta_i,P)
		\]
		Why? The problem is that if $n=1$ then the first step of the chain is \sott{deterministic}, since we are imposing it to be $j$ instead of following the transition matrix:
		\[
		\prob(X_{T'+1}=j|X_{T'}=i)=1\neq p_{ij} \qquad\text{since}\quad X_T=X_{T'+1}=j
		\]
		so the chain does not start afresh with the same distribution properties.
	\end{example}
	\begin{example}
		Suppose we observe $X$ only when $X_n\in A\subset S$ (for instance, imagine we have an instrument only capable of measuring above a certain threshold $A$). $T^{(m)}=inf\{n> T^{(m)}:X_n \in A\}$ is the time of the $n$-th visit to $A$ and $Y_m:=X_{T^{(m)}}$.\\
		Assume $\prob(T^{(m)}<\infty)=1 \quad \forall m \geqslant 1$:
		\begin{align*}
			&\prob(Y_{m+1}=i_{m+1}|Y_1=i_1,\ldots,Y_m=i_m)=\\
			=&\prob(X_{T^{(m+1)}}=i_{m+1}|X_{T^{(1)}}=i_1,\ldots,X_{T^{(m)}}=i_m)=\\
			&\text{\small(by strong Markov property)}\\
			=&\prob(X_{T^{(m+1)}}=i_{m+1}|X_{T^{(m)}}=i_m)
		\end{align*}
		which means that $Y$ is a Markov Chain on $A$. If I have a Markov chain only observable on a subset then I can "skip" intermediate steps and still have a Markov chain. This is pretty cool.
	\end{example}
	\subsection{Properties of Markov chains}
	The goal of this section is to find the conditions on $P$ to claim certain properties of Markov Chains, especially concerning long-run behaviour.
	
	\subsubsection{Communication classes}
	\begin{definition}
		A state is \enf{accessible} from $i$ if $p_{ij}^{(n)}>0$ for some $n$. Two states reciprocally accessible are said to \enf{communicate}.
	\end{definition}
	If a state is accessible from another state it means that the chain can go there in a finite number of steps.
	\begin{example}
		\begin{minipage}{0.5\textwidth}
			\begin{figure}[H]
				\centering
				\begin{tikzpicture}[->]
					\foreach \i [count=\j from 1] in {90,18,...,-198} \node[circle,draw] (\j) at (\i:1) {\j};
					\path (1) edge [bend left] node [below] {} (2);
					\path (2) edge [bend left] node [below] {} (3);
					\path (3) edge [bend left] node [below] {} (4);
					\path (4) edge [bend left] node [below] {} (5);
					\path (5) edge [bend left] node [below] {} (1);
				\end{tikzpicture}
			\end{figure}
		\end{minipage} \hfill
		\begin{minipage}{0.45\textwidth}
			In this case all the states communicate, since it is possible to go from one to any other with at most 5$<\infty$ steps. 
		\end{minipage}
	\end{example}
	Communicating states form an \enf{equivalence class}: the relation is reflective, symmetric and transitive. This can be proved using the Chapman-Kolmogorov equation (Eq. \ref{chapkolm}). $S$ can be split in two or more classes of communicating states, thus obtaining a \textbf{macroscopic description} of the chain dynamics.
	\begin{example}\label{irr}
		\[
		\begin{tikzpicture}[node distance=2cm]
			\node[circle,draw](2){$S_2$};
			\node[circle, draw](1)[left of=2]{$S_1$} edge[->] (2);
		\end{tikzpicture}
		\qquad S_1, S_2 \subset S
		\]
		In this case, once the chain leaves $S_1$ it can't go back. $S_2$ can be accessed by $S_1$ but not vice-versa. On the long run we can say that we should focus on $S_2$, since $S_1$ will be left for good sooner or later.
	\end{example}
	
	\subsubsection{Irreducibility
	}
	\begin{definition}
		A Markov chain with a single equivalence class generated by communicating states is said to be \enf{irreducible}.
	\end{definition}
	For instance, in the previous example the chain is not irreducible but once the chain leaves $S_1$ it becomes irreducible in $S_2$.\\
	What about the main Markov processes?
	\begin{itemize}
		\item The simple random walk is \sott{irreducible}: we can always go in every state;
		\item in branching processes, if $X_n=0$ then the process stops and $X_{n+1}=0$. This means that $\{1,2,\ldots\}$ are not accessible from 0 and the chain is therefore \sott{not irreducible};
		\item in Wright-Fisher process with no mutations we face a similar situation: $X_n=0\implies X_{n+1}=0 $ and $X_n=N\implies X_{n+1}=N $: $\{1,\ldots,N-1\}$ are not accessible from $\{0,N\}$ and the chain is therefore \sott{not irreducible}. The introduction of mutations, though, would make it irreducible.
	\end{itemize}
	
	\subsubsection{Periodicity}
	\begin{definition}
		We define the \enf{period} $d(i)$ of $i\in S$ the greatest common divisor of all $n\geqslant 1$ such that $p_{ii}^{(n)}>0$. If $p_{ii}^{(n)}>0$ for all sufficiently large $n$ (or if $d(i)=1$) we say that $i$ is \enf{aperiodic}.
	\end{definition}
	\begin{example}
		\begin{minipage}{0.5\textwidth}
			\begin{figure}[H]
				\centering
				\begin{tikzpicture}[->,node distance=2cm]
					\foreach \i [count=\j from 1] in {90,-30,-150} \node[circle,draw] (\j) at (\i:1) {\j};
					\path (1) edge [bend left] node [right] {1} (2);
					\path (2) edge [bend left] node [below] {1} (3);
					\path (3) edge [bend left] node [left] {1} (1);
				\end{tikzpicture}
			\end{figure}
		\end{minipage} \hfill
		\begin{minipage}{0.45\textwidth}
			$\forall i=1,2,3:$\medskip \\
			$p_{ii}^{(3n)}=1\qquad \forall n+1$\\
			$p_{ii}^{(3n+1)}=0\qquad \forall n\geqslant 1$\\
			$p_{ii}^{(3n+2)}=0\qquad \forall n\geqslant 1$\\
			$d(i)=3 \qquad i\in S$
		\end{minipage}
	\end{example}
	Periodicity is a \enf{class property}: if $i,j$ communicate then they have the same period. This means that the period of the communication class $S_1$ is enough to study the period of a single $i\in S_1$.
	\begin{example}
		The simple random walk is irreducible: $S = \mathbb{Z}$ is a single communicating class. For $i>0$:
		\[
		p_{00}^{(2n)}>0 \qquad p_{00}^{(2n+1)}=0 \qquad n \geqslant 0
		\]
		so the period is 2.
	\end{example}
	
	\begin{proposition}
		If $X$ is irreducible and $p_{ii}>0$ for some $i \in S$ then $X$ is aperiodic.
	\end{proposition}
	\begin{example}
		\begin{minipage}{0.5\textwidth}
			\begin{figure}[H]
				\centering
				\begin{tikzpicture}[->,node distance=2cm]
					\foreach \i [count=\j from 1] in {90,-30,-150} \node[circle,draw] (\j) at (\i:1) {\j};
					\path (1) edge [bend left] node [right] {1} (2);
					\path (2) edge [bend left] node [below] {1/2} (3);
					\path (3) edge [bend left] node [left] {1} (1);
					\path (2) edge [loop below] node [below] {1/2} (2);
				\end{tikzpicture}
			\end{figure}
		\end{minipage} \hfill
		\begin{minipage}{0.45\textwidth}
			The chain is aperiodic: I can get from 3 back again to 3 in 3 steps of in 4,5,6,$\ldots$ if I cycle in 2.
			\[p_{33}^{(n)}>0\qquad n\geqslant 3.\]
			The greatest common denominator is 1: $d(3)=1$
		\end{minipage}
	\end{example}
	We can exploit this property: if $P$ is periodic, define
	\[
	P'=\varepsilon P+(1-\varepsilon)I,\hspace{2cm} \varepsilon\in(0,1)
	\]
	This is called the "\sott{lazy chain}" because the chain is not going to move from its state in $n$ with probability $\varepsilon$ and $P'$ is aperiodic. We will see that this modification essentially does not alter the distributional properties of the chain. 
	\subsubsection{Recurrence}
	Recall now the first visit to $i$
	\[
	T_i=\inf\{n\geqslant 1: X_n=i\}
	\]
	
	\begin{definition}
		A state $i$ is said do be \enf{recurrent} if \[\prob(T_i<\infty|X_0=i)=1\],  or equivalently \[\prob(T_i<\infty \quad\text{for some}\quad n|X_0=i)=1\] which means that the return time is finite almost surely. The state $i$ is otherwise said to be \enf{transient}.
	\end{definition}
	A transient $i$ is such that \[\prob(T_i<\infty|X_0=i)<1\]: it is worth noting that a transient state still has a positive probability of coming back on $\infty$.
	
	\begin{proposition}
		Let $p_{ii}^{(n)}$ be the return probability to $i$ in $n$ steps. Then $i \in S$ is recurrent if and only if \[\sum_{n\geqslant 1} p_{ii}^{(n)}=\infty \] and it is transient otherwise.
	\end{proposition}
	Let $I_n=\mathbbm{1}(X_n=i)$:
	\begin{align*}
		\sum_{n\geqslant 1} p_{ii}^{(n)}&=\sum_{n\geqslant 1} \prob(X_n|X_0=i)\\
		&=\lim_{N \to \infty}\sum_{n=1}^N\ev{I_n|X_0=i}\\
		&=\lim_{N \to \infty}\ev{\underbrace{\sum_{n=1}^NI_n}_{\mathclap{f_N}}|X_0=i}\\
		&=\ev{\sum_{n=1}^{\infty}I_n|X_0=i}\\
		&\text{\footnotesize(by monotone convergence theorem)}
	\end{align*}
	So $i$ is recurrent if in expectation it is visited $\infty$ many times over the whole time horizon. \\
	We can define \[G:=\sum_{n\geqslant 0}p^n\] which is sometimes called \sott{potential matrix}.
	
	\begin{proposition}
		A state $i$ is visited almost surely:
		\begin{itemize}
			\item infinitely often if recurrent;
			\item finitely often if transient
		\end{itemize}
	\end{proposition}
	Intuition: $T_i^{(1)}$ is the first passage time to $i$. If $i$ is recurrent then by definition
	\[
	\prob(T_i^{(1)}<\infty|X_0=i)=1.
	\]
	Since $X_{T_i^{(1)}}=i$, from the strong Markov property we get: \[X'=X_{T_i^{(1)}+n}\sim Markov(\delta_i,P)\]
	so $X_{T_i^{(2)}}$ for $X$ is the first passage time for $X'$, which implies 
	\[T_i^{(1)}\stackrel{d}{=}T_i^{(2)} \implies \prob(T_i^{(2)}<\infty|X_{T_i^{(1)}}=i)=1.\]
	So over an infinite time horizon we have infinitely many visits with probability 1.
	
	Recurrence is a class property, so if a chain is irreducible it is enough to check one state. If we checked that $i$ is recurrent all states that communicate with $i$ will be visited infinitely many times.
	
	\begin{proposition}
		The simple random walk on $\mathbb{Z}$ is recurrent if and only if $p=\frac{1}{2}$, that is if the walk is symmetric. 
	\end{proposition}
	
	\begin{proof2}
		To check that the chain is irreducible, it is enough to check that $i=0$ is irreducible:
		\[p_{00}^{(2n)}>0 \hspace{2cm}  p_{00}^{(2n+1)}=0 \qquad n\geqslant 1\] so it is enough to check the even number of steps:
		\begin{center}
			\begin{tikzpicture}[->]
				\node [circle] (zero) {0};
				\node [circle] (one) [right of=zero] {1};
				\node [circle] (two) [right of=one] {2};
				\node [circle] (mone) [left of=zero] {-1};
				\node [circle] (mtwo) [left of=mone] {-2};
				\path (zero) edge [bend left] node [right] {} (one);
				\path (one) edge [bend left] node [right] {} (two);
				\path (zero) edge [bend right] node [right] {} (mone);
				\path (mone) edge [bend right] node [right] {} (mtwo);
				\path (mtwo) edge [bend right] node [right] {} (mone);
				\path (mone) edge [bend right] node [right] {} (zero);
				\path (two) edge [bend left] node [right] {} (one);
				\path (one) edge [bend left] node [right] {} (zero);
			\end{tikzpicture}
		\end{center}
		I sample right and left steps $n$ times each:
		\[p_{00}^{(2n)}=\binom{2n}{n}p^n(1-p)^n\]
		Verify using Stirling's approximation that
		\[n!\approx \text{const}\times n^{n+\frac{1}{2}}e^{-n}\implies p_{00}^{(2n)}\approx \frac{[4p(1-p)]^n}{\sqrt{n}}\]
		so
		\[
		\sum_{n\geqslant 0}p_{00}^{(n)}\approx  \sum_{n\geqslant 0}\frac{[4p(1-p)]^n}{\sqrt{n}}
		\]
		\[
		\rightarrow\begin{cases}
			<\infty &4p(1-p)<1=p(1-p)<\frac{1}{4}\; \text{which means}\;p\neq \frac{1}{2}\\
			=\infty &p=\frac{1}{2}
		\end{cases}
		\]
		which means that the series is infinite when $p=1/2$.
	\end{proof2}
	What about a random walk on $\Z^d$? 
	\[
	d=2,\; i,j\in\Z^2 \qquad p_{ij}=\begin{cases}
		\frac{1}{4} &\text{if}\quad|i_1-j_1|+|i_2-j_2|\\
		0 &\text{else}.
	\end{cases}
	\]
	\begin{minipage}{0.5\textwidth}
		\begin{tikzpicture}[
			>=stealth', axis/.style={<->},
			point/.style={circle, inner sep=0pt, fill, minimum size=4pt, label=#1},
			scale=.75]
			\draw[axis] (0,-3) -- (0,3) node[above right]{$y$};
			\draw[axis] (-3,0) -- (3,0) node[below right]{$x$};
			\draw[dashed,RedViolet] (0,0) -- (3,3);
			\draw[dashed,RedViolet] (1.5,1.5) -- (3,0);
			\draw[dashed,RedViolet] (0,0) -- (3,-3);
			\draw[dashed,RedViolet] (1.5,-1.5) -- (3,0);
			\coordinate (A) at (1.5,1.5);
			\coordinate (B) at (0,0);
			\draw [|-|]([yshift=0.1cm,xshift=-0.1cm]A) -- ([yshift=0.1cm,xshift=-0.1cm]B)node [black,midway,sloped,above]{$\frac{1}{\sqrt{2}}$};
		\end{tikzpicture}
	\end{minipage} \hfill
	\begin{minipage}{0.45\textwidth}
		The random walk on $\Z^2$ can be seen as 2 independent random walk on the diagonals of the cartesian plane.
	\end{minipage}
	\begin{align*}
		&p_{(0,0),(0,0)}^{(2n+1)}=0\\
		&p_{(0,0),(0,0)}^{(2n)}=\Biggl[\underbrace{\binom{2n}{n}\biggl(\frac{1}{2}\biggr)^n\biggl(\frac{1}{2}\biggr)^n}_{\mathclap{\large\approx \frac{1}{\sqrt{n}}}}\Biggr]^2\\
		&\sum_{n \geqslant 0}p_{(0,0),(0,0)}^{(n)}=\infty\implies\text{the chain is recurrent.}
	\end{align*}
	Consider now a symmetric random walk on $\Z^3$:
	\[
	\sum_{n \geqslant 0}p_{(0,0,0),(0,0,0)}^{(n)}=\Biggl[\underbrace{\binom{2n}{n}\biggl(\frac{1}{2}\biggr)^n\biggl(\frac{1}{2}\biggr)^n}_{\mathclap{\large\approx \frac{1}{\sqrt{n}}}}\Biggr]^3\approx\frac{1}{n^{\frac{3}{2}}}
	\]
	This is a convergent series, which means that from 3 dimensions onward the random walk \textit{is transient}.
	
	In the transient case we have:
	\[
	\sum_{n\geqslant 1}p_{ii}^{(n)}<\infty\implies p_{ii}^{(n)}\xrightarrow[n\rightarrow\infty]{}0
	\]
	It can be proved that $\forall j \in S$, $p_{ii}^{(n)}\xrightarrow[n\rightarrow\infty]{}0$. In the recurrent case, where we have $\sum p_{ii}^{(n)}=\infty$:
	\begin{itemize}
		\item $p_{ii}^{(n)}\rightarrow c>0$, which causes the \textit{divergence} of the series;
		\item  $p_{ii}^{(n)}\rightarrow 0$ but slowly, not faster than $\frac{1}{n}$, causing divergence in this case as well.
	\end{itemize}
	But is this dichotomy useful? define
	\[m_i=\ev{T_i|X_0=i}\] as the \enf{mean return time to} $\enf{i}$. If $i$ is transient, then \[\prob(T_i<\infty|X_0=i)<1\implies\prob(T_i=\infty|X_0=i)>0\implies m_i=\infty\].
	
	\begin{definition}
		A recurrent state $i$ is called:
		\begin{itemize}
			\item \enf{positive} if $m_i<\infty$
			\item \enf{null} if $m_i=\infty$
		\end{itemize}
		and these are \textit{class properties}.
	\end{definition}
	Later we will show that a \sott{null recurrent} state $i$ is such that $p_{ji}^{(n)}\rightarrow 0$ $\forall j\in S$, so transient and null/positive recurrence can be understood in terms of the \sott{tail of the distribution} of the return times:
	\begin{itemize}[-]
		\item positive probability at $\infty$ $\rightarrow$ transience
		\item probability mass is on $\N$ but it is not integrable ("heavy tailed") $\rightarrow$ null recurrence
		\item probability mass is on $\N$ and it is integrable $\rightarrow$ positive recurrence
	\end{itemize}
	\begin{example}
		The symmetric random walk on $\Z$ and $\Z^2$ are the only recurrent cases:
		\[p_{00}^{(2n)}\propto\frac{1}{\sqrt{n}} \hspace{2.7cm} p_{(0,0),(0,0)}^{2n}\propto\frac{1}{n} \]
		Both tend to 0 as $n$ tends to $\infty$, which means that both the states are null recurrent.
	\end{example}
	We are now interested in establishing how a chain can be classified as positive recurrent.
	
	\begin{proposition}
		On a finite $S$, an irreducible chain is positive recurrent.
	\end{proposition}
	\begin{proof2}
		\[S=\{0,\ldots,N\}\]
		\[\forall n\geqslant1:\qquad\sum_{j=0}^N p_{ij}^{(n)}=1. \]
		This means that we cannot have $p_{ji}^{(n)}\rightarrow 0$ $\forall j\in S$. So there exist a state $i \in S$ that is both positive and irreducible and therefore $X$ must be positive
	\end{proof2} 
	\begin{example}
		Consider a Wright-Fisher chain with $S={0,\ldots,N}$. It has:
		\begin{align*}
			X_n=0 &\implies X_{n+1}=0\\
			X_n= &\implies X_{n+1}
		\end{align*}
		so $S$ is finite but $X$ is not irreducible.
		\[
		\begin{tikzpicture}[->,>=stealth',shorten >=2pt, line width=0.5pt, node distance=1.5cm]
			\node[circle, draw](1){$S_1$};
			\node[circle, draw](2)[above right of=1]{$S_2$};
			\node[circle, draw](3)[below right of=1]{$S_3$};
			\path (1) edge (2);
			\path (1) edge (3);
			\node[text width=4cm] [below of=3] {Case without off mutations};
		\end{tikzpicture}\hspace{3cm}
		\begin{tikzpicture}[<->,>=stealth',shorten >=2pt, line width=0.5pt, node distance=1.5cm]
			\node[circle, draw](1){$S_1$};
			\node[circle, draw](2)[above right of=1]{$S_2$};
			\node[circle, draw](3)[below right of=1]{$S_3$};
			\path (1) edge (2);
			\path (1) edge (3);
			\node[text width=4cm] [below of=3] {Case with odd mutations;};
		\end{tikzpicture}
		\]
		More generally, the task can be difficult. Often, so-called \textit{Lypaunov methods} are useful as a sufficient conidition.
	\end{example}
	
	\begin{proposition}
		Let $P$ be irreducible and let $h:S\rightarrow\R$ be such that $h(i)\geqslant0 \;\forall i \in S$ and:
		\begin{itemize}
			\item $\sum_{k\in S} p_{ik} \cdot h(k)<\infty\qquad\forall i \in S_0$
			\item $\sum_{k\in S} p_{ik} \cdot h(k)\leqslant h(i)\cdot \varepsilon\qquad\forall i \notin S_0$
		\end{itemize}
		for a finite set $S_0 \subset S$ and some $\varepsilon>0$. Then $P$ is positive recurrent.
	\end{proposition}
	The role of $h(\cdot)$ is similar to the one of Lyopunov function used for the stability of ODEs. Here
	\[\sum_{k} p_{ik} \cdot h(k)-\ev{h(x_{n+1}|X_n=1)}\]
	The second condition says that $h(\cdot)$ decreases in expectation outside $S_0$: $S_0$ is \enf{attractive}.
	\begin{example}
		$S+\Z_+, \qquad h(i)=i$. The condition requires
		\[\ev{X_{n+1}-X_{n}|X_n=1}<0, \qquad i>i_0\]
		so the chain is atracted to the set $\{i:i\leqslant i_0\}$, giving stochastic stability.
	\end{example}
	\subsubsection{Stationarity}
	Note:
	\begin{align*}
		\prob(X_n=j) &= \overbrace{\sum_{i\in S}\prob(X_0=i)\prob(X_n=j|P_0=i) }^{\mathclap{\text{disintegrating the joint}}}\\
		&=\sum_{i\in S}\lambda_i p_{ij}^{(n)}=\left(\lambda P^n\right)_j
	\end{align*}
	$\left(\lambda P^n\right)_j$ can be seen as the marginal distribution of $X$ at time $n$: in other words
	\[X_0\sim \lambda\implies X_n\sim\lambda P^n\]
	
	\begin{definition}
		A non negative (row) vector $\pi=(\pi_i, i\in S)$ is said to be an \enf{invariant measure} for $P$ if $\pi P=\pi$, called \enf{global balance equation}. Namely:
		\[\sum_{i \in S} \pi_i p_{oj}=\pi_j \]
		i.e. marginalizing out the initial state with respect to $\pi$, the marginal measure after one step is preserved:
		\[
		X_0 \sim \pi \implies X_i \sim \pi P = \pi
		\]
	\end{definition}
	Furthermore,
	\[
	\pi P^2=\underbrace{\pi P}_{\pi}=\pi P= \pi
	\]
	Iterativity yields that $\pi P^n=\pi$, therefore
	\[
	X_0\sim \pi \implies X_n\sim\pi \qquad \forall n \geqslant 0
	\]
	We have extended the one-step invariance to the $n$-step invariance. If we can normalize $\pi$:
	\[
	\Tilde{\pi}_i=\frac{\pi_i}{\sum_{j\in S}\pi_j}\Tilde{\pi}\]
	and we call $\Tilde{\pi}$ \enf{stationary distribution}.
	\begin{example}
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[->,>=stealth',shorten >=2pt, line width=0.5pt, node distance=2cm]
				\node [circle, draw] (zero) {0};
				\node [circle, draw] (one) [right of=zero] {1};
				\path (zero) edge [bend left] node [above] {$\alpha$} (one);
				\path (zero) edge [loop left] node [left] {$1-\alpha$} (zero);
				\path (one) edge [loop right] node [right] {$1-\beta$} (zero);
				\path (one) edge [bend left] node [below] {$\beta$} (zero);
			\end{tikzpicture}
		\end{figure}
		with $\pi=(\pi_0,\pi_1)$, $\pi_i>0$.
		We need to try and solve the global balance equation:
		\[
		\pi P=\begin{bmatrix}
			\pi_0 & \pi_1
		\end{bmatrix}\begin{bmatrix}
			1-\alpha & \alpha \\
			\beta & 1-\beta
		\end{bmatrix}=\begin{bmatrix}
			\pi_0(1-\alpha)+\pi_1\beta & \pi_0\alpha+\pi_1(1-\beta)
		\end{bmatrix}.
		\]
		Impose $\pi P=\pi=(\pi_0, \pi_1)$:
		\[
		\pi_0(1-\alpha)+\pi_1\beta=\pi_0 \implies \pi_0=\pi_1\frac{\beta}{\alpha}.
		\]
		Substituting,
		\[
		\pi_1\frac{\beta}{\cancel{\alpha}}\cancel{\alpha}+\pi_1\beta=\pi_1 \implies \pi_1=\pi_1 \quad\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;
		\]
		with normalization we get
		\begin{align*}
			\pi_0+\pi_1=1&\longrightarrow{}\pi_1\frac{\beta}{\alpha}+\pi_1=1\\
			&\longrightarrow\pi_1=\frac{\beta}{\alpha+\beta},\quad\pi_0=\frac{\alpha}{\alpha+\beta}.
		\end{align*}
	\end{example}
	\begin{example}
		If $P$ is stationary with respect to $\pi$ then its "lazy" version \[P'=\varepsilon P+(1-\varepsilon)I,\quad\varepsilon\in[0,1]\]is still stationary.
	\end{example}
	\begin{example}
		Consider the random walk on $\Z$. The global balance equation is:
		\begin{align*}
			\sum_{i \in S} \pi_i p_{ij}&=\pi_{j-1}p_{j-1,j}+\pi_{j+1}p_{j+1,j}\\
			&=\pi_{j-1}p+\pi_{j+1}(1-p)\stackrel{?}{=}\pi_j
		\end{align*}
		Set $\pi_i=c\geqslant0$:
		\[cp+c(1-p)=c\]
		So:\begin{itemize}
			\item the uniform measure on $\Z$ is invariant;
			\item the invariant measures are not necessarily unique;
			\item $p\neq\frac{1}{2}$ is the transient case, $p=\frac{1}{2}$ is the recurrent case.
		\end{itemize}
		So there exists an invariant measure that doesn't imply recurrence.
		\[
		\sum_{i\in\Z}\pi_i=\begin{cases}
			\infty &c>0\\
			0 &c=0
		\end{cases}
		\]
		We cannot normalize in this case ($\pi$ is only $\sigma$-finite, not finite) so we have invariant measures but we don't have any stationary distribution: the chain is \sott{non stationary}.
	\end{example}
	\begin{exercise}
		Find the stationary distribution for $RW(p,1-p)$ and $p_{00}=1-p$ under correct restrictions on $p$.
	\end{exercise}
	\begin{exercise}
		Do the same for $B\&D(p,1-p)$ and $p_{00}=1-p$ under the correct restrictions on $p$.
	\end{exercise}
	\begin{exercise}
		Show that in $B\&D(p,1-p)$:
		\begin{align*}
			p_i&=\frac{b}{b+i},\quad b>0\\
			\implies\pi_i&=\frac{b+i}{2b}\sim Poiss(i,b)
		\end{align*}
		(called \sott{size-biased Poisson}).
	\end{exercise}
	We are interested in conditions that provide stationarity:
	
	\begin{proposition}
		If for some $i\in S$ \[
		p_{ij}^{(n)}\xrightarrow[n\rightarrow\infty]{}\pi_j\implies\pi\text{ is invariant.}
		\]
	\end{proposition}
	\begin{proof2}
		\[S=\{0,\ldots,N\}\]
		\[\forall n\geqslant1:\qquad\sum_{j=0}^N p_{ij}^{(n)}=1. \]
		This means that we cannot have $p_{ji}^{(n)}\rightarrow 0$ $\forall j\in S$. So there exist a state $i \in S$ that is both positive and irreducible and therefore $X$ must be positive
	\end{proof2} 
	\begin{example}
		Consider the random walk on $\Z$:
		\[p_{ij}^{(n)}\rightarrow0\implies\pi=(\pi_i,i\in\Z)\quad\pi_i=0\]
		Which is invariant, as found above, with $c=0$.
	\end{example}
	\begin{example}
		\[
		P=\begin{bmatrix}
			1-\alpha & \alpha \\
			\beta & 1-\beta
		\end{bmatrix}\qquad\alpha,\beta\neq 1
		\]
		\begin{align*}
			p_{00}^{(n+1)}&\equalexpl{\text{C.K.}}\hspace{1em}\sum_{k\in S}p_{0k}^{(n)}p_{k0}=p_{00}^{(n)}\underbrace{(1-\alpha}_{p_{00}}+\underbrace{pp{01}^{(n)}}_{1-p_{00}^{(n)}}\underbrace{\beta}_{p_{10}}\\
			p_{00}^{(n+1)}&=\ldots=\beta+p_{00}^{(n)}(1-\alpha-\beta)
		\end{align*}
		The recurrence equation brings to solution:
		\[p_{00}^{(n)}=\frac{\beta}{\alpha+\beta}+(1-\alpha-\beta)^n\frac{\alpha}{\alpha+\beta}\rightarrow\frac{\beta}{\alpha+\beta}\]
		as found earlier.
	\end{example}
	In general we do not want to assume that $P^n$ converges.
	\begin{theorem}
		An irreducible Markov Chain has invariant distribution $\pi$ if and only if it is positive recurrent, in which case $\pi$ is unique and 
		\begin{equation*}
			\pi_i = \frac{1}{m_i}
		\end{equation*}
		where 
		\begin{equation*}
			m_i = \ev{T_i | X_0 = i}
		\end{equation*}
		is the \textbf{expected return time}. 
	\end{theorem}
	\begin{example}
		Considering $P=I$, we found out that the invariant distribution is not unique. This is a contradiction, due to the fact that irreducibility is violated.
	\end{example}
	\begin{example}
		Consider a random walk on $\mathbb{Z}$ for $p \in (0,1)$ which is transient and null recurrent: This implies that $m_i = \infty$. Indeed $\pi_i = \frac{1}{m_i} = 0$ is invariant for the random walk. 
	\end{example}
	\begin{example}
		Consider a 2-state chain with
		\begin{equation*}
			(\pi_0, \pi_1) = (\frac{\beta}{\alpha + \beta}, \frac{\alpha}{\alpha + \beta}).
		\end{equation*}
		If we interpret the statement literally, then\[(m_0,m_1)= (\frac{\alpha +\beta}{\beta}, \frac{\alpha + \beta}{\alpha})\]
		Let's choose, for example,  $\alpha = 0.5$ and $\beta = 1$:\[   
		\begin{tikzpicture}[->,>=stealth',shorten >=2pt, line width=0.5pt, node distance=2cm]
			\node [circle, draw] (zero) {0};
			\node [circle, draw] (one) [right of=zero] {1};
			\path (zero) edge [bend left] node [above] {$\frac{1}{2}$} (one);
			\path (zero) edge [loop left] node [left] {$\frac{1}{2}$} (zero);
			\path (one) edge [bend left] node [below] {$1$} (zero);
		\end{tikzpicture}\]
		with $m_0=\frac{3}{2}$ and $m_1=3$
		We can double check:
		\begin{itemize}
			\item Return to 0: $\begin{cases}
				1 &\text{ with probability } \frac{1}{2}\\
				2 &\text{ with probability } \frac{1}{2}\\
			\end{cases}$ $\implies 1 \cdot \frac{1}{2} + 2 \cdot \frac{1}{2} = \frac{3}{2} = m_0$
			\item Return to 1: $\begin{cases}
				2 &\text{ with probability } \frac{1}{2}\\
				3 &\text{ with probability } \frac{1}{2^2}\\
				\vdots \\
				(1+k) &\text{ with probability } \frac{1}{2^k}\\
				\vdots\\
			\end{cases}$
			so\[m_1 = \sum_{k \geq 1} (1+k) \frac{1}{2^k} = \ldots = \sum_{k \geq 1} \frac{1}{2^k} + \sum_{k \geq 1} \frac{k}{2^k} = 1+2 = 3.\]
		\end{itemize}
	\end{example}
	\begin{definition}
		An irreducible Markov chain is said to be \enf{reversible  with respect to $\pi$} if 
		\[
		\pi_i p_{ij} = \pi_j p_{ji} \hspace{1 cm} \forall i,j \in S  \hspace{1 cm} \]
		which is called \enf{detailed balance equation}.
	\end{definition}\begin{proposition}
		If $P$ and $\pi$ are in detailed balance (that is, they satisfy the detailed balance equation, then $\pi$ is invariant for $P$.
	\end{proposition}
	
	\begin{proof2}
		Integrate both sides of the equation with respect to $i$:
		\begin{align*}
			\sum_i\pi_ip_{ij}&= \sum_i\pi_jp_{ij}\\
			&=\pi_j\underbrace{\sum_ip_{ij}}_{=1}=\pi_j.
		\end{align*}
	\end{proof2}
	Detailed balance is a sort of \sott{local criterion} on the chain propensity to go from $i\rightarrow j$ and $j \rightarrow i$, while the global balance is a \sott{global criterion} on the chain propensity to go from $i\rightarrow j$ and $j \rightarrow i$.
	\begin{example}
		\begin{minipage}{0.5\textwidth}
			\center
			\begin{tikzpicture}[->,>=stealth',node distance=2cm]
				\foreach \i [count=\j from 0] in {150,30,-90} 
				\node[circle,draw] (\j) at (\i:1.3) {\j};
				\path (0) edge [bend left] node [above] {\footnotesize 2/3} (1);
				\path (1) edge [bend left] node [below right] {\footnotesize 2/3} (2);
				\path (2) edge [bend left] node [below left] {\footnotesize 2/3} (0);
				\path (1) edge [bend left] node [above] {\footnotesize 1/3} (0);
				\path (2) edge [bend left] node [right] {\footnotesize 1/3} (1);
				\path (0) edge [bend left] node [left] {\footnotesize 1/3} (2);
			\end{tikzpicture}
		\end{minipage} \hfill
		\begin{minipage}{0.45\textwidth}
			Check first that the uniform distribution is invariant: \[\pi = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3}).\] 
			Nevertheless, 
			\[
			\pi_0 p_{01} = \frac{1}{3} \frac{2}{3} \neq \frac{1}{3}\frac{1}{3} = \pi_1 p_{10}
			\]
		\end{minipage}
	\end{example}
	\begin{example}
		Consider a Birth and Death process of parameters $(p,1-p)$ with 
		\begin{equation*}
			p_{00} = 1-p
		\end{equation*}
		The detailed balance equation is
		\begin{equation*}
			\pi_i p_{ij} = \pi_j p_{ji}   \hspace{1 cm} \forall i,j
		\end{equation*}
		Take $j=i+1$: we get
		\begin{align*}
			\pi_i p_{i,i+1} &= \pi_{i+1} p_{i+1,i}  \hspace{1 cm} q:=1-p \\
			\pi_i p &= \pi_{i+1} q\\
			i = 0 &\implies \pi_1 = \pi_0 \frac{p}{q}\\
			i = 1 &\implies \pi_2 = \pi_1 \frac{p}{q} = \pi_0 \Big(\frac{p}{q}\Big)^2 \\
			&\implies \pi_k = \pi_0 \Big(\frac{p}{q}\Big)^k
		\end{align*}
		So, $\pi_k$ is proportional to $\rho^k$, where $\rho = \frac{p}{q}$. \\
		We now integrate $\pi_k$:
		\begin{equation*}
			\sum_{k \geq 0} \pi_k < \infty \iff p < \frac{1}{2} \hspace{0.5 cm} (\rho < 1) + \ldots
		\end{equation*}
		which implies that 
		\begin{equation*}
			\Tilde{\pi} = \frac{\pi_k}{\sum_j \pi_j} \sim \text{Geom}(\rho).
		\end{equation*}
		So, using detailed balance equations, we get a result faster than what we would obtain through global balance equations.
	\end{example}
	Let 
	\begin{equation*}
		<x,y>:= \sum_{i \in S} x_i y_i \pi_i.
	\end{equation*}
	\begin{definition}
		We define 
		\begin{equation*}
			\ell_2(\pi) := \{x \in \mathbb{R}^S: <x,x> < \infty\}.
		\end{equation*}
		A matrix $P$ is \enf{self-adjoint with respect to} $\pi$ if 
		\begin{equation*}
			<Px,y> = <x, Py>, \hspace{1 cm} x,y \in \ell_2(\pi).
		\end{equation*}
	\end{definition}\begin{proposition}
		Markov chain with transition matrix $P$ is reversible with respect to $\pi$ if and only if $P$ is self-adjoint with respect to $\pi$.
	\end{proposition}
	It can be proved that if $P$ is reversible with respect to $\pi$, then the \enf{time reversal}
	\begin{equation*}
		Y_n := X_{N-n}, \hspace{1 cm} 0 \leqslant n \leqslant N
	\end{equation*}
	has the same distribution as $X$ if both are started in equilibrium.
	\subsubsection{Convergence}
	We can interpret time reversal as the possibility of reversing the sense of time. If the chain $X$ starts from an arbitrary initial distribution, not necessarily at equilibrium, is it going to converge to the stationary distribution?
	\begin{equation*}
		\lambda P^n \xrightarrow[\text{?}]{n \rightarrow \infty} \pi
	\end{equation*}
	The notion of convergence convergence has to be made more precise.
	\begin{example}
		Consider the matrix $P$:\\
		\begin{minipage}{0.5\textwidth}
			\center
			\[P=\begin{bmatrix}
				0 & 1 \\
				1 & 0 \\
			\end{bmatrix}\]
		\end{minipage} \hfill
		\begin{minipage}{0.45\textwidth}
			\begin{figure}[H]
				\centering
				\begin{tikzpicture}[->,>=stealth',shorten >=2pt, line width=0.5pt, node distance=2cm]
					\node [circle, draw] (zero) {0};
					\node [circle, draw] (one) [right of=zero] {1};
					\path (zero) edge [bend left] node [above] {} (one);
					\path (one) edge [bend left] node [below] {} (zero);
				\end{tikzpicture}
			\end{figure}
		\end{minipage}
		The chain is 
		\begin{itemize}
			\item [-] irreducible 
			\item [-] positive-recurrent (the mean return time, which is the only return time, equals $2$)
			\item [-] The stationary distribution
			\begin{equation*}
				\pi = (\frac{1}{2}, \frac{1}{2}) 
			\end{equation*}
			is invariant \checkmark
		\end{itemize}
		But, 
		\begin{itemize}
			\item [-] $P^{2n+1} = P, \;P^{2n} = I$, so $p_{ij}^{(n)} \not\rightarrow \pi_j$. Previously, we assumed convergence and  said that a limiting distribution is invariant
			\item [-] $\lambda P^{2n} = \lambda \hspace{1 cm} \lambda P^{2n+1}=1-\lambda$
		\end{itemize}
		Why? The periodicity is \sott{preventing the chain to converge}. Periodicity prevents convergence by making the dependence on the initial state too strong: if the chain starts from 0 the all even steps will bring to 0 and all odd steps will bring to 1.
	\end{example}
	\begin{definition}
		An irreducible, aperiodic, positive recurrent chain is called \enf{ergodic}.
	\end{definition}
	Under these assumptions, we want to show that an ergodic chain converges to equilibrium from any initial distribution. To do so, we are going to use \sott{\textit{coupling}}. Consider
	\begin{itemize}
		\item $\pi$ invariant for $P$
		\item $X \sim$ Markov $(\lambda, P)$. $\lambda$ is arbitrary and the so distributed $X$ is the chain of interest.
		\item $Y \sim$ Markov $(\pi, P)$ is an auxiliary chain
		\[\implies Y_n \sim \pi, \forall n \geq 0\]
		\item Assume $X$ and $Y$ meet in finite time.
	\end{itemize}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[
			>=stealth', axis/.style={->},
			point/.style={circle, inner sep=0pt, fill, minimum size=4pt, label=#1},
			scale=1]
			\draw[axis] (0,-0.5) -- (0,4.5) node[above right]{$y$};
			\draw[axis] (-0.5,0) -- (8.5,0) node[below right]{$x$};
			\draw[loosely dashed, ultra thick, black] (0,4) -- (3,1);
			\draw[loosely dashed, ultra thick, black] (3,1) -- (5,3);
			\draw[loosely dashed, ultra thick, black] (5,3) -- (6,2);
			\draw[loosely dashed, ultra thick, black] (6,2) -- (8,4) node [black,midway,above]{$y$};
			\draw[densely dotted, thick, black] (0,3) -- (2,1);
			\draw[densely dotted, thick, black] (2,1) -- (5,4);
			\draw[densely dotted, thick, black] (5,4) -- (6,3);
			\draw[densely dotted, thick, black] (6,3) -- (7,2);
			\draw[densely dotted, thick, black] (7,2) -- (8,3) node [black,midway,above]{$x$};
			\begin{scope}[shift={(0,-0.15)}]
				\draw[ultra thick, RedViolet] (0,3) -- (2,1);
				\draw[ultra thick, RedViolet] (2,1) -- (2.5,1.5);
				\draw[ultra thick, RedViolet] (2.5,1.5) -- (3,1);
				\draw[ultra thick, RedViolet] (3,1) -- (5,3);
				\draw[ultra thick, RedViolet] (5,3) -- (6,2);
				\draw[ultra thick, RedViolet] (6,2) -- (8,4) node [RedViolet, right]{$\enf{z}$};
			\end{scope}
			\draw[thick](2.5,-0.2) -- (2.5,1.5);
			\node[below] at (2.5,-0.2) {\footnotesize$\tau<\infty$};
			\node[circle, fill, inner sep=0pt, minimum size=5pt,RedViolet] at (2.5,1.5) {};
		\end{tikzpicture}
	\end{figure}
	We obtain a new chain $Z_n$ so defined:
	\[
	Z_n = 
	\begin{cases}
		X_n & n<\tau \\
		Y_n & n \geq \tau \\
	\end{cases}
	\]
	Now, \underline{if} we show that $Z \sim$ Markov $(\lambda, P)$ (reaching quote $2$ assumptions, together with meeting in finite time), then 
	\[
	X \stackrel{d}= Z \qquad\text{and}\qquad
	Z_n \sim \pi \hspace{0.5 cm} \forall n \geq \tau\]
	which would imply that 
	\begin{equation*}
		X_0 \sim \lambda, X_n \xrightarrow{n \rightarrow \infty} \pi 
	\end{equation*}
	\begin{definition}
		Two Markov chains $Z$ and $Y$ on $S$ are said to \enf{couple} if there exists an almost surely finite stopping time $\tau$, called \enf{coupling time}, such that 
		\begin{equation*}
			Z_n = Y_n \hspace{1 cm} \forall n \geq \tau
		\end{equation*}
	\end{definition}
	This relates to the \enf{total variation distance} between the marginals:
	\begin{equation*}
		V \sim \lambda,\; W \sim \mu \text{ on } S
	\end{equation*}
	The total variation distance is 
	\begin{align*}
		d_{TV}(\lambda,\mu) &= \sup_{A \subset S} |\mathbb{P}(V \in A) - \mathbb{P}(W \in A)| \\
		&= \frac{1}{2} \sum_{i \in S} |\lambda_i - \mu_i|
	\end{align*}
	\begin{proposition}
		\label{totvardist}
		Let $Z \sim$ Markov$(\lambda, P)$ and 
		let $Y \sim$ Markov$(\mu, P)$; \\
		assume that a coupling time exists. Then,
		\begin{equation*}
			d_{TV}(\lambda P^n, \mu P^n) \xrightarrow{n \rightarrow \infty} 0
		\end{equation*}
	\end{proposition}
	
	\begin{proof2}
		Consider $A \subset S$ and 
		\begin{align*}
			\mathbb{P}(Z_n \in A) - \mathbb{P}(Y_n \in A) &= \mathbb{P}(Z_n \in A, n < \tau) + \mathbb{P}(Z_n \in A, n \geq \tau) +\\
			&- \mathbb{P}(Y_n \in A, n < \tau) - \mathbb{P}(Y_n \in A, n \geq \tau) \\
			&\stackrel{Z_n = Y_n, \forall n \geq \tau}= \mathbb{P}(Z_n \in A, n < \tau)- \underbrace{\mathbb{P}(Y_n \in A, n < \tau)}_{\geq 0} \\
			&\leq \mathbb{P}(\underbrace{Z_n \in A, n < \tau}_{\subset \{n < \tau\}})\\
			&\leq \mathbb{P}(n < \tau)
		\end{align*}
		Hence, by the arbitrarity of $A$,
		\begin{equation*}
			\sup_{A \subset S} |\mathbb{P}(Z_n \in A) - \mathbb{P}(Y_n \in A)| \leq \mathbb{P}(\tau > n)
		\end{equation*}
		but $\tau < \infty$ almost surely, so
		\begin{equation*}
			\mathbb{P}(\tau > n) \xrightarrow{n \rightarrow \infty} 0
		\end{equation*}
	\end{proof2}
	So, we are sure that in a finite time they meet and hence $Z$ is going to have the stationary distribution $\pi$. Also, 
	\begin{equation*}
		\exists \tau < \infty \implies d_{TV} \rightarrow 0
	\end{equation*}
	It is enough to show that such $\tau$ exists for ergodic chains. 
	\begin{theorem}
		Let 
		\begin{itemize}
			\item $P$ be ergodic 
			\item $X \sim$ Markov$(\lambda, P)$ and $Y \sim $ Markov$(\mu, P)$ be independent 
		\end{itemize}
		Then, the stopping time 
		\begin{equation*}
			\tau = \inf\{n \geq 0: X_n = Y_n\}
		\end{equation*}
		is almost surely finite, and the chain
		\[
		Z_n =   
		\begin{cases}
			X_n & n<\tau \\
			Y_n & n \geq \tau \\
		\end{cases}
		\]
		is Markov$(\lambda, P)$.
	\end{theorem}
	So, this yields a coupling time between $Z \sim$ Markov$(\lambda, P)$ and $Y \sim$ Markov $(\mu, P)$, which implies that 
	\begin{equation*}
		Z_n = Y_n \sim \mu P^n, \forall n \geq \tau
	\end{equation*}
	So, by the Proposition \ref{totvardist}, 
	\begin{equation*}
		d_{TV}(\lambda P^n, \mu P^n) \xrightarrow{n \rightarrow \infty} 0
	\end{equation*}
	If we now let $\mu = \pi$, then $\mu P^n = \pi$ and hence 
	\begin{equation*}
		d_{TV}(\lambda P^n, \pi) \rightarrow 0
	\end{equation*}
	\begin{remark}
		Imposing $\mu = \pi$ is not cheating since $\mu$ is auxiliary and we can equal it to what we need. The result is that all the marginals converge to the stationary. 
	\end{remark}
	Since $X \stackrel{d} = Z$, then
	\begin{equation*}
		\mathbb{P}(X_n = j) \xrightarrow{n \rightarrow \infty} \pi_j, \hspace{1 cm} \text{ for every initial distribution } \lambda
	\end{equation*}
	If $\lambda = \delta_i$:
	\begin{align*}
		d_{TV}(\lambda P^n, \pi) &= \frac{1}{2} \sum_{j \in S} |(\lambda P^n)_j - \pi_j| \\
		&= \frac{1}{2} \sum_{j \in S} |\sum_h \lambda_h p_{hj}^{(n)} - \pi_j| \\
		&= \frac{1}{2} \sum_{j \in S} |1 \cdot p_{ij}^{(n)}-\pi_j| \xrightarrow{n \rightarrow \infty} 0
	\end{align*}
	\begin{remark}
		In the first line, we take the supremum over a discrete space, so in the worst case we integrate the differences. 
	\end{remark}
	Hence, all the transition probabilities 
	\begin{equation*}
		p_{ij}^{(n)} \xrightarrow{n \rightarrow \infty} \pi_j, \forall i \in S
	\end{equation*}
	converge to $\pi_j$ for every starting point (state) $i$.\\
	In summary:
	\begin{theorem}
		\label{th erg}
		Let $P$ be ergodic with invariant distribution $\pi$, and let $X \sim$ Markov$(\lambda, P)$. Then,
		\begin{equation*}
			d_{TV}(\lambda P^n, \pi) \xrightarrow{n \rightarrow \infty} 0
		\end{equation*}
		and
		\begin{equation*}
			p_{ij}^{(n)} \xrightarrow{n \rightarrow \infty} \pi_j, \hspace{1 cm} \forall i,j \in S. 
		\end{equation*}
	\end{theorem}
	\begin{remark}
		Ergodicity ensures that the two chains meet on the diagonal. $\lambda$ is orthogonal to $\mu$ and there exists $B \in S$ such that 
		\begin{align*}
			&\sum_{i \in B} \lambda_i = 1 \\
			&\sum_{i \in B^C} \mu_i = 1 \\
		\end{align*}
	\end{remark}
	\begin{example}
		Consider the matrix $P$:\\
		\begin{minipage}{0.5\textwidth}
			\center
			\[P=\begin{bmatrix}
				0 & 1 \\
				1 & 0 \\
			\end{bmatrix}\]
		\end{minipage} \hfill
		\begin{minipage}{0.45\textwidth}
			\begin{figure}[H]
				\centering
				\begin{tikzpicture}[->,>=stealth',shorten >=2pt, line width=0.5pt, node distance=2cm]
					\node [circle, draw] (zero) {0};
					\node [circle, draw] (one) [right of=zero] {1};
					\path (zero) edge [bend left] node [above] {} (one);
					\path (one) edge [bend left] node [below] {} (zero);
				\end{tikzpicture}
			\end{figure}
		\end{minipage}
		We know that \[
		\pi = (\frac{1}{2}, \frac{1}{2}) \qquad\text{and}\qquad P^{(2n)} = I, P^{(2n+1)} = P \]
		Consider also $X, Y$ as characterized in Theorem \ref{th erg}, with $\lambda = \delta_0, \mu = \pi$. This means that $X$ starts at $0$ with probability $1$:
		\begin{align*}
			X_0 &= 0 \text{ a.s.} \\
			Y_0 &=
			\begin{cases}
				0 &\text{ with probability } \frac{1}{2} \\
				1 &\text{ with probability } \frac{1}{2} \\
			\end{cases}
		\end{align*}
		So, either they meet at $n = 0$ or they never meet. 
	\end{example}
	It's useful to remind that are talking about almost sure meeting, not with probability $1$. Which factor breaks the conclusion of meeting almost surely? The chain is:
	\begin{itemize}
		\item positive recurrent;
		\item irreducible;
		\item \sott{not} aperiodic.
	\end{itemize}
	\begin{theorem}
		\enf{Ergodic theorem}. Let $X\sim Markov(\lambda,P)$ be irreducible and let $m_i=\mathbb{E}\Bigl[T_i|X_0=i\Bigr]$. Then, almost surely,
		\[\frac{1}{N}\sum_{n=1}^N\mathbbm{1}(X_n=j)\xrightarrow[N\rightarrow\infty]{}\Pi_j=\frac{1}{m_j}.\]
		Moreover, if $P$ is positive recurrent with unique stationary distribution $\pi$ and $f:S\rightarrow\mathbb{R}$ with respect to $\pi$, then
		\[\frac{1}{N}\sum_{n=1}^Nf(X_n)\xrightarrow[N\rightarrow\infty]{}\sum_{j\in S}f(j)\Pi_j.\]
	\end{theorem}
	The quantity $\frac{1}{N}\sum_{n=1}^Nf(X_n)$ is the \textbf{ergodic average} and it is taken along the sample path of the chain. The right-hand side shows an object whose form recalls an expectation. \\
	So, this is a way of relaxing the assumption of an i.i.d. sample. We are substituting it with the correlation associated to the Markovian structure, so that the convergence holds. 
	\[\sum_{j\in S}f(j)\Pi_j=\ev{f(X_n)} \qquad \text{at equilibrium.}\]
	So we have an equivalent form of the Strong Law of Large numbers for Markov Chains.
	\begin{remark}
		In the one-dimensional case, we have convergence of order $\frac{1}{\sqrt{10}}$ to $0$, which is not enough to make $m_j < \infty$. Hence $m_j = \infty \implies \frac{1}{m_i} \rightarrow 0$, which is consistent with what we know.
	\end{remark}
	The first claim holds for all irreducible chains, for example the Random Walk whereby:
	\[p_{00}^{(n)}\rightarrow0 \qquad\text{as }\frac{1}{\sqrt{n}}\text{ and }m_i=\infty\]
	implies
	\[\frac{1}{N}\sum_n\mathbbm{1}(X_n=j)\rightarrow0.\]
	What about the speed of convergence?
	\begin{example}
		Consider \[   P=\begin{bmatrix}
			1-\alpha & \alpha \\
			\beta & 1-\beta
		\end{bmatrix}\]
		With
		\[
		p_{00}^{(n)}=\frac{\beta}{\alpha+\beta}+(1-\alpha-\beta)^n\frac{\alpha}{\alpha+\beta}.
		\]
		Set $\gamma_1=1$ and $\gamma_2=1-\alpha-\beta$.
		We can prove that
		\begin{align*}
			P^n &=\frac{1}{\alpha+\beta}\begin{bmatrix}
				\beta & \alpha \\
				\beta & \alpha
			\end{bmatrix}=\frac{1-\alpha-\beta}{\alpha+\beta}\begin{bmatrix}
				& \\
				& \\
			\end{bmatrix}\\
			\pi &=\mathbf{1}^T=\begin{bmatrix}
				\pi_0 & \pi_1 \\
				\pi_0 & \pi_1
			\end{bmatrix}\\
			P^n-\pi=\gamma_2^n B
		\end{align*} %boooh non ho mica tanto capito
	\end{example}
	\begin{theorem}
		Let $P$ be a $k\times k$ irreducible and aperiodic transition matrix. Denote the distinct eigenvalues as
		\[1=\gamma_1>|\gamma_2|>|\gamma_3|>\ldots>|\gamma_k|.\]Then
		\[P^n=\pi+
		\mathcal{O}(n^{m-1}|\gamma_2|^n)\] where $m$ is the algebraic multiplicity of $\gamma_2$.
	\end{theorem}
	\begin{definition}
		A statement like\[
		d_{TV}(\lambda P^n,\pi)\leqslant C(\lambda)\rho^n
		\] is called \textbf{geometric ergodicity}, with $C(\lambda)\in\R$ depending on $\lambda$.
	\end{definition}
	\subsubsection{Quasi-stationary distributions}
	Let's now take into account \textbf{quasi-stationary distributions}.\\
	Let $a\in S$ be an absorbing state with $\prob(T_a<\infty)=1$ visited almost surely in finite time. if $X$ is irreducible then there is no stationarity.\\
	\textbf{Example}: the Galton-Watson branching process with mean offspring $m=\ev{g}<1$.
	It could be of interest to study the behaviour before absorption.\\
	Let $\lambda$ be supported by $S_a=S/\{a\}$ and denote\[\prob_\lambda(\cdot):=\prob(\cdot|X_0\sim\lambda).\] Then, given $A\subset S_a$, we are interested in 
	\[\prob_\lambda(X_n\in A|T_a>n)=\frac{\prob_\lambda(X_n\in A,T_a>n)}{\prob_\lambda(T_a>n)}=\frac{\lambda P^n|_a}{\lambda P^n|_{S_a}}.\]
	\begin{definition}
		We say that $\pi$ is a \textbf{quasi-stationary distribution} if
		\[
		P_\pi (X_n=i|T_a>n)=\Pi_i\qquad \forall n \geqslant 0.
		\]
		This process preserves the marginal, conditional on not getting observed.
	\end{definition}
	\begin{example}
		With $S=\{0,\dots,N\}$, let $Y$ be a symmetric random walk on $S_0=\{1,\ldots,N\}$:
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[->,node distance=2cm]
				\node [circle,draw] (one) [] {1};
				\node [circle,draw] (two) [right of=one] {2};
				\node [circle,draw] (dots) [right of=two]{$\ldots$};
				\node [circle,draw] (enne) [right of=dots] {$N$};
				\path (one) edge [bend left] node [above] {1/2} (two);
				\path (two) edge [bend left] node [above] {1/2} (dots);
				\path (dots) edge [bend left] node [above] {1/2} (enne);
				\path (enne) edge [bend left] node [below] {1/2} (dots);
				\path (dots) edge [bend left] node [below] {1/2} (two);
				\path (two) edge [bend left] node [below] {1/2} (one);
				\path (one) edge [loop left] node [left] {1/2} (one);
			\end{tikzpicture}
			\label{symmrw}
		\end{figure} whose invariant $\pi$ is the uniform on on $S_0$. Let $\tau$ be a finite random time on $\N$, independent of $Y$, such that:
		\[
		X_n=\begin{cases}
			Y_n &n<\tau\\
			0 &n>\tau
		\end{cases}.
		\]
		So, in $\tau$, $X$ jumps to 0 and there remains. The transition probabilities are:
		\[
		i \in S_0:\;p_{ij}^{(n)}=\underbrace{\begin{cases}
				\prob(\tau=n)&j=0\\
				\frac{1}{2}(1-\prob(\tau-n)) &j=i\pm 1
		\end{cases}}_{\footnotesize\text{there are called \textit{modulo boundaries}}}
		\]
		Since $\tau$ is independent, we have 
		\begin{align*}
			\prob_{\pi}(X_n=i|\tau>n)&=\prob_{\pi}(X_n=i|X_n\neq0)\\
			&=\prob_{\pi}(Y_n=i)=\Pi_i
		\end{align*}
	\end{example}
	\subsection{Hidden Markov Chains}
	Hidden Markov Chains are a widely applied statistical framework, useful whenever it is necessary to estimate the current status of a system based on noisy or incomplete observations.
	
	\textbf{Examples}:\begin{itemize}
		\item $X_n$: position of a moving object;\\
		$Y_n$: noisy observations of the position (by means, for instance, of a radar or a sonar);
		\item $X_n$: state of a productive system (that can be good or bad);\\
		$Y_n$: conditions of the output (that can be perfect or faulty);
		\item $X_n$: latent level of volatility in a financial market (high or low);\\
		$Y_n$: amount of financial products exchanged;
		\item $X_n$: number of alleles of type 0 in a population;\\
		$Y_n$: number of alleles of type 0 in a sample;
	\end{itemize}
	Let $X\sim Markov(\lambda, P)$ be unobserved (or hidden). This chain is sometimes called \textit{signal}. Every time $X$ enters a sate, an observation $Y$ (that we assume discrete on $\{a_1, a_2,\ldots\}$) is emitted with probability that depends on the current state of $X$ only. The model is:
	\begin{align*}
		&\diamond \prob(X_n=j|X_{n-1}+i)=p_{ij}\\
		&\diamond \prob(\underbrace{Y_n=y_n}_{\mathclap{\text{generic realization of }Y}}|X_n=j)=p_j(y_n)
	\end{align*}
	$p_j(y_n)$ is the \textbf{emission distribution}, that is the likelihood of observing $Y_n$ if $X$ is in $j$. If $S+\{0,1\}$:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[->,>=stealth', line width=0.5pt, node distance=2cm]
			\node [circle,draw] (0) [] {0};
			\node [circle,draw] (a1) [below left=1.5cm and 0.2 cm of 0] {$a_1$};
			\node [circle,draw] (a2) [below right=1.5cm and 0.2 cm of 0]{$a_2$};
			\node [circle,draw] (1) [above right=1.5cm and 0.2 cm of a2] {1};
			\node [circle,draw] (a3) [below right=1.5cm and 0.2 cm  of 1] {$a_3$};
			\path (0) edge [black] node [above,midway,sloped] {\footnotesize$p_0(a_1)$} (a1);
			\path (0) edge [black] node [above,near end,sloped, rotate=180] {\footnotesize$\ldots$} (a2);
			\path (0) edge [black] node [above,near end,sloped, rotate=180] {\footnotesize$\ldots$} (a3);
			\path (1) edge [red] node [red,above,midway,sloped] {\footnotesize$p_1(a_3)$} (a3);
			\path (1) edge [red] node [red,above,near start,sloped, rotate=180] {\footnotesize$\ldots$} (a2);
			\path (1) edge [red] node [red,above,near start,sloped, rotate=180] {\footnotesize$\ldots$} (a1);
			\path (0) edge [bend left] node {} (1);
			\path (1) edge [bend left] node {} (0);
			\path (0) edge [loop left] node {} (0);
			\path (1) edge [loop right] node {} (1);
		\end{tikzpicture}
		\label{hidden}
	\end{figure}
	An alternative depiction is:
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[->,>=stealth', line width=0.5pt, node distance=2cm]
			\node [circle,draw] (x0) [] {$X_0$};
			\node [circle,draw] (x1) [right of= x0] {$X_1$};
			\node [circle,draw] (x2) [right of= x1] {$X_2$};
			\node [circle] (dots) [right of= x2] {$\ldots$};
			\node [circle,draw] (xt) [right of= dots] {$X_T$};
			\node [circle,draw] (y0) [below of=x0] {$Y_0$};
			\node [circle,draw] (y1) [right of= y0] {$Y_1$};
			\node [circle,draw] (y2) [right of= y1] {$Y_2$};
			\node [circle] (ydots) [right of= y2] {$\ldots$};
			\node [circle,draw] (yt) [right of= ydots] {$Y_T$};
			\node [align=left] (indep) [RedViolet,below=1.6cm of y0] {\footnotesize These arrows represent the conditional \\ \footnotesize independence of $Y_i|X_i$, which is independent\\ \footnotesize from everything else};
			\node [align=left] (markoviality) [Dandelion,below=0.5cm of y2] {\footnotesize Markoviality: $X_2|X_1\independent X_0,Y_0, Y_1$};
			\path (x0) edge (x1);
			\draw[->] (x1) -- (x2) node [sloped,midway](L){};
			\path (x2) edge (dots);
			\path (dots) edge (xt);
			\path (x0) edge (y0);
			\draw[->] (x1) -- (y1) node [sloped,midway](M){};
			\path (x2) edge (y2);
			\path (dots) edge (ydots);
			\path (xt) edge (yt);
			\draw[->,RedViolet] (indep) to[bend left] (M.center);
			\draw[->,Dandelion] (markoviality) to[bend left] (L.center);
		\end{tikzpicture}
		\label{hiddalt}
	\end{figure}
	The goal is to establish
	\begin{align*}
		&\prob(X_{0:T}|Y_{0:T})\\
		\text{with}\quad &X_{0:T}=(X_0,\ldots,X_T)\\
		&Y_{0:T}=(Y_0,\ldots,Y_T)
	\end{align*}
	as the probability of a certain sequence of states of the system, given a sequence of observations. We can use the chain rule to write:
	\[
	\prob(X_T|Y_{0:T})\prob(X_{T-1}|X_T,Y_{0:T})\cdot\ldots\cdot\prob(X_0|X_{1:T},Y_{0:T})
	\]
	as the backwards decomposition. The generic factor is:
	\begin{equation*}
		\prob(X_n|X_{n+1:T},Y_{0:T})=\prob(\underbrace{X_n|Y_{0:T}}_{\mathclap{Z}},\underbrace{X_{n+1}}_{W})
	\end{equation*}
	by virtue of the fact that $X_n|X_{n+1:T}\perp X_{n+2:T},Y_{n+1:T}$. By Bayes' theorem:
	\begin{align*}
		\prob(Z|W)&\propto\prob(Z)\prob(W|Z)\\
		&\propto\prob(X_n|Y_{0:n})\prob(X_{n+1}|X_n,Y_{0:n})\\
		&=\prob(X_n|Y_{0:n})\prob(X_{n+1}|X_n)
	\end{align*}
	Since $X_{n+1}|X_n$ is independent from everything. $\prob(X_n|Y_{0:n})$ is the \textbf{filtering distribution}, the conditional law of $X$ given the information of past and present data, while $\prob(X_{n+1}|X_n)$ is the transition probability of $X$. \\Define
	\[F_n(j)=\prob(X_n=j,Y_{0:n})\] so that
	\[
	\prob(X_n|Y_{0:n})=\frac{F_n(j)}{\sum_i F_n(i)}.
	\]
	Then
	\begin{align*}
		F_n(j)&=\prob(Y_{0:n-1},X_n=j,Y_n=y_n)\\
		&=\sum_i\prob(\underbrace{Y_{0:n-1},X_{n-1}=i}_{F_{n-1}(i)},X_n=j,Y_n=y_n)\\
		&=\sum_i F_{n-1}(i)\underbrace{\prob(X_n=j,Y_n=y_n|X_{n-1}=i,Y_{0:n-1})}_{\prob(X_n|X_{n-1})\prob(Y_n|X_n)}\\
		&=\sum_i F_{n-1}(i)p_{ij}p_j(y_n)\\
		&\implies F_n(j)+\Bigl(\sum_iF_{n-1}(i)p_{ij}\Bigr)p_j(y_n)
	\end{align*}
	So that these can be computed recursively.
	\begin{align*}
		F_0(j)&=\prob(X_0=j,Y_0)=\lambda_jp_j(y_0)\\
		F_1(j)&=\sum_i F_0(i)p_{ij}p_j(y_1)\\
		&=p_j(y_1)\sum_i\lambda_i p_i(y_0)p_{ij}
	\end{align*}
	If $S$ is finite, we can compute these quantities. Normalizing, we get the filtering distribution
	\[
	\prob(X_n=j|Y_{0:n})=\frac{F_n(j)}{\sum_i F_n(i)}.
	\]
	Moreover, if we consider the \textbf{marginal smoothing distribution}
	\begin{align*}
		\prob(X_n|y_{0:n})&=\prob(\underbrace{X_n|Y_{0:n}}_{Z},\underbrace{Y_{n+1:T}}_{W})\\
		&\propto\prob(Z)\prob(W|Z)\qquad\text{since }Y_{n+1:T}|X_n \independent Y_{0:n}\\
		&=\underbrace{\prob(X_n|Y_{0:n})}_{\mathclap{\text{filtering distribution}}}\overbrace{\prob(Y_{n+1:T}|X_n)}^{\mathclap{B_n(\cdot)}}.
	\end{align*}
	$B_n(\cdot)$ is the \textbf{cost-to-go} function: it measures the likelihood of the future observations given a state of $X$:
	\[B_n(j)+\prob(Y_{n+1:T}|X_n=j)\]
	with
	\[
	\prob(X_n|Y_{0:T})\propto F_n(j)B_n(j).
	\]
	Also, $B_n$ can be computed recursively as
	\[B_n(j)=\sum_i p_{ij}p_{j}(y_{n+1})B_{n+1}(j)\]
	starting from $T$ and proceeding backwards. Finally, with these we have
	\[
	\prob(Y_{0:T})=\sum_jF_n(j)B_n(j)
	\]
	which represents the likelihood of the observations, which in principle can be maximized.
	\subsection{General state space}
	Assume for simplicity that $S\subset\R$ or $\R^k$ (uncountable). Define a time-homogeneous transition probability from 
	\[\prob(x,A):=\prob(X_{n+1}\in A|X_n=x),\qquad A\in\mathscr{B}(S).\]
	If this has a density with respect to some dominant measure $\nu$, we call
	\[p=\frac{dP}{d\nu}\]
	the \textbf{transition density}.\\
	For example, if $\nu$ is the Lebesgue measure,
	\[\prob(x,A)=\int_A \prob(x.y)dy\].
	\begin{definition}
		A Markov Chain o $S$ uncountable is said to be $\mathbf{\varphi}$\textbf{-irreducible} if there exists a $\sigma$-finite measure $\varphi$ on $S$ such that $\forall A\in\mathscr{B}(S)$ with $\varphi(A)>0$ and for all $x\in S\;\exists\geqslant1,\;n=n(x,A)$ such that $p^{(n)}(x,A)>0$.
	\end{definition}
	\begin{definition}
		A $\varphi$-irreducible Markov Chain is said to be \textbf{Harris Recurrent} if $\forall A \in \mathscr{B}(S)$ such that $\varphi(A)>0$ then $\prob(X_n\in A \text{ i.o.})=1$. It is called \textit{positive} if it admits an invariant distribution $\pi$.
	\end{definition}
	$\pi$ is invariant if $\int_A\pi(dx)p(x,dy)=\pi(A)$.
	\begin{theorem}
		Let $X$ be aperiodic and positive Harris recurrent. Then
		\[d_{TV}(\lambda P^n,\pi)\xrightarrow[n\to\infty]{}0
		\]
		where
		\begin{itemize}
			\item $\displaystyle\lambda P^n(A)=\int\limits_SP^n(x,A)\lambda(dx)$
			\item $\displaystyle d_{TV}(\lambda,\mu)=\sup\limits_{A\in\mathscr{B}}|\prob(V\in A)-\prob(W\in A)|$ if $V\sim\lambda$ and $W\sim\mu$.
		\end{itemize}
		If, in addition, $f:S\to\R$ is $\pi$-integrable
		\[
		\frac{1}{N}\sum_{i=1}^Nf(X_n)\xrightarrow[N\to\infty]{}\int_Sf(x)\pi(dx)
		\]
		with probability 1.
	\end{theorem}
	\begin{definition}
		Let $X$ be aperiodic and positive Harris recurrent. Then
		\[d_{TV}(\lambda P^n,\pi)\xrightarrow[n\to\infty]{}0
		\]
		where
		\begin{itemize}
			\item $\displaystyle\lambda P^n(A)=\int\limits_SP^n(x,A)\lambda(dx)$
			\item $\displaystyle d_{TV}(\lambda,\mu)=\sup\limits_{A\in\mathscr{B}}|\prob(V\in A)-\prob(W\in A)|$ if $V\sim\lambda$ and $W\sim\mu$.
		\end{itemize}
		If, in addition, $f:S\to\R$ is $\pi$-integrable
		\[
		\frac{1}{N}\sum_{i=1}^Nf(X_n)\xrightarrow[N\to\infty]{}\int_Sf(x)\pi(dx)
		\]
		with probability 1.
	\end{definition}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[->,node distance=2cm]
			\node[circle, draw](c1){$C_1$};
			\node[circle,draw](c2)[right of=c1]{$C_2$};
			\node[](dots)[right of=c2]{$\ldots$};
			\node[circle,draw](cd)[right of=dots]{$C_d$};
			\path (c1) edge (c2);
			\path (c2) edge (dots);
			\path (dots) edge (cd);
			\path (cd) edge [bend left] (c1);
		\end{tikzpicture}
		\label{dperiod}
	\end{figure}
	\section{Monte-Carlo integration}
	We are interested in evaluating 
	\begin{equation*}
		\mu_f := \mathbb{E}_{\pi}  [f(X)] = \int_S f(x) \pi(dx)
	\end{equation*}
	as an integration problem.
	If $S$ is discrete this takes a more familiar form $\sum_{i \in S}f(i)\pi_i$ \\
	The problems can arise from the fact that$\pi$ can be multidimensional, and possibly unmormalized. Some example of these situations may be:
	\begin{itemize}
		\item $f(x) = 1$ $\implies \mu_f$ is normalizing constant.
		\item If $f$ is the identity we are calculating the mean of $\pi$.
		\item If $f(x) = \mathbbm{1}_A(x) $ then tail probability and confidence intervals. 
	\end{itemize}
	We can also be interested in finding 
	\begin{equation*}
		argmax_{x \in S} \pi (x) 
	\end{equation*}
	and this is an optimization problem, like for instance the mode of a distribution of interest. \\
	Often an analytical computation is unfeasible and thus we need to use an approximation. Deterministic approximation include:
	\begin{itemize}
		\item Riemann integration: given a partition of $S$ through points  $x_1, \dots, x_n$ then 
		\begin{equation*}
			\sum_{i=1}^n f(x_i) \pi (x_i) (x_i+x_{i-1}) \rightarrow \int f d\pi
		\end{equation*}
		as $sup_i |x_i+x_{i-1}| \rightarrow 0$.picture \\
		potentially ineficient especially in high dimention.
		\item Laplace approximation: use a Gaussian kernel to approximate a function of interest centered at the mode.
	\end{itemize}
	\begin{example}
		Let $f$ be unimodal:
		\begin{equation*}
			\int f (x) dx = \int e^{h(x)} dx 
		\end{equation*}
		take $h(x)= log f(x)$ and take a taylor of $h$ around the mode of $f$
		\begin{equation*}
			h(x)  \approx h (x_0) + \underbrace{h'(x_0)(x-x0)}_{f'/f|_{x_0}=0}+ 1/2h''(x_0)(x-x0) manca 
		\end{equation*}
		negative because we are at the mode
		\begin{equation*}
			\int f(x) dx \approx e ^{h(x_0)} \int  e ^{1/2 |h''(x_0)|(x-x0)} dx=
		\end{equation*}
		multiply and divide by $c:=\sqrt{2 \pi |h''(x_0)|^{-1}}$ and we have 
		\begin{equation*}
			=f(x_0) c \int \mathcal{N}(x; x_0, |h''(x_0)|^{-1})dx
		\end{equation*}
		this approximation is only good around the mode, but it loses a great part of its accuracy when we depart from the centre of the distribution: this makes it particularly bad for approximating, for instance, tails of a distribution. Moreover, if we work in high dimension more problems arise. $f$ needs to be unimodal, else this approach can fail (e.g. mixture models that came up a lot in statistics)     
	\end{example}
	
	Deterministic approximations typically do not exploit information about the shape of the distribution of interest and this naturally leads to \enf{stochastic approximation}. 
	\subsection{The Monte Carlo principle}
	the main idea consists in exploiting information about $\pi$ and concentrate resources where they are most useful. From a general point of view simulate from $\pi$ and compute an approximation of the functional of interest. 
	
	\begin{algorithm}
		\enf{General strategy:}
		\begin{itemize}
			\item sample $X_1, \dots, X_n \stackrel{iid}{\sim}\pi$. \\
			($N=$ MC sample size )
			\item $\mu_N = \frac{1}{N} \sum_{i=1}^N f(X_i) $\\
		\end{itemize}
	\end{algorithm}
	Note that if $\pi_N:= \frac{1}{N} \sum_{i=1}^N \delta_{X_i}$ then we have just 
	\begin{equation*}
		\mu_N = \int f(x) \pi_N (dx) = \frac{1}{N} \sum_{i=1}^N\int f(x)\delta_{X_i}  (dx) = \frac{1}{N} \sum_{i=1}^N f(X_i)
	\end{equation*}
	now 
	\[
	\mathbb{E}_\pi\left[\mu_N\right]=\frac{1}{N}\sum_{i=1}^{N}\mathbb{E}_\pi\left[f(x_i)\right]=\int f(x)\pi(dx)=\mu_f
	\]
	so $\mu_N$ is unbiased. Moreover, if $\mathbb{E}_\pi(f)<\infty$,
	\[
	\mu_N\xrightarrow{a.s.}\mu_f\qquad\text{by SLLN.}
	\]
	If $\mathbb{E}_{\pi} (f^2) < \infty$, set 
	\begin{equation*}
		\sigma_f ^2 := Var [f(X)] = \mathbb{E}_{\pi} [(f(X)- \mu_f)^2]
	\end{equation*}
	then $\mu_N$ has variance 
	\begin{equation*}
		\sigma_N ^2= Var(\mu_N) = \frac{\sigma_f ^2}{N}
	\end{equation*}
	so $\mu_N$ is a constant estimator of $\mu_f$. We also have a central limit theorem
	\begin{equation*}
		\sqrt{N} (\mu_N-\mu_f) \xrightarrow{d} \mathcal{N}(0, \sigma_f ^2)
	\end{equation*}
	to be used, e.g., for constructing asymptotic confidence intervals. 
	Some potential issues are:
	\begin{itemize}
		\item it may be computationally expensive;
		\item sampling directly from $\pi$ could be unfeasible and/or too difficult (e.g. high-dimensional distribution, distributions whose normalizing constant is unknown...).
	\end{itemize}
	example. $Z \in \mathcal{N}(0,1)$. We are interested in 
	\begin{equation*}
		\mu= \mathbb{P}(Z> 5) = \int _{\mathbb{R}}\underbrace{\mathbbm{1}(x >5)}_{\mathclap{f(x)}} \varphi (z) dz
	\end{equation*}
	The value for $\mu$ is around $2.87\times10^{-7}$. Now draw $X_i \stackrel{i.i.d.}{\sim} \mathcal{N}(0,1)$
	\begin{equation*}
		\mu_N = \frac{1}{N} \sum_{i=1}^N \mathbbm{1}(x_i >5) \rightarrow \mu
	\end{equation*}
	This means that as long as we draw $N<\mathcal{O}(10^7)$ we expect to see a positive indicator... 0 times. This is an example of situation where simulating $\pi$ is computationally infeasible. To face this limitation, we turn to a very important idea: the \textbf{rejection sampling}. 
	\subsection{Rejection Sampling}
	\begin{algorithm}
		until $N$ points are saved repeat:
		\begin{itemize}
			\item draw independently $Z \sim Unif (a,b)$ and $ Y \sim Unif(0,M) $;
			\item if $y \leq \pi(z)$, set $X=z$.
		\end{itemize}
	\end{algorithm}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}%[scale=0.8]
			\begin{axis}[
				xlabel=\empty,
				x axis line style={opacity=100},
				ylabel=\empty,
				xmin=0, xmax=1200,
				ymin=0, ymax=100,
				axis y line=left,
				y axis line style={opacity=100},
				ytick={50,90},
				xtick={100,550,1100},
				xticklabels={$a$,\textcolor{RedViolet}{z},$b$},
				yticklabels={\textcolor{Dandelion}{y},$M$},
				axis x line*=bottom
				]
				\addplot[smooth] coordinates {
					(100,0)
					(200,50)
					(350,30)
					(500,85)
					(689,58)
					(800,70)
					(1100,0)
				};
				\draw [dashed] (0,90) -- (1200,90);
				\draw [RedViolet,dash dot] (550,0) -- (550,50);
				\draw [Dandelion,dash dot] (0,50) -- (550,50);
				\node[align=left] at (990,40) {$\pi$};
				\node[circle,fill,inner sep=2pt] at (550,50) {};
			\end{axis}
		\end{tikzpicture}
		\label{rejsampl}
	\end{figure}
	Only points under $\pi $ are kept, and these are draws from $\pi$. Cdf of accepted points:
	\begin{equation*}
		\begin{split}
			\mathbb{P}(Z \leq x| Y \leq \pi(Z))&= \frac{\mathbb{P}(Z \leq x, Y \leq \pi(Z))}{\mathbb{P}( Y \leq \pi(Z))}\\
			&= \frac{\mathbb{P}(Z \leq x, Y \leq \pi(Z))}{\mathbb{P}( Z \leq b, Y \leq \pi(Z))}\\
			&= \frac{\int_a^x \int_0 ^{\pi(Z)}\frac{1}{M}dy \frac{1}{b-a}dz} {\int_a^b \int_0 ^{\pi(Z)}\frac{1}{M}dy \frac{1}{b-a}dz}\\
			&= \int_a^x \pi(Z) dz
		\end{split}
	\end{equation*}
	so now we can compute  $\mu_N = \frac{1}{N} \sum_{i=1}^N f(X_i) $. 
	
	If the support of $\pi$ is unbounded, we cannot draw from $Unif(a,b)$, the idea is that we are gonna use a non uniform bound and an auxiliary distribution. \\
	Assume:
	\begin{itemize}
		\item $\exists M>0$ and a density $q$ such that
		\begin{equation*}
			\pi (x) \leq Mq(x)
		\end{equation*}
		\item we can draw from $q$, meaning that we should choose a $q$ we can simulate relatively easily.
	\end{itemize}
	\begin{algorithm}
		until $N$ points are saved:
		\begin{itemize}
			\item draw $Z\sim q$, with $Y|Z=z\sim Unif(0, Mq(z))$;
			\item if $Z\leqslant\pi(z)$, set $x=Z$.
		\end{itemize}
	\end{algorithm}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}%[scale=0.8]
			\begin{axis}[
				xlabel=\empty,
				x axis line style={opacity=100},
				ylabel=\empty,
				xmin=-1, xmax=1500,
				ymin=-1, ymax=100,
				axis y line=left,
				y axis line style={opacity=100},
				ytick={35},
				xtick={800},
				xticklabels={\textcolor{Dandelion}{$z$}},
				yticklabels={\textcolor{Periwinkle}{$y$}},
				axis x line*=bottom
				]
				\addplot[smooth,restrict x to domain=50:1300] coordinates {
					(100,10)
					(400,30)
					(550,85)
					(740,58)
					(850,70)
					(1050,20)
					(1300,10)
				};
				\addplot[smooth,dashed,restrict expr to domain={(x>=-50)&&(x<=100)||(x>=1300)&&(x<=1550)}{1:1}] coordinates {
					(-50,5)
					(100,10)
					(400,30)
					(550,85)
					(740,58)
					(850,70)
					(1050,20)
					(1300,10)
					(1550,5)
				};
				\addplot[smooth, RedViolet, name path=M] coordinates {
					(-50,25)
					(200,50)
					(550,90)
					(700,95)
					(800,90)
					(1100,50)
					(1550,25)
				};
				\node[align=left] at (1050,40) {$\pi$};
				\node[align=left,RedViolet] at (1100,70) {$Mq$};
				\path [name path=A] (axis cs: 800, 0) -- (axis cs: 800, 100);
				\path [name intersections={of=A and M}];
				\draw [dotted, thick, Dandelion] (800,0) -- (intersection-1);
				\node [circle,Dandelion,fill,inner sep=2pt,label=right:{$Mq(z)$}] at (intersection-1) {};
				\draw [dash dot,Periwinkle] (0,35)--(800,35);
				\node [circle,fill,inner sep=2pt] at (800,35) {};
			\end{axis}
		\end{tikzpicture}
		\label{res2}
	\end{figure}
	Verify $(-\infty\leqslant a < b \leqslant\infty)$=\[
	=\dfrac{\displaystyle\int_{-\infty}^{x}\int_0^{\pi(z)}\frac{1}{\cancel{Mq(z)}}dy\,\cancel{q(z)}dz}{\displaystyle\int_{-\infty}^{\textcolor{red}{\infty}}\int_0^{\pi(z)}\frac{1}{\cancel{Mq(z)}}dy\,\cancel{q(z)}dz}=\mathbb{P}(X\leqslant x)\
	\]
	\begin{example}
		take $\mu=\mathbb{P}(z>5)=2.87\cdot10^{-7}$. We need a non-uniform bound:
		\[
		\begin{tikzpicture}[scale=0.7]
			\begin{axis}[
				xlabel=\empty,
				x axis line style={opacity=100},
				ylabel=\empty,
				xmin=5, xmax=10,
				ymin=0, ymax=1,
				axis y line=left,
				y axis line style={opacity=100},
				ytick={1.4487},
				xtick={5},
				xticklabels={5},
				yticklabels={$\phi(5)$},
				axis x line*=bottom
				]
				\addplot[domain=0:150, RedViolet, ultra thick,smooth] {e^(-x*0.1)-0.2};
				\addplot[domain=0:150, black, ultra thick,smooth,dotted] {e^(-x*0.2+1)};
				\node[align=left] at (9,0.5) {$Mq$};
				\node[align=left, RedViolet] at (9,0.27) {$\pi$};
			\end{axis}
		\end{tikzpicture}
		\]
		Choose $q(x)=e^{-(x-5)}\mathbbm{1}(x>5)$ with $M=\phi(5)$. We know that
		\[\mu_N=\% \text{ of accepted points}\times\underbrace{\text{area of graph where we generate values}}_{\int_5^\infty Mq(x)dx=M\int_5^\infty e^{-(x-5)}d=M}\]
		This algorithm is generally more efficient than naive Monte-Carlo where we would have to generate $x_i\sim N(0,1)$ variables and keep $x_i>5$. It can be, anyway, costly: the acceptance probability over an interval $(c,d)$ is
		\begin{align*}
			\mathbb{P}(c\leqslant Z\leqslant d, Y\leqslant\pi(z))&=\int_c^d\int_0^{\pi(z)}\frac{dy}{Mq(z)}q(z)dz\\
			&=\frac{1}{M}\int_c^d\pi(z)dz.
		\end{align*}
		In general, over the support, the acceptance probability is $\frac{1}{N}$. So, in order to have $N$ points, we need to generate a number of points equal to
		\[N'\sim Neg-Bin\Bigl(N,\frac{1}{M}\Bigr).\]
		\[\frac{1}{M}\int_5^\infty\pi(x)dx=\frac{2.87\cdot10^{-7}}{1.49\cdot10^{-6}}=0.19\]
	\end{example}
	\subsection{Importance Sampling}
	This idea starts with a little analytical trick, in general terms the idea is to use all generated samples. Unlike the accept-reject method. \\
	\begin{equation*}
		\begin{split}
			\mu&= \mathbb{E}_{\pi}[f(x)] = \int f(x) \pi(x) dx = \int f(x) \frac{\pi(x)}{q(x)} q(x) dx \\
			&= \mathbb{E}_{q}[f(x)\frac{\pi(x)}{q(x)}]
		\end{split}
	\end{equation*}
	where $q$ is a density whose support include that of $\pi$.\\ (i.e. $q(x)=0$ $\implies$ $\pi(x)=0$)\\
	Then we can:
	\begin{itemize}
		\item draw $X_i \stackrel{\text{i.i.d.}}{\sim} q$ 
		\item assign weight $w(x_i):= \frac{\pi(X_i)}{q(X_ i)}$ to $X_i$. ($w$ importance weight)
		\item  set 
		\begin{equation*}
			\mu_N= \frac{1}{N} \sum_{i=1}^N f(X_i)w(X_i)
		\end{equation*}
	\end{itemize}
	Then
	\begin{equation*}
		\mathbb{E}_q[\mu_N]= \mathbb{E}_q [f(x)w(x)] \stackrel{above}{=} \mu 
	\end{equation*}
	moreover
	\begin{equation*}
		\mu_N \xrightarrow{a.s.} \mu \hspace{0.5 cm} \text{ as } n \rightarrow + \infty
	\end{equation*}
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
				xlabel=0,
				every axis x label/.style={
					at={(ticklabel* cs:-0.01)},
					anchor=east,
				},
				ylabel=\empty,
				xmin=0, xmax=150,
				ymin=0, ymax=50,
				axis y line=left,
				y axis line style={opacity=0},
				ytick=\empty,
				xtick={60,100}, 
				xticklabels={$x'$,$x$},
				xtick style={draw=black}, 
				axis x line*=bottom
				]
				\addplot[RedViolet,smooth,name path=p] coordinates {
					(5,0)
					(20,15)
					(30,13)
					(40,20)
					(50,25)
					(60,19)
					(70,22)
					(80,15)
					(90,17)
					(115,0)
				};
				\addplot[black,smooth,name path=q] coordinates {
					(-50,0)
					(40,1)
					(90,30)
					(140,1)
					(240,0)
				};
				\path[name path=l1](60,0)--(60,50);
				\fill [name intersections={of=p and l1}] (intersection-1) coordinate (a) node[circle,fill,inner sep=1.5pt,label=above:\footnotesize$\pi(x')$]{};
				\draw[gray](60,0)--(a); 
				\path[name path=l2](100,0)--(100,50);
				\fill [name intersections={of=q and l2}] (intersection-1) coordinate (b) node[circle,fill,inner sep=1.5pt,label=above:\footnotesize$Q(x)$]{};
				\fill [name intersections={of=p and l2}] (intersection-1) coordinate (c) node[circle,fill,inner sep=1.5pt,label={[yshift=-2pt]\footnotesize$\pi(x)$}]{};
				\draw[gray](100,0)--(c);
				\draw[gray,dotted,thick](b)--(c);
			\end{axis}
		\end{tikzpicture}
		\label{impsamp}
	\end{figure}
	\begin{itemize}
		\item down-weight importance of $x$\\
		$\pi (x) < q(x) \implies w(x) = \frac{\pi (x)}{q(x)}< 1$. 
		\item $w(x')>1$ up-weight of $x'$.
	\end{itemize}
	draws in relevant regions are up-weighed automatically and no draws are wasted. The choice of $q$ is more flexible than in the AR method.\\ 
	\textbf{Toy Example.} $\mathbb{P}(Z>5) = \mu= 2.87 \times 10 ^{-7}$\\\
	Use 
	\begin{equation*}
		q(x)= e ^{-(x-5)} \mathbbm{1}_{\{x>5\}},
	\end{equation*}
	and so 
	\begin{equation*}
		\mu = \int_5^{\infty} \phi(x) dx= \int_{\mathbb{R}} \mathbbm{1}_{\{x>5\}} \phi(x) dx = \int_{\mathbb{R}} \underbrace{\mathbbm{1}_{\{x>5\}}}_{= f(x)} \underbrace{\frac{\phi(x)}{q(x)}}_{=w(x)} q(x) dx
	\end{equation*}
	so $X_i \stackrel{\text{i.i.d.}}{\sim} q $.
	\begin{equation*}
		\mu_N= \frac{1}{N} \sum_{i=1}^N f(X_i)w(X_i) = \frac{1}{N} \sum_{i=1}^N \underbrace{\mathbbm{1}_{X_i>5} }_{\text{since } X_i \sim q }\frac{\phi(x_i)}{q(x_i)}  = \frac{1}{N} \sum_{i=1}^N\frac{\phi(x_i)}{q(x_i)} 
	\end{equation*}
	Additional upside:\\
	if $\pi= \frac{\tilde{\pi}}{z}$ with $z$ unknown. Then
	\begin{equation*}
		\mu = \int f(x) \pi (x) dx = \int f(x)\frac{\tilde{\pi(x)}}{z q(x)} q(x) dx 
	\end{equation*}
	\begin{equation*}
		\rightarrow \ \ z= \int \tilde{\pi(x)} dx = \int \frac{\tilde{\pi(x)}}{q(x)}q(x) dx 
	\end{equation*}
	so setting $\tilde{w}(x)=\frac{\tilde{\pi(x)}}{q(x)}$ we have 
	\begin{equation*}
		\mu = \underbrace{\frac{\int f(x) \Tilde{w}(x) q(x) dx }{\int \Tilde{w}(x) q(x) dx}}_{\text{self-normalizing estimate}}
	\end{equation*}
	Hence for $X_i \stackrel{\text{i.i.d.}}{\sim} q $ we now have 
	\begin{equation*}
		\Tilde{\mu}_N = \frac{\frac{1}{N} \sum_{i=1}^N f(X_i)\Tilde{w}(X_i)}{\frac{1}{N} \sum_{j=1}^N \Tilde{w}(X_j)} = \sum_{i=1}^N  f(X_i) \hat{w}(X_i)
	\end{equation*}
	with $\hat{w}(X_i)= \frac{\Tilde{w}(X_i)}{\sum_{j=1}^N \Tilde{w}(X_j)}$. Strong law of large numbers still applies. \\
	\subsection{Markov-Chain Monte-Carlo}
	When:
	\begin{itemize}
		\item Tails of $q$ versus $\pi$ can be very different so that $w(x)$ possibly unbounded
		\item $q$ may be difficult to identify in high-dimensions
	\end{itemize}
	Idea: we can generate draws from $\pi$ using Markov Chains, obtaining what is called \textbf{Markov Chains Monte Carlo}.\\
	Given a target distribution $\pi$, we want to construct an ergodic Markov chain $X$ with stationary distribution $\pi$, and use its trajectory to get draws from $\pi$. \\
	Assume for now $P$ is ergodic. \\ Given an intial state $X_0$, we can in principal simulate trajectory :\\
	$\forall \ n \geq 0 $, if $X_n = i$, draw $X_{n+1}=j$ with probability $p_{ij}$
	from ergodicity we know 
	\begin{equation*}
		\exists n_0 \in \mathbb{N}: X_n \sim \pi \ \ \forall n \geq n_0
	\end{equation*}
	Such $n_0$ is called Mixing time, meant as an order of magnitude, not as a step. (e.g. $n_0=O(10^4)$)\\
	Studying mixing times is typically difficult. Usually one uses convergence test or diagnostics.
	(there is literature, Gelman-Rubin criterion).\\
	Suppose we know $n_0$. Then 
	\begin{itemize}
		\item simulate $\{X_n ^{(i)}, n \leq n_0\}$ for $i=1, \dots, N$.
		\item set $Y_i = X_{n_0+i}$ (sample path endpoint).
	\end{itemize}
	Then $(Y_i, \dots, Y_N)$ is the MCMC sample. These are i.i.d $\sim \pi$ (assume $X_0^{(i)}$ independent).\\
	$\implies$ we are back t0 the Monte-Carlo scenario with 
	\begin{equation*}
		\mu_N = \frac{1}{N}\sum_{i=1}^N f(Y_i) \xrightarrow{a.s.} \mu_f
	\end{equation*}
	where 
	\begin{equation*}
		Var(\mu_N)= \frac{\sigma_f^2}{N}, \  \  \ \sigma_f^2= Var_{\pi}(f(Y)). 
	\end{equation*}
	However we are discarding  $N=n_0$ sample. If, for example, $N= 10^4$, $n_0= 10^5$ $\implies 10^9$, which is expensive.\\
	alternatively, we can think of using a single chain . \\
	\begin{algorithm}[\textbf{Generic MCMC strategy}]
		Given $N$:
		\begin{itemize}
			\item simulate $\{X_n, n\}$
			\item  for $n_0$ large enough, set 
			\begin{equation*}
				\hat{Y}_i= X_{n_0+i}, \ \ i = 1,\dots,N 
			\end{equation*}
		\end{itemize}
		\begin{equation*}
			\implies \mu_N = \frac{1}{N}\sum_{i=1}^N f(\hat{Y}_i)\approx \mu_f 
		\end{equation*} 
	\end{algorithm}
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
				axis lines=middle,
				unit vector ratio*=1 1 1,
				axis line style={->,very thick},
				xmin=-0.5,xmax=10,ymin=-0.5,ymax=5,
				xlabel=$n$,
				ylabel=$y$,
				xtick={0,4.5},
				ytick=\empty,
				xticklabels={0,$n_0$}
				]
				\addplot[smooth] coordinates {
					(0,2)
					(0.4,1.9)
					(0.7,1.8)
					(1.1,2.3)
					(1.4,1.9)
					(2,1.5)
					(2.4,1.6)
					(2.7,2.1)
					(3.1,2.4)
					(3.6,3)
					(3.9,2.6)
					(4,2)
					(4.5,1.7)
					(4.8,2.4)
					(5.1,2.6)
					(5.7,2.7)
					(6.2,3)
					(6.9,3.5)
					(7.2,3.3)
					(7.6,3.9)
					(8.1,3.8)
					(8.6,4.1)
					(9,4.5)
					(9.5,4.9)
				};
				\addplot [
				only marks,
				mark=ball,
				mark size=2pt,
				ball color=Dandelion,
				point meta=explicit symbolic,
				nodes near coords,
				every node near coord/.append style={xshift=0.15cm, yshift=-0.6cm},
				axis on top
				] coordinates {
					(4.5,1.7) [$Y_1$]
					(4.8,2.4) [$Y_2$]
					(5.7,2.7) [$Y_3$]
					(6.9,3.5)
					(7.6,3.9)
					(8.1,3.8)
					(8.6,4.1)
					(9.5,4.9) [$Y_N$]
				};
				\draw [Black,dotted,thick] (4.5,0) -- (4.5,1.7);
				\draw [dotted,thick,RedViolet] (4.4,0.55) -- (0.1,0.55) node[RedViolet,midway,above]{\footnotesize burn-in period};
				\draw [dotted,thick,RedViolet] (4.4,0.55) -- (4.4,0);
				\draw [dotted,thick,RedViolet] (0.1,0.55) -- (0.1,0);
				\draw [dotted,thick,Dandelion] (4.6,0.55) -- (9.5,0.55) node[Dandelion,midway,above]{\footnotesize MCMC sample};
				\draw [dotted,thick,Dandelion] (4.6,0.55) -- (4.6,0);
				\draw [dotted,thick,Dandelion] (9.5,0.55) -- (9.5,0);
			\end{axis}
		\end{tikzpicture}
		\label{burnin}
	\end{figure}
	Only $n_0$ values wasted. %it's a strong statement man. 
	The MCMC samples are now correlated and the MCMC estimator is now no longer a generic sample average but it is an ergodic average. In terms of theoretical result we have the ergodic theorem that guarantees 
	\begin{equation*}
		\frac{1}{N}\sum_{i=1}^N f(\hat{Y}_i) \xrightarrow{a.s.} \mu_f = \mathbb{E}_{\pi}[f(Y)]
	\end{equation*}
	for any initial distribution.\\ Understand what we are losing, consider the variance of $\hat{\mu}_N$ 
	\begin{equation*}
		Var\Bigg(\frac{1}{N}\sum_{i=1}^N f(\hat{y}_i)\Bigg) = \frac{1}{N^2}\sum_{i=1}^N\Bigg[Var\Big(f(\hat{y}_i)\Big)+2\sum_{k=1}^{N-1}Cov\Big(\underbrace{f(\hat{y}_i,f(\hat{y}_{i+k})}_{\mathclap{\text{autocovariance function}}}\Big)\Bigg]
	\end{equation*}
	at equilibrium, $Var$ and $Cov$ do not depend on time but only on the lag.  In particular $Var(f(\hat{Y}_i))= \sigma_f^2$. 
	\begin{equation*}
		%Ha ripreso la roba di prima
		\frac{1}{N^2}  \sum_{i=1}^N \left[\sigma_f^2 + 2 \sigma_f^2\underbracket{\sum_{k=1}^{N-i}\frac{Cov(f(\hat{Y}_i),f(\hat{Y}_{i+k} )}{\sigma_f^2}}_{\mathclap{\gamma_k=Corr\left(f(\hat{y}_i,f(\hat{y}_{i+k})\right)}}\right] \approx \frac{\sigma_f^2}{N}(1+\sum_{k\geq 1}\gamma_k)
	\end{equation*}
	This is higher the the variance of the MCMC estimator with $N$ chains (in fact it is a MC estimator). The quantity 
	\begin{equation*}
		\Tilde{N} = \frac{N}{1 +2\sum_{k\geq 1}\gamma_k }
	\end{equation*}
	is called \textbf{effective sample size}. Indeed
	
	\begin{align*}
		&\frac{Var(\hat{\mu}_N)}{Var(\mu_N)} = \frac{\sigma_f^2/\Tilde{N}}{\sigma_f^2/N} \\
		\implies &ESS= N \frac{Var(\hat{\mu}_N)}{Var(\mu_N)}  \in [0,N] 
	\end{align*}
	
	so the $ESS$ reppresent the size of an iid smaple with the same variance of the MCMC sample, giving a measure of the loss of efficiency determined by using a correlated sample. \\
	Here we are hoping $\gamma_k$ decays fast. Other wise the common practice is to perform what is called thinning, that is setting 
	\begin{equation*}
		\hat{Y}_i = X_{n_0+ih } \hspace{0.5 cm} i = 1, \dots, N
	\end{equation*}
	for a chosen $h \in \mathbb{N}$. This lowering the correlation among succesive samples. \\
	In fact debated practice 
	\begin{itemize}
		\item now we discard a higher number of samples.
		\item it is belived that keeping all sample after the burn-in yield's a better approximation of $\mu_f$.
	\end{itemize}
	Observation even if are correlated are pieces of information so don't waste them, on the other hand maybe you want to remove correlation. 
	\subsection{Metropolis-Hastings algorithm}
	The \textbf{Metropolis-Hastings algorithm} has been selected among the 10 most important algorithms of the 20th century. 
	
	The idea consists in running a chain with arbitrary transition matrix, but sometimes suppressing transition (in a "right way"). Given a target $\pi$ on $S$, let:
	\begin{itemize}
		\item [-]$Q$ be an aribtrary, irreducible transition matrix called \textit{proposal matrix};
		\item [-]$A$ be a matrix with entries $a_{ij}\in[0,1]$ called \textit{acceptance matrix}.
	\end{itemize}
	\begin{algorithm}[\textbf{Generic Metropolis-Hastings}]
		For $n\geqslant 1$, if $X_{n-1}=i$:
		\begin{itemize}
			\item [-] draw $j$ with probability $q_{ij}$;
			\item [-] set $X_n=j$ with probability $a_{ij}$; else set $X_n=X_{n-1}$
		\end{itemize}
	\end{algorithm}
	$Q$ provides proposal states: from row $i$ draw state $j$, $A$ tells us if we should go in $j$ or stay where we are. The resulting transitions are:
	\[
	p_{ij}=\begin{cases}
		q_{ij}a_{ij} &j\neq i\\
		1-\sum_{k\in S}  q_{ik}a_{ik} &j=i.
	\end{cases}
	\]
	\begin{itemize}
		\item[$\rightarrow$]$Q=(q_{ij})_{i,j}\in S$ is aribtrary (which means we can choose a distribution easy to simulate from);
		\item[$\rightarrow$]$P=(p_{ij})_{i,j\in S}$ is irreducible since $Q$ is aperiodic and since $X_n=X_{n-1}$ with positive probability.
	\end{itemize}
	So if we show $P$ has invariant distribution $\pi$, then it is positive recurrent and the ergodic theorem applies: $\pi$ is therefore also the equilibrium distribution. \\
	Idea: occasionally suppress transitions to state that are less likely "with respect to $\pi$", so to speak.
	\begin{proposition}
		The Metropolis Hastings algirthm with proposal matrix $Q$ and acceptance probabilities
		\[
		a_{ij}=\min\bigg\{1,\frac{\pi_jq_{ji}}{\pi_iq_{ij}}\bigg\}
		\]
		generates a \textbf{reversible chain} with respect to $\pi$.
	\end{proposition}
	\begin{proof2}
		We need to show that $\pi_ip_{ij}=\pi_jp_{ji}$ (detailed balance for the resulting chain).
		\begin{align*}
			\pi_ip_{ij}&=\pi_iq_{ij}a_{ij}\\
			&=\pi_iq_{ij}\min\bigg\{1,\frac{\pi_jq_{ji}}{\pi_iq_{ij}}\bigg\}=\\
			&=min\bigg\{\pi_iq_{ij},\pi_jq_{ji}\bigg\}.
		\end{align*}
		But since minimum is symmetric and \[
		min\big\{\pi_jq_{ji},\pi_iq_{ij}\big\}=min\big\{\pi_iq_{ij},\pi_jq_{j1}\big\}.
		\]
		starting from $\pi_jq_{ji}$ yields the same term, so $\pi_i p_{ij}=\pi_jp_{ji}$.
	\end{proof2}
	\begin{algorithm}
		\textbf{Metropolis-Hastings Algorithm.} For $n\geqslant1$, if $X_{n-1}=i$:
		\begin{itemize}
			\item [-] draw $J=j$ with probability $q_{ij}$;
			\item [-] draw $U\sim Unif(0,1)$;
			\item [-] if $U<\min\left\{1,\frac{\pi_jq_{ji}}{\pi_iq_{ij}}\right\}$ set $X_n=j$; else $X_n=X_{n-1}$
		\end{itemize}
	\end{algorithm}
	This algorithm has important features:
	\begin{itemize}
		\item the proposal distribution is fully arbitrary, with the sole condition of being irreducible. This leaves us a lot of freedom.
		\item it applies even if the normalizing constant of $pi$ is unknown, since
		\[
		\frac{\pi_j}{\pi_i}=\frac{\tilde{\pi}_j/\cancel{z}}{\tilde{\pi}_i/\cancel{z}}=\frac{\tilde{\pi}_j}{\tilde{\pi}_i}
		\]
	\end{itemize}
	\begin{example}
		\[
		\pi(\cdot)=cPois(\cdot;\lambda_1)+(1-c)Pois(\cdot;\lambda_2)
		\]
		is a mixture of Poisson distributions. We want to reconstruct this target using ergodic average. Propose states with a simple MC: we will use a $B\&D\left(\frac{1}{2},\frac{1}{2}\right)$ (i.e. a symmetric random walk on $\Z_+$).
		\begin{align*}
			\implies &q_{ij}=\frac{1}{2}\qquad\text{for }j=(i\pm1)^+\\
			& a_{ij}=\min\{1,\alpha_{ij}\}\qquad\alpha_{ij}=\frac{\pi_jq_{ji}}{\pi_iq_{ij}}\\
			\implies &\alpha_{ij}=\frac{cPois(j;\lambda_1)+(1-c)Pois(j;\lambda_2)}{cPois(i;\lambda_1)+(1-c)Pois(i;\lambda_2)}\cdot\frac{\cancel{1/2}}{\cancel{1/2}}
		\end{align*} This last equality is true for $j=(i\pm1)^+$ and arbitrary for all other $j$s, which are not going to be proposed wince we use a random variable. The code can be found in appendix. %\ref{poiscode}.
		The random walk is not positive recurrent, but Metropolis-Hastings "corrects" the trajectory so that it basically becomes recurrent.
	\end{example}
	The previous example refers to a typical class of Metropolis-Hastings algorithms called \textit{random walk Metropolis-Hastings}, where
	\[
	X_n=X_{n-1}+Z_n
	\]
	Where $Z_n$ has symmetric distribution around zero. The example provided, in particular, is a special case called \textbf{Metropolis} algorithm, where the proposal is symmetric so that
	\[
	q_{ij}=q_{ji}\implies a_{ij}=\min\left\{1,\frac{\pi_j}{\pi_i}\right\}.
	\]
	So:
	\begin{itemize}
		\item if $\pi_j<\pi_i$ we accept the step with probability $<0$;
		\item if $\pi_j>\pi_i$ we accept the step with probability 1.
	\end{itemize}
	
	We can interpret this consequence of the algorithm as the fact that if the density of the new step increases, we accept the step and we continue exploring that direction; otherwise, we still have a chance to accept the new steps even if it leads us to a zone with lower density. Of course, if we only accepted steps that increase the density we would basically have an optimization algorithm that maximizes probability density (which is not what we want).
	\begin{example}
		\[
		\pi(\cdot)=cN(\cdot;\mu_1,\sigma_1^2)+(1-c)N(\cdot;\mu_2,\sigma_2^2).
		\]
		We can use a random walk Metropolis-Hastings that proposes
		\[
		Y=X+Z\qquad Z\sim N(0,\sigma_0^2)
		\]
		if $X_{n-1}=X$, and $\alpha_{ij}$ is now
		\[
		\alpha(x,y)=\frac{\pi(y)q(x|y)}{\pi(x)q(y|x)}
		\]
		where $q(y|x)=N(y;x,\sigma_0^2)$. They are symmetric, so
		\[
		\alpha(x,y)=\frac{\pi(y)}{\pi(x)}
		\]
	\end{example}
	Another special case is the \textit{independent Metropolis-Hastings}, where the proposal does not depend on the current state, i.e.
	\[
	q_{ij}=q_j\implies a_{ij}=\min\left\{1,\frac{\pi_jq_i}{\pi_iq_j}\right\}=\min\left\{1,\frac{\pi_j/q_j}{\pi_i/q_i}\right\}.
	\]
	Proposals are accepted with probability 1 if
	\[
	w_j=\frac{\pi_j}{\pi_j}>\frac{\pi_i}{\pi_i}=w_i\qquad\text{(it's a reweighing)}
	\]
	Analogies:
	\begin{itemize}
		\item with \textit{importance sampling}: $w(x)=\frac{\pi(x)}{q(x)}$ is the importance weight, i.e. we accept with probability 1 if $w_j>w_i$ (which means there has been an improvement in importance weights).
		\item with \textit{rejection sampling}: propose $z=j$ with probability $q_j$ and accept if $y\leqslant\pi_j$ where $y|z=j\sim Unif(0,Mq)$ which is the same as \[
		U<\frac{\pi_j}{Mq_j},\qquad U\sim Unif(0,1)
		\]
		that here becomes $U<\frac{\pi_j/q_j}{\pi_i/q_i}$ instead of $\frac{\pi_j}{Mq_j}$.
	\end{itemize}
	\subsection{Gibbs sampling}
	Gibbs sampling is formally a special case of Metropolis-Hastings but it found wider applications. It is useful with:
	\begin{itemize}
		\item multivariate $\pi$;
		\item models with latent variables;
		\item models specified using conditionals.
	\end{itemize}
	Gibbs sampling needs at least a bivariate space. Let $\pi=\pi_{X,Y}$ density on $S\times S$. We cannot sample directly from $\pi$ but we can sample from the conditionals $\pi_{X|Y}$ and $\pi_{Y|X}$.
	\begin{algorithm}[\textbf{Two-component Gibbs sampler}]
		Given $(X_{n-1},Y_{n-1})=(x,y)$ as our current state,
		\begin{itemize}
			\item [-] draw $X'\sim\pi_{X|Y}(\cdot|Y)$ 
			\item [-] draw $Y'\sim\pi_{X|Y}(\cdot|X)$
			\item [-] set $(X_n,Y_n)=(X',Y')$
		\end{itemize}
	\end{algorithm}
	Why is this useful? If $(X,Y)\sim\pi_{x,y}$ this is equivalent to say that
	\[
	Y\sim \pi_y \;\text{(marginal distribution)},\quad X|Y\sim\pi_{X|Y}\;\text{(chain rule)}
	\]
	but if $X'\sim \pi_{X|Y}$ then 
	\[
	(X',Y)\sim\pi_{X,Y}
	\]
	which is obvious, since we drew it that way. The same holds for $Y'$, so these transitions preserve the joint distribution $\pi_{X,Y}$ which is therefore invariant.
	\begin{proof2}
		The first step of the algorithm can be seen as a Metropolis-Hastings step with proposal on $S^2=S\times S$:
		\[
		q=(X',Y'|X,Y)=\pi_{X'|Y'}(X'|y)\mathbbm{1}(Y'=y).
		\]
		In practice, we fix $Y$ and then we update $X$. The acceptance rate is $\min\left\{1,\frac{\pi_jq_{ji}}{\pi_iq_{ij}}\right\}$. If we factorize in marginals and conditional we get:
		\[
		\frac{\textcolor{red}{\pi(X'Y')}\textcolor{blue}{q(X,Y|X',Y')}}{\textcolor{Dandelion}{\pi(X,Y)}\textcolor{OliveGreen}{q(X',Y'|X,Y)}}=\frac{\textcolor{red}{\pi(Y')\pi(X'|Y')}\textcolor{blue}{\pi(X|Y')\mathbbm{1}(Y=Y')}}{\textcolor{Dandelion}{\pi(Y)\pi(X|Y)}\textcolor{OliveGreen}{\pi(X'|Y)\mathbbm{1}(Y=Y')}}
		\]
		but since we imposed $Y=Y'$, all these factors cancel out:\[
		=\frac{\textcolor{red}{\cancel{\pi(Y')\pi(X'|Y')}}\textcolor{blue}{\cancel{\pi(X|Y')}\cancel{\mathbbm{1}(Y=Y')}}}{\textcolor{Dandelion}{\cancel{\pi(Y)\pi(X|Y)}}\textcolor{OliveGreen}{\cancel{\pi(X'|Y)}\cancel{\mathbbm{1}(Y=Y')}}}=1.\]
	\end{proof2}
	So every step of the Gibbs samples is made of 2 mini Metropolis-Hastings steps with probability of acceptance 1: we never reject any data. If we are only interested in $X$ univeriates we can sometimes augment the state space to $S\times S'$ through can auxiliary variable $Y\in S'$ paradoxally introduced to ease the computation. The transition for $X$ (marginal) can be written, integrating $Y$ out, as \[
	p_X(x'|X)=\int_{S'}\pi_{Y|X}(y|x)\pi_{X|Y}(x'|y)dy.
	\]
	Stationarity requires, by the global balance conditions:
	\[
	\int_S\pi_X(x)p_X(x'|X)dx=\pi_X(x').
	\]
	We have
	\[
	\int_{S}\pi_{X}(x)p_X(x'|X)dx=\int_S\pi_X(x)\left(\int_{S'}\pi_{Y|X}(y|X)\pi_{X|Y}(x'|Y))dx\right)dy.
	\]
	Using Fubini's theorem
	\begin{align*}
		&\int_{S'} \pi_{X|Y}(x'|Y)\int_S\underbracket{\pi_{Y|X}(y|X)\pi_X(X)}_{\pi_{X,Y}(X'Y)}dxdy\\
		&\int_S \underbracket{\pi_{X|Y}(x'|Y)\pi_{Y}(y)}_{\pi_{X,Y}(X',Y)}dy=\pi_X(x')
	\end{align*}
	A Gibbs sampler on $(X_n,Y_n)$ yields, marginally, a stationary $X_n$ chain with respect to $\pi_X$.
	If we are interested in $X$, $Y$ can be sometimes an auxiliary variable: \\
	\begin{itemize}
		\item we run the Gibbs sampler on $(X_n, Y_n)$
		\item discard $Y_n$
		\begin{equation*}
			\begin{split}
				\implies \ \ X_n (\text{ at stationarity })\\
				\text{is from }\pi_x = \int \pi_{X,Y}(x,y)dy
			\end{split}
		\end{equation*}
	\end{itemize}
	\begin{example}
		In a popular Bayesian model we have 
		\begin{equation*}
			\pi_X (x) \propto x ^{\alpha+k-1} e ^{-\beta x} \frac{\Gamma(x)}{\Gamma(x+n)}, \hspace{0.5 cm } x \geqslant 0.
		\end{equation*}
		Normalizing and simulating from $\pi$ is not trivial, due to the presence of gamma functions. However, 
		\begin{equation*}
			\frac{\Gamma(x)\Gamma(n)}{\Gamma(x+n)}=\int_0^1 y^{x-1}(1-y)^{n-1}\dif y
		\end{equation*}
		so we can formulate a joint model on the augmented state space $\mathbb{R}_+ \times [0,1]$.
		\begin{align*}
			\pi_{X,Y}(x,y)&\propto x ^{\alpha+k-1} e ^{-\beta x}\overbracket{y^{x-1}(1-y)^{n-1}}^{\text{Beta kernel}}\\
			&\implies\int_0^1 \pi_{X,Y}(x,y)\dif y=\pi_X(x)
		\end{align*}
		So we have 
		\begin{itemize}
			\item for fixed $x$, $\pi(y|x)= Beta(x,n) $
			\item  for fixed $y$,
			\begin{align*}
				\pi (x|y) &\propto x ^{\alpha+k-1} x ^{\alpha+k-1} e ^{-\beta x} \underbrace{e ^{\ln(y^x)}}_{\mathclap{= y^x}}\\
				&= \underbracket{x ^{\alpha+k-1} e ^{-(\beta x -\ln y)x}.}_{\mathclap{x^ae^{-bx}\text{: Gamma distribution, once we fix }y}}
			\end{align*}
			So the Gibbs sampler runs 
			\begin{itemize}
				\item $X|Y \sim Gamma(\alpha+k, \beta- \ln y)$
				\item $Y|X \sim Beta(x,n)$
			\end{itemize}
			and the just discards discard the value of $Y$.
		\end{itemize}
	\end{example}
	More generally, let $\pi (x)= \pi(x_1, \dots, x_d)$ be a density on $S^d$, $d \geqslant 2$. \\
	Denote $x_{(-i)}=(x_1, \dots,x_{i-1}, x_{i+1}, \dots,  x_d) $. Assume we know how to sample from the full conditional distributions 
	\begin{equation*}
		\pi(x_i |x_{(-i)})
	\end{equation*}
	The $i^{th}$ component Gibbs sampler updates $X_i$ and leaves the other coordinates unchanged, so 
	\[
	\begin{rcases*}
		(X_1,\ldots,X_i,\ldots,X_d)\sim\pi\\
		X_i'\sim\pi(x_i |x_{(-i)})
	\end{rcases*}\implies (X_i,\ldots,X_i',\ldots,X_d)\sim\pi.
	\]
	So $\pi$ is invariant and this is a Metropolis-Hastings step with proposal 
	\begin{equation*}
		q(x'|x) = \pi ((x_i |x_{(-i)}) \mathbbm{1}(x'_{(-i)}= x_{(-i)})
	\end{equation*}
	and the acceptance probability is $1$.
	
	The full Gibbs Sampler typically reads:
	\begin{algorithm}[\textbf{Random Scan Gibbs Sampler}]
		Given $X_n = x \in S^d$:
		\begin{itemize}
			\item draw $i$ with probability $p_i $ ($\sum^d p_i = 1 $);
			\item draw $X_i'\sim \pi_{X_i|X_{(-i)}}(\cdot|x_{(-i)} )$;
			\item set $X_{n+1}=(x_1, \dots, x_{i-1},  x_{i-1}, x_{i+1}, \dots, x_d )$.
		\end{itemize}
	\end{algorithm}
	If $p_i >0$ for all $i=1, \dots, d$, this algorithm produces a reversible chain with respect to $\pi$. This Gibbs sampling is so powerful that the cases in which we should \textit{not} use it are very specific. 
	\subsection{Slice sampler}
	Let $\pi_X$ be a density on $S$.
	\begin{lemma}\label{slizer}
		Let $A$ be the area under $\pi_x$, i.e. 
		\begin{equation*}
			A= \{(x,y) \in S \times \mathbb{R}_+: 0 \leqslant y\leqslant \pi_X(x)\}
		\end{equation*}
		If $(X,Y)$ has uniform distribution on $A$, then $X \sim \pi_x $
	\end{lemma}
	\[
	\begin{tikzpicture}[scale=0.70]
		\pgfdeclarepatternformonly{ildiocane}%
		{\pgfqpoint{-1pt}{-1pt}}%
		{\pgfqpoint{10pt}{10pt}}%
		{\pgfqpoint{9pt}{9pt}}%
		{
			\pgfsetlinewidth{0.4pt}
			\pgfpathmoveto{\pgfqpoint{0pt}{0pt}}
			\pgfpathlineto{\pgfqpoint{9.1pt}{9.1pt}}
			\pgfusepath{stroke}
		}
		
		\begin{axis}[
			xlabel=\empty,
			ticks=none,
			x axis line style={->,opacity=100},
			ylabel=\empty,
			xmin=0, xmax=1200,
			ymin=0, ymax=100,
			axis y line=left,
			y axis line style={->,opacity=100},
			axis x line*=bottom
			]
			\addplot[smooth,name path=A] coordinates {
				(100,0)
				(200,50)
				(350,30)
				(500,85)
				(689,58)
				(800,70)
				(900,30)
				(1000,40)
				(1100,0)
			};
			\path[name path=B] (axis cs:\pgfkeysvalueof{/pgfplots/xmin},0) -- (axis cs:\pgfkeysvalueof{/pgfplots/xmax},0);
			
			\addplot+[draw,pattern=ildiocane,pattern color=RedViolet]
			fill between[
			of=A and B,
			soft clip={domain=0:1},
			];
			\node[align=left] at (990,45) {$\pi$};
			\node[fill=white,inner sep=2pt] at (650,30) {$A$};
		\end{axis}
	\end{tikzpicture}
	\]
	\begin{proof2}
		If (X,Y) is uniform on A we have $\pi_{X,Y}(x,y)\propto\mathbbm{1}_{0\leqslant y\leqslant\pi(x)}$ whose normalizing constant is
		\[
		\int_S\int_0^\infty\mathbbm{1}_{0\leqslant y\leqslant\pi(x)}\dif y \dif x =\ldots=1.\]
		Then the marginal of $X$ is
		\begin{align*}
			\int_0^\infty \pi_{X,Y}(x,y) \dif y&= \int_o^\infty \mathbbm{1}_{0\leqslant y\leqslant\pi(x)}\dif y\\
			&=\int_0^{\pi_X(x)}\dif y=\pi_X(x)
		\end{align*}
	\end{proof2}
	AR methods are based on this result. 
	How does this connect with Markov-Chain Monte-Carlo? The main idea is that we can use  a Markov Chain whose equilibrium distribution is a uniform on $A$, then discarded $Y$ and keep $X$. The target is \[\pi_{X|Y}(x,y) \alpha 1_{0 \leqslant y \leqslant \pi(x)} \] and we need to construct a Markov Chain that is ergodic with respect to this target.  We can think a Gibbs sampler that alternate the draws from the two condition:
	\begin{itemize}
		\item $Y'|X\sim \pi_{Y|X}(y|x)\propto\mathbbm{1}_{0\leqslant y\leqslant\pi(x)}$ ($x$ is fixed: this step sets the height of the \textit{vertical slice});
		\item $X'|Y\sim \pi_{X|Y}(x|y)\propto\mathbbm{1}_{0\leqslant y\leqslant\pi(x)}$ ($y$ is fixed: this step sets the length of the \textit{horizontal slice}).
	\end{itemize}
	\begin{figure}[H]
		\centering
		
		\begin{tikzpicture}[every label/.style={align=left}]
			\begin{axis}[
				xlabel=\empty,
				xtick={400,850},
				xticklabels={\textcolor{yellow!70!black}{$x'$},\textcolor{Turquoise!80!black}{$x$}},
				x axis line style={->,opacity=100},
				ytick={35},
				yticklabels={\textcolor{Periwinkle}{$y'$}},
				ylabel=\empty,
				xmin=0, xmax=1200,
				ymin=0, ymax=100,
				axis y line=left,
				y axis line style={->,opacity=100},
				axis x line*=bottom
				]
				\addplot[smooth,name path=A] coordinates {
					(100,0)
					(200,50)
					(350,30)
					(500,85)
					(689,58)
					(800,70)
					(900,30)
					(1000,40)
					(1100,0)
				};
				\node[align=left] at (990,45) {$\pi$};
				\path[name path=vert1](850,0)--(850,100);
				\fill [name intersections={of=A and vert1}] (intersection-1) coordinate (a) node[circle,fill,Turquoise!80!black,inner sep=1.5pt,label=left:\textcolor{Turquoise!80!black}{\footnotesize$\pi_X(x)$}]{};
				\draw[Turquoise!80!black] (850,0) -- (a);
				\path[name path=hor1](0,35)--(1200,35);
				\fill [name intersections={of=A and hor1}] 
				(intersection-1) coordinate (b) 
				(intersection-2) coordinate (c) 
				(intersection-3) coordinate (d)
				(intersection-4) coordinate (e)
				(intersection-5) coordinate (f)
				(intersection-6) coordinate (g)
				;
				\draw[Periwinkle](b)--(c);
				\draw[Periwinkle](d)--(e) node[circle,midway,above]{\footnotesize$y'$};
				\draw[Periwinkle](f)--(g);
				\draw[Periwinkle,dashed](0,35)--(b);
				\fill[name intersections={of=vert1 and hor1}] (intersection-1) node[circle,fill,Periwinkle,inner sep=1.5pt]{};
				\draw[yellow!70!black,dashed](400,0)--(400,35) node[circle,fill,inner sep=1.5pt]{};
				\node[circle,draw,RedViolet,inner sep=1.5pt,scale=0.7]at(818,31){2};
				\node[circle,draw,RedViolet,inner sep=1.5pt,scale=0.7]at(430,3.5){3};
			\end{axis}
			\node[circle,draw,RedViolet,inner sep=1.5pt,scale=0.7]at(5.15,-0.2){1};
			\begin{scope}[scale=0.8, every node/.append style={transform shape}]
				\node[circle,draw,RedViolet,inner sep=1.5pt,scale=1,label=right:{select an \textcolor{Turquoise!80!black}{$x$};}](lol) at(8,6){1};
				\node[circle,draw,RedViolet,inner sep=1.5pt,scale=1,label=right:{select a \textcolor{Periwinkle}{$y'$} ranging from \textcolor{Turquoise!80!black}{0} to\\ \textcolor{Turquoise!80!black}{$\pi_X(x)$}, as long as it is \textit{below} $\pi$;}](lmao)[below=20pt of lol]{2};
				\node[circle,draw,RedViolet,inner sep=1.5pt,scale=1,label=right:{select a \textcolor{yellow!70!black}{$x'$} ranging from the \\segments of the \textcolor{Periwinkle}{$y'$} slice that are \textit{below} $\pi$;}](kek)[below=20pt of lmao]{3}; 
				\node[circle,draw,RedViolet,inner sep=1.5pt,scale=0.8,label=right:{}](dotz)[below=19pt of kek]{$\ldots$};
			\end{scope}
		\end{tikzpicture}
		
		\label{slicesampler}
	\end{figure}
	So these are reversible step with respect to $\pi_{X,Y}$ and at equilibrium we have uniform sample on A: then using the lemma \ref{slizer}, $X$ is from $\pi_X$.
	\begin{example}
		
		$S=\mathbb{R}_+, \hspace{0.5 cm} \pi(x)=\frac{1}{2}e^{-\sqrt{x}}, x > 0$.
		\begin{equation*}
			\pi^{-1}(y) = (\log2y)^2
		\end{equation*}
		so the slice sampler is 
		\begin{itemize}
			\item $Y'|x \sim$ Unif $(0,\frac{1}{2}e^{-\sqrt{x}} )$
			\item $X'|y' \sim$ Unif $(0,(\log2y))^2$
		\end{itemize}
		
		$\pi(x) \propto e^{-\sqrt{x}}, x \in \mathbb{Z}_+$ unnormalized.
		\begin{equation*}
			\pi^{-1}(y) = (\log(y))^2
		\end{equation*}
		\begin{itemize}
			\item $Y'|x \sim$ Unif $(0,e^{-\sqrt{x}} )$
			\item $X'|y' \sim$ Unif$(\{0,1,\ldots, \lfloor{(\log(y))^2 \rfloor}\})$ 
		\end{itemize}
		Remember that 
		\begin{equation*}
			\left \lceil{x}\right \rceil =\max \{n \geqslant 0: n \leqslant x\}
		\end{equation*}
		If $\pi$ is d-dimensional, it is typically difficult to identify
		\begin{equation*}
			A_y = \{x: y \leqslant \pi(x)\}
		\end{equation*}
		If we can write 
		\begin{equation*}
			\pi(x) \propto \prod_{i = 1}^d \pi_i(x)
		\end{equation*}
		we can use $d$ auxiliary variables $y_1, \ldots, y_d$, so that 
		\begin{equation*}
			\pi_i(x) = \int_0^{\pi_i(x)}\dif y_i = \int \mathbbm{1}_{0 \leqslant y_i \leqslant \pi_i(x)} \dif y_i
		\end{equation*}
		so the augmented target is 
		\begin{equation*}
			\pi(x,y)=\pi(x_1, \ldots, x_d, y_1, \ldots, y_d) \propto \prod_{i = 1}^d \mathbbm{1}_{0 \leqslant y_i \leqslant \pi_i(x)} 
		\end{equation*}
		In the following graphs, the chain oscillates but stays stable around a certain level. Even if it seems not to converge, the oscillations are actually pretty stable and do not change throughout time. So the chain is convergent in the end.\\
		
		Sometimes convergence of the MCMC is deceptive, as one run exhibits convergence but multiple runs reveal differently. \\
		In these cases we can combine different strategies. \\
		For example, if the transition matrices $P'$ and $P''$ both have invariant $\pi$, the convex linear combination 
	\end{example}
	\begin{equation*}
		P = w P' + (1-w) P'', \hspace{1 cm} w \in (0,1)
	\end{equation*}
	has invariant $\pi$ (exercise). \\
	This is called a \enf{mixture transition}. This means that with probability w we use the matrix $P'$ and with compementary probability we use the matrix $P''$. \\
	For example, one could choose
	\begin{itemize}
		\item $P'$ as a RW-MH to explore locally
		\item $P''$ as an independent MH (that is, the proposal is independent on the current space: it is fixed) to explore globally 
	\end{itemize}
	so with w close to $1$, the chain once in a while takes a jump. This is useful to explore distribution with particularly low density areas that the chain will cross very seldom.\\
	%take attention vibes
	Example: local exploration and sometimes the chain takes a jump. 
	More generally
	\begin{equation*}
		P = \sum_{i = 1}^k w_i P:i, \hspace{1 cm} \sum_{i=1}^k w_i=1, \pi P_i = \pi
	\end{equation*}
	Another idea is to use a \enf{cycle transition}. 
	\begin{equation*}
		P=P' P''    
	\end{equation*}
	which leaves $\pi$ invariant (exercise), but not reversible in general.\\
	More generally, 
	\begin{equation*}
		P=P_1P_2 \ldots P_k
	\end{equation*}
	For example, the multicomponent Gibbs sampler (with deterministic visits to coordinates).\\
	
	%Esempio di un metodo con nome lungo
	This method aims to update $k$ coordinates; when one of them is difficult to update it uses a Metropolis step to do it. \\
	\includepdf[pages=-,pagecommand={}]{external/fake conv}
	Extensions and other methods:
	\begin{itemize}
		\item \textbf{Reversible jump MCMC}: it moves among spaces of different dimensions.  
		\item \textbf{Langevin algorithms} or \textbf{gradient-based MCMC}: is it inspired by the Langevin diffusion
		\begin{equation*}
			dX_t = \frac{1}{2} \frac{d}{dX_t} \log\pi(X_t)dt + dB_t
		\end{equation*}
		It uses information on the gradient of $\pi$ to move towards regions of higher density.
	\end{itemize}
	There is a principle (Goldilocks's principle of MCMC): the variance of the steps has to be not too large, not too small.
	\section{Continuous time Markov chains}
	\begin{definition}
		A continuous time Markov chain (CTMC) is a stochastic process $\{X(t), t \in T\}$ with index set $T=[0,+ \infty)$ taking values in a countable set $S$, s.t. 
		\begin{equation*}
			\mathbb{P}(X(t_n)=i_n | X(t_0)=i_0, \dots, X(t_{n-1})= x(t_{n-1}))
		\end{equation*}
		i.e. the Markov property holds for all $i_0, \dots, i_n \in S$ and $0\leqslant t_0 < \dots < t_n$. \\
		We can define the transition probability 
		\begin{equation*}
			p_{ij}(s,s+t):= \mathbb{P}(X(s+t)=j|X(s)=i)
		\end{equation*}
		for $t>0$, which we assume are time homogeneous, i.e. depend on $t$ not on $s$. Hence we can define 
		\begin{equation*}
			p_{ij}(t):= p_{ij}(0,t)
		\end{equation*}
		since $p_{ij}(s,s+t)= p_{ij}(0,t)$.\\
		Set \[p_{ij}(0):=\delta_{ij}=\begin{cases}
			1 &i=j\\
			0 &i\neq j.
		\end{cases}\]
	\end{definition}
	\begin{example}
		The Poisson process with intensity $\lambda>0 $ is defined by $X(0)=0$ a.s. and 
		\begin{equation*}
			p_{ij}(t)= \frac{(\lambda t)^{j-1}e^{-\lambda t}}{(j-i)!}, \hspace{0.5 cm} j \geqslant 1
		\end{equation*}
		and $0$ elsewhere. We see it is time-homogeneous on $S = \mathbb{Z}_+$. The increments are s.t.
		\begin{equation*}
			X(s+t)-X(s) \sim Poisson (\lambda t)
		\end{equation*}
		and if we set $s=0$, then $X(t) \sim Poisson(\lambda t)$.
	\end{example}
	\begin{exercise}
		Show that the transition function of a MTMC, together with an initial distribution, determines all joint distribution 
		\begin{equation*}
			\mathbb{P}( X(t_0)=i_0, \dots, X(t_{n})= x(t_{n}))
		\end{equation*}
		for all choice of $i_0, \dots, i_n$ and $t_o <\dots < t_n$. \\
	\end{exercise}
	Denote $P_t$ the square matrix with entries $\{p_{ij}(t)\_{i, j \in S}$. \\
	the family of $\{P_t\}_{t\geqslant 0}$ is called \enf{transition semigroup}, since:
	\begin{itemize}
		\item $\sum_{j \in S} p_{ij} = 1$: row sum
		\item $P_0=I$ (i.e. $p_{ij}(0):=\delta_{ij}$)
		\item $P_{t+s}= P_t P_s$:  \enf{semigroup property}, namely the Chapman-Kolmogorov's equations 
		\begin{equation*}
			p_{ij}(s+t)= \sum_{k \in S} p_{ik}(s) p_{kj}(t)
		\end{equation*}
	\end{itemize}
	We will assume $P_t$ is continuous at the origin (called \textit{standard})
	\begin{equation*}
		p_{ij}(t) \rightarrow \delta_{ij} \hspace{0.5 cm} t \rightarrow 0
	\end{equation*}
	which implies continuity at all $t>0$.\\
	$P_t$ is parametrized by time, and ideally we would like on object similar to the transition matrix for DTMC's, , relative to the temporal unit. We look at infinitesimal intervals. 
	\begin{definition}
		The matrix $Q$ given by 
		\begin{equation*}
			Q= \frac{d}{dt}P_t |_{t=0}
		\end{equation*}
		is called infinitesimal generator of the chain, and its entries $(q_{ij})_{i,j \in S}$ are called \enf{infinitesimal rates}.
	\end{definition}
	\begin{proposition}
		Let $P_t$ be a continuous transition semigroup on $S$ countable. For all $i,j$ the quantity $q_{ij}:=p'_{ij}(0)$ exists and 
		\[q_{ij} \in
		\begin{cases}
			[0,\infty) & j \neq i \\
			[-\infty, 0] & j = i \\
		\end{cases}
		\]
		For $j \neq i$
		\begin{equation*}
			q_[ij] = \lim_{h \rightarrow 0} \frac{p_{ij}(h) - \overbrace{p_{ij}(0)}^{0}}{h}
		\end{equation*}
		%In a small time interval there is a  negligibile probability that the chain goes to (?infinity)
	\end{proposition}
	\begin{itemize}
		\item  $\implies p_{ij}(h)=q_{ij}h + o(h)$ 
		\item  $\implies X$ goes to $j$ from $i$ with probability approximately $q_{ij}h $.
		\item  $\implies o(h)$ collects the error of approximation and all other events (for example, $X$) goes to $k$ before $j$).
	\end{itemize}
	So the chain has at most one jump at each instant. When $j=i$, since $p_{ii}(0) = 1$, the above calculation gives $q_{ii} \leqslant 0$. \\
	Sometimes we will denote $q_i = - q_{ii}$.
	\begin{definition}
		A state $i$ is called
		\begin{itemize}
			\item \enf{absorbing} if $q_i = 0$
			\item \enf{stable} if $0 < q_i < \infty$
			\item \enf{instantaneous} if $q_i = \infty$ 
		\end{itemize}
		If $\sup_{i \in S} q_i < \infty, Q$ is \textbf{stable}. \\
		If $q_i = \sum_{j \neq i} q_{ij}, \hspace{0.2 cm} \forall i \in S, Q $ is called \enf{conservative}.     
	\end{definition}
	\begin{remark}
		The absolute value of the diagonal entry equals the sum of the off-diagonal entries.
	\end{remark}\begin{equation*}
		1 = \sum_{j \in S} p_{ij}(h) = \sum_{j \neq i} p_{ij}(h) + p_{ii}(h)
	\end{equation*}
	which leads to 
	\begin{equation*}
		\frac{1-p_{ii}(h)}{h} = \sum_{j \neq i} \frac{p_{ij}(h)}{h}
	\end{equation*}
	Taking the limit as $h$ decreases to $0$, we get
	\begin{equation*}
		q_i = \lim_{h \downarrow 0} \sum_{j \neq i} \frac{p_{ij}(h)}{h}
	\end{equation*}
	If limit and sum exchange, we get 
	\begin{equation*}
		q_i = \sum_{j \neq i} q_{ij} \implies \sum_{j \in S} q_{ij} = 0
	\end{equation*}
	\begin{remark}
		Conservativity comes from the fact that transition probabilities conserve the mass.
	\end{remark}
	We assumed we had the transition probabilities are available but this is rarely the case. In many cases, when we move on to general and complex spaces, probabilities are specified through the generator (instead of the transition semigroup).
	We are going to argument later that this gives complete information about the process too, but if we have the generator, how can we check that the implied process is well defined? We need criteria to be checked on the generator. Often, only $Q$ is available, while $P_t$ is unknown. So, this identity relates to the conservation of probability mass by the transition semigroup. So it is enough to check that $Q$ row sums are null (to verify this condition). \\
	If $Q$ is stable and conservative, $q_i$ is interpreted as the rate of leaving the state $i$. So
	\begin{equation*}
		p_{ii}(h) = 1-q_i h + o(h) 
	\end{equation*}
	If 
	\begin{itemize}
		\item $q_i = 0$, then $X$ leaves $i$ at rate $0$, so $i$ is absorbing. 
		\item $q_i = \infty$, then $x$ leaves the state $i$ instantaneously. 
	\end{itemize}
	\begin{proposition}
		A CTMC with finite state space is always stable and conservative.
	\end{proposition}
	So we can reverse the statement, saying that trouble scenario happen when the chain is not stable or not  conservative.
	\begin{example}
		Consider $X$ on $\mathbb{Z}_+$ with 
		\begin{equation*}
			q_{i,i+1}= \lambda \hspace{0.5 cm} q_{i,i-1}= i \hspace{0.5 cm} q_{ii}= -(\lambda + i)
		\end{equation*}
		and $0$ elsewhere. $Q$ is conservative ($P_t$ would be well defined, if we knew it) but 
		\begin{equation*}
			\sup_{i \in S} q_i = \sup_{i \geqslant 0} \lambda + i = \infty
		\end{equation*}
		so it is not stable. \\
		The meaning of this distinction will be treated later.
	\end{example}
	\begin{definition}
		A general class of stable and conservative CTMC's is given by \textbf{regular jump processes} such that 
		\begin{itemize}
			\item their paths are piecewise constant: for almost all $\omega$ and all $t \geqslant 0, \exists \varepsilon = \varepsilon(t,\omega)$ such that 
			\begin{equation*}
				X(t+s,\omega) = X(t,\omega) \hspace{0.5 cm} \forall s \in [0,\varepsilon]
			\end{equation*}
			This means that the chain remains where it is for a positive amount of time. 
			\item they have finitely many points of discontinuity in every bounded time interval.
		\end{itemize}
	\end{definition}
	The typical behaviour is the one represented below. %metti figura. sì capo \\
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}%[scale=0.8]
			\begin{axis}[
				xlabel=\empty,
				x axis line style={->,opacity=100},
				ylabel=\empty,
				xmin=-1, xmax=100,
				ymin=-1, ymax=100,
				axis y line=left,
				y axis line style={->,opacity=100},
				ticks=none,
				axis x line*=bottom
				]
				\draw (0,20)--(30,20) node[circle,fill,RedViolet, pos=0,inner sep=2pt]{};
				\draw (30,60)--(50,60) node[circle,fill,RedViolet, pos=0,inner sep=2pt]{};
				\draw (50,30)--(85,30) node[circle,fill,RedViolet, pos=0,inner sep=2pt]{};
				\draw (85,40)--(100,40) node[circle,fill,RedViolet, pos=0,inner sep=2pt]{};
				\draw[dashed] (30,60)--(30,0);
				\draw[dashed] (50,60)--(50,0);
				\draw[dashed] (85,40)--(85,0);
			\end{axis}
		\end{tikzpicture}
		\label{regjump}
	\end{figure}
	For example, \enf{explosive chains} are not regular in the above sense: these chains have closer and closer jumps until there are infinitely many around $x=s$.%metti altra figura). fatto capo\\
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[
			declare function={a(\x)=99;},
			declare function={b(\x)=1;}
			]%[scale=0.8]
			\begin{axis}[
				xlabel=\empty,
				x axis line style={->,opacity=100},
				ylabel=\empty,
				xmin=-1, xmax=100,
				ymin=-1, ymax=100,
				axis y line=left,
				y axis line style={->,opacity=100},
				ytick=\empty,
				xtick={86},
				xticklabel={\textcolor{RedViolet}{s}},
				axis x line*=bottom
				]
				\draw (0,20)--(10,20) node[circle,fill, pos=0,inner sep=1.4pt]{};
				\draw (10,60)--(20,60) node[circle,fill, pos=0,inner sep=1.4pt]{};
				\draw (20,30)--(25,30) node[circle,fill, pos=0,inner sep=1.4pt]{};
				\draw (25,40)--(30,40) node[circle,fill, pos=0,inner sep=1.4pt]{};
				\draw (30,50)--(34,50) node[circle,fill, pos=0,inner sep=1.4pt]{};
				\draw (34,60)--(37,60) node[circle,fill, pos=0,inner sep=1.4pt]{};
				\draw (37,70)--(40,70) node[circle,fill, pos=0,inner sep=1.4pt]{};
				\draw (40,10)--(42,10) node[circle,fill, pos=0,inner sep=1.4pt]{};
				\draw (42,55)--(43,55) node[circle,fill, pos=0,inner sep=1.4pt]{};
				\draw (43,60)--(44,60) node[circle,fill, pos=0,inner sep=1.4pt]{};
				\addplot [black, domain=50:60, only marks, mark=*, samples=40, mark size=1.2]
				{0.5*(a(x)+b(x)) + 5*rand*(a(x)-b(x))};
				\addplot [black, domain=60:70, only marks, mark=*, samples=70, mark size=1.2]
				{0.5*(a(x)+b(x)) + 5*rand*(a(x)-b(x))};
				\addplot [black, domain=70:80, only marks, mark=*, samples=80, mark size=1.2]
				{0.5*(a(x)+b(x)) + 5*rand*(a(x)-b(x))};
				\addplot [black, domain=80:85, only marks, mark=*, samples=100, mark size=1.2]
				{0.5*(a(x)+b(x)) + 5*rand*(a(x)-b(x))};
				\draw[ultra thick,RedViolet](86,0)--(86,100);
			\end{axis}
		\end{tikzpicture}
		\label{xplchain}
	\end{figure}
	\begin{example}[Poisson Process]
		\begin{align*}
			&p_{ij}(t)=\frac{(\lambda t)^{j-i}e^{-\lambda t}}{(j-i)!}\qquad j\geqslant 1\\
			\implies &p_{ii}(t)=e^{-\lambda t}\\
			&p_{i,i+1}(t)=\lambda t e^{-\lambda t}\\
			&p_{i,i+k}(t)=\frac{(\lambda t)^{k}e^{-\lambda t}}{k!}\\
			\implies &q_{ii}=-\lambda,\quad q_{i,i+1}=\lambda,\qquad\text{and 0 elsewhere.}
		\end{align*}
		\textit{Interarrival times}: $T_i$ waiting time in $i$.
		\begin{align*}
			\mathbb{P}(T_0 > t) &= \mathbb{P}(X(t) = 0|X(0) = 0) = e^{- \lambda t}\\
			\implies \mathbb{P}(T_0 < t) &= 1-e^{-\lambda t} \implies T_0 \sim \text{Exp}(\lambda)
		\end{align*}
		So $\prob(T_i>t|T_0=t_0,\ldots,T_{i-1}=t_{i-1})=$
		\begin{align*}
			s &= t_0+\ldots+t_{i-1}\\
			&=\prob(x(s+t)=i|x(s)=i)=e^{-\lambda t}\\
			&\implies T_i\sim\text{Exp}(\lambda)
		\end{align*}
	\end{example}
	\begin{example}(Birth and Death processes)\\
		Denote B\&D$(\lambda _i, \mu_i)$ such that 
		\[q_{ij} = 
		\begin{cases}
			\lambda_i & j = i+1  \hspace{0.5 cm} i \geqslant 0 \\
			\mu_i  & j = i-1  \hspace{0.5 cm} i > 0 \\
			0 & \text{elsewhere} \\
		\end{cases}
		\]
		\begin{equation*}
			q_{ii} = - (\lambda _ i + \mu_i)
		\end{equation*}
		%figura arriva!!
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[->,node distance=2cm]
				\node[circle,draw](zero){0};
				\node [circle,draw] (one) [right of=zero] {1};
				\node [circle,draw] (two) [right of=one] {2};
				\node [circle,draw] (three) [right of=two] {3};    
				\node [circle,draw] (dots) [right of=three]{$\ldots$};
				\path (one) edge [bend left] node [above] {$\lambda_1$} (two);
				\path (two) edge [bend left] node [above] {$\lambda_2$} (three);
				\path (three) edge [bend left] node [below] {$\mu_3$} (two);
				\path (two) edge [bend left] node [below] {$\mu_2$} (one);
				\path (three) edge [bend left] node [below] {} (dots);
				\path (dots) edge [bend left] node [below] {} (three);
				\path (zero) edge [bend left] node [above] {$\lambda_3$} (one);
				\path (one) edge [bend left] node [below] {$\mu_1$} (zero);
			\end{tikzpicture}
			\label{bedcontinuous}
		\end{figure} whose invariant $\pi$ is the uniform on on $S_0$. Let $\tau$ be a finite random time on $\N$, independent of $Y$, such that:  
	\end{example}
	Special cases:
	\begin{itemize}
		\item $\mu_i=0 \forall i, \lambda_i > 0$ \textbf{Birth process} which can be 
		\begin{itemize}
			\item [*] Poisson
			\item [*] $\lambda_i = \underbrace{\lambda}_{\mathclap{\text{per capita birth rate}}}\cdot i $ \textbf{Yule process}
		\end{itemize}
		$\lambda_i = \lambda \cdot i + c$, adding immigration rate $c$.
		\item $\lambda_i = 0 \forall i, \mu_i > 0$ \textbf{Pure death process}
	\end{itemize}
	For example, if 
	\begin{equation*}
		\lambda_i = \lambda \cdot i, \hspace{0.5 cm} \mu_i = \mu \cdot i
	\end{equation*}
	the transition probabilities are very complicated to find, like in the following case: the process has $i = 1$
	\begin{equation*}
		p_{1j}(t) = (1-\mu \gamma) (1-\lambda \gamma) (\lambda \gamma)^{j-i}
	\end{equation*}
	where 
	\begin{equation*}
		\gamma= \frac{e^{(\lambda-\mu) t} - 1}{\lambda e^{(\lambda - \mu)t}-\mu}
	\end{equation*}
	If $X$ is specified only through the generator $Q$, a general task is to be able to describe the transition semigroup (which is difficult in general). \\
	A first tool to do so is given by the so called \textbf{Kolmogorov's equations}.
	\begin{theorem}
		Let $Q$ be stable and conservative. Then, $P_t$ and $Q$ satisfy:
		\begin{itemize}
			\item The \enf{backward equation}
			\begin{equation*}
				P'_t =  Q P_t \hspace{1 cm} p'_{ij}(t) = \sum_{k \in S} q_{ik}p_{kj}(t)
			\end{equation*}
			\item If 
			\begin{equation*}
				\sum_{k \in S} p_{ik}(t) q_k < \infty
			\end{equation*}
			the \enf{forward equation}
			\begin{equation*}
				P'_t = P_t Q \hspace{1 cm} p'_{ij}(t) = \sum_{k \in S} p_{ik}(t)q_{kj}
			\end{equation*}
		\end{itemize}
	\end{theorem}
	\begin{exercise}
		prove forward eq. for $S$ finite using CK and linearization.\\
		The intuition consists in thinking about the backwards equation.
		To obtain an intuition for the forward equation, let the second interval go to $0$.\\
		\\
		In principle (which boils down to simple cases), one could solve the Kolmogorov's equations to find the semigroup. \\
	\end{exercise}
	\textbf{Example} \\
	Consider the generator of the Poisson process:
	\begin{equation*}
		q_{i,i+1}=\lambda \hspace{1 cm} q_{ii} = - \lambda
	\end{equation*}
	Derive the transition probabilities from Kolmogorov's equations.
\end{document}


%--- NON FULL ---%