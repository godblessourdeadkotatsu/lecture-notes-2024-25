{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQz7bG1N4EFC",
        "outputId": "08bcef5e-7368-48c0-d593-6d1c204fef4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting it-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download it_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPU1awYw4QWO",
        "outputId": "49ac8462-4d99-4c76-c088-b18be4a603fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rMzWyraHP_Y"
      },
      "outputs": [],
      "source": [
        "class BigramCounter:\n",
        "    def __init__(self, external_vocab=None):\n",
        "        \"\"\"\n",
        "        Inizializza il contatore di bigrammi.\n",
        "        \"\"\"\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.vocab_size = 0\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.external_vocab = list(set(nlp.vocab.strings)) if external_vocab is None else external_vocab\n",
        "\n",
        "    def fit(self, data, threshold=2):\n",
        "        \"\"\"\n",
        "        Tokenizza il dataframe e calcola le frequenze relative dei bigrammi.\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame contenente il testo da analizzare\n",
        "            threshold: Soglia minima di frequenza per includere una parola nel vocabolario\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Contatore per tutte le parole\n",
        "        word_counts = Counter()\n",
        "\n",
        "        # Tokenizza i dati con word_tokenize di NLTK\n",
        "        data_tokenized = []\n",
        "        for text in data:\n",
        "            if pd.isna(text) or len(text.strip()) == 0:\n",
        "                continue\n",
        "\n",
        "            # Tokenizzazione\n",
        "            tokens = word_tokenize(text)\n",
        "\n",
        "            # Aggiorna conteggi\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "            data_tokenized.append(tokens)\n",
        "\n",
        "        # Filtra vocabolario in base alla soglia threshold\n",
        "        filtered_vocab = [word for word, count in word_counts.items() if count >= threshold]\n",
        "\n",
        "        # Aggiungi token speciali\n",
        "        filtered_vocab.extend([\"<s>\", \"</s>\", \"<UNK>\"])\n",
        "\n",
        "        # Crea mappature\n",
        "        self.word_to_idx = {label: i for i, label in enumerate(filtered_vocab)}\n",
        "        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n",
        "        self.filtered_vocab = filtered_vocab\n",
        "        self.vocab_size = len(filtered_vocab)\n",
        "\n",
        "        # Inizializza conteggi come matrici sparse\n",
        "        word_counts = Counter()\n",
        "        bigram_matrix = Counter()\n",
        "\n",
        "        # Training\n",
        "        # Conta unigrammi e bigrammi\n",
        "        for tokens in data_tokenized:\n",
        "\n",
        "\n",
        "            # Sostituisci parole rare con <UNK>, porcodio!\n",
        "            tokens_processed = [token if token in filtered_vocab else \"<UNK>\" for token in tokens]\n",
        "\n",
        "            # Aggiungi token di inizio e fine\n",
        "            tokens_processed.insert(0, \"<s>\")\n",
        "            tokens_processed.append(\"</s>\")\n",
        "\n",
        "            # Conta unigrammi\n",
        "            word_counts.update(tokens_processed)\n",
        "\n",
        "            # Conta bigrammi\n",
        "            bgrams = [(tokens_processed[i], tokens_processed[i+1]) for i in range(len(tokens_processed) - 1)]\n",
        "            bigram_matrix.update(bgrams)\n",
        "\n",
        "        # Salva i conteggi nelle variabili di istanza\n",
        "        self.unigram_counts = word_counts\n",
        "        self.bigram_matrix = bigram_matrix\n",
        "\n",
        "\n",
        "    def get_log_conditional_distribution(self, word):\n",
        "        \"\"\"\n",
        "        Restituisce la distribuzione di log-probabilità log(P(w|word)) per tutte le parole w.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Array di log-probabilità. Valori -inf indicano probabilità zero.\n",
        "        \"\"\"\n",
        "        # Ottieni il conteggio dell'unigramma per la parola precedente\n",
        "        w1_count = self.unigram_counts[word]\n",
        "\n",
        "        # Estrai la riga dalla matrice dei conteggi originali\n",
        "        row = np.array([self.bigram_matrix.get((word, self.idx_to_word[i]), 0) for i in range(self.vocab_size)])\n",
        "\n",
        "        # Applica la correzione di Laplace: (count + 1) / (total + vocab_size)\n",
        "        smoothed_probs = (row + 1) / (w1_count + self.vocab_size)\n",
        "\n",
        "        # Calcola il logaritmo delle probabilità\n",
        "        log_probs = np.log(smoothed_probs)\n",
        "\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "    def generate_text(self, max_length=30):\n",
        "        \"\"\"\n",
        "        Genera del testo utilizzando il modello di bigrammi.\n",
        "\n",
        "        Args:\n",
        "            max_length: Lunghezza massima del testo generato (default: 30 parole)\n",
        "\n",
        "        Returns:\n",
        "            tuple: (generated_text, total_score, step_scores)\n",
        "                - generated_text: Il testo generato\n",
        "                - total_score: Lo score totale (somma delle log-probabilità)\n",
        "                - step_scores: Lista di tuple (parola, log-probabilità) per ogni passo\n",
        "        \"\"\"\n",
        "        # Inizia con il token di inizio frase\n",
        "        if \"<s>\" not in self.word_to_idx:\n",
        "            raise ValueError(\"Token di inizio frase '<s>' non trovato nel vocabolario\")\n",
        "\n",
        "        # Inizializza la generazione\n",
        "        current_word = '<s>'\n",
        "        generated_tokens = []\n",
        "\n",
        "        # Inizializza lo score totale e i punteggi dei passi\n",
        "        total_score = 0.0\n",
        "        step_scores = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Ottieni la distribuzione delle probabilità per le parole successive\n",
        "\n",
        "            # Verifica che l'indice sia valido\n",
        "            if current_word not in self.filtered_vocab or current_word == \"<UNK>\":\n",
        "                # Distribuzione uniforme per indici non validi\n",
        "                uniform_prob = 1.0 / len(self.external_vocab)\n",
        "                log_probs = np.full(len(self.external_vocab), math.log(uniform_prob))\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0], p=np.exp(log_probs))\n",
        "                next_word = self.external_vocab[next_word_idx]\n",
        "            else:\n",
        "                log_probs = self.get_log_conditional_distribution(current_word)\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0], p=np.exp(log_probs))\n",
        "                next_word = self.idx_to_word[next_word_idx]\n",
        "\n",
        "            if next_word == \"<UNK>\":\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0])\n",
        "                next_word = self.external_vocab[next_word_idx]\n",
        "\n",
        "            # Ottieni la log-probabilità della parola scelta\n",
        "            log_prob = log_probs[next_word_idx]\n",
        "\n",
        "            # Aggiorna lo score totale\n",
        "            total_score += log_prob\n",
        "\n",
        "            # Aggiungi la parola e il suo score ai risultati\n",
        "            step_scores.append((next_word, log_prob))\n",
        "\n",
        "            # Se raggiungiamo il token di fine frase, interrompi\n",
        "            if next_word == \"</s>\":\n",
        "                break\n",
        "\n",
        "            # Aggiungi la parola al testo generato\n",
        "            generated_tokens.append(next_word)\n",
        "\n",
        "            # Aggiorna la parola corrente\n",
        "            current_word = next_word\n",
        "\n",
        "        # Unisci i token in un'unica stringa\n",
        "        generated_text = \" \".join(generated_tokens)\n",
        "\n",
        "        return generated_text, total_score, step_scores\n",
        "\n",
        "    def get_bgram_prob(self, prev_word, word):\n",
        "        \"\"\"\n",
        "        Restituisce la probabilità di P(word|prev_word).\n",
        "        \"\"\"\n",
        "        prob = self.bigram_matrix.get((prev_word, word), 0)\n",
        "        return (prob + 1) / (self.unigram_counts.get(prev_word, 0) + self.vocab_size)\n",
        "\n",
        "    def ppl(self, text):\n",
        "        # Tokenizzazione\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens.insert(0, \"<s>\")\n",
        "        tokens.append(\"</s>\")\n",
        "        ppl = 0.0\n",
        "        for i in range(len(tokens) - 1):\n",
        "            prev_word = tokens[i] if tokens[i] in self.filtered_vocab else \"<UNK>\"\n",
        "            word = tokens[i + 1] if tokens[i + 1] in self.filtered_vocab else \"<UNK>\"\n",
        "            ppl += math.log(self.get_bgram_prob(prev_word, word))\n",
        "\n",
        "        return math.exp(-ppl/len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN7JO8YTGZtb"
      },
      "outputs": [],
      "source": [
        "class TrigramCounter:\n",
        "    def __init__(self, bgram_model=None, external_vocab=None):\n",
        "        \"\"\"\n",
        "        Inizializza il contatore di trigrammi.\n",
        "        \"\"\"\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.vocab_size = 0\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.external_vocab = list(set(nlp.vocab.strings)) if external_vocab is None else external_vocab\n",
        "        self._bgram_model = bgram_model\n",
        "\n",
        "    def fit(self, data, threshold=2):\n",
        "        \"\"\"\n",
        "        Tokenizza i dati e calcola i conteggi dei trigrammi in modo efficiente.\n",
        "\n",
        "        Args:\n",
        "            data: Iterable di testi da analizzare\n",
        "            threshold: Soglia minima di frequenza per includere una parola nel vocabolario\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Contatore per tutte le parole\n",
        "        word_counts = Counter()\n",
        "\n",
        "        # Tokenizza i dati con word_tokenize di NLTK\n",
        "        data_tokenized = []\n",
        "        for text in data:\n",
        "            if pd.isna(text) or len(text.strip()) == 0:\n",
        "                continue\n",
        "\n",
        "            # Tokenizzazione\n",
        "            tokens = word_tokenize(text)\n",
        "\n",
        "            # Aggiorna conteggi\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "            data_tokenized.append(tokens)\n",
        "\n",
        "        # Filtra vocabolario in base alla soglia threshold\n",
        "        filtered_vocab = [word for word, count in word_counts.items() if count >= threshold]\n",
        "\n",
        "        # Aggiungi token speciali\n",
        "        filtered_vocab.extend([\"<s>\", \"</s>\", \"<UNK>\"])\n",
        "\n",
        "        # Crea mappature\n",
        "        self.word_to_idx = {label: i for i, label in enumerate(filtered_vocab)}\n",
        "        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n",
        "        self.filtered_vocab = filtered_vocab\n",
        "        self.vocab_size = len(filtered_vocab)\n",
        "\n",
        "        # Inizializza conteggi come matrici sparse\n",
        "        bigram_matrix = Counter()\n",
        "        trigram_matrix = Counter()\n",
        "\n",
        "        # Conta unigrammi e bigrammi\n",
        "        for tokens in data_tokenized:\n",
        "\n",
        "            # Sostituisci parole rare con <UNK>\n",
        "            tokens_processed = [token if token in filtered_vocab else \"<UNK>\" for token in tokens]\n",
        "\n",
        "            # Aggiungi token di inizio e fine\n",
        "            tokens_processed.insert(0, \"<s>\")\n",
        "            tokens_processed.insert(0, \"<s>\")\n",
        "            tokens_processed.append(\"</s>\")\n",
        "            tokens_processed.append(\"</s>\")\n",
        "\n",
        "            # Conta trigrammi\n",
        "            trigrams = [(tokens_processed[i], tokens_processed[i+1], tokens_processed[i+2]) for i in range(len(tokens_processed) - 2)]\n",
        "            trigram_matrix.update(trigrams)\n",
        "\n",
        "            # Conta bigrammi\n",
        "            bgrams = [(tokens_processed[i], tokens_processed[i+1]) for i in range(len(tokens_processed) - 1)]\n",
        "            bigram_matrix.update(bgrams)\n",
        "\n",
        "        # Salva i conteggi nelle variabili di istanza\n",
        "        self.trigram_matrix = trigram_matrix\n",
        "        self.bigram_matrix = bigram_matrix\n",
        "\n",
        "\n",
        "    def get_log_conditional_distribution(self, w1, w2):\n",
        "        \"\"\"\n",
        "        Restituisce la distribuzione di log-probabilità log(P(w | w1, w2)) per tutte le parole w.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Array di log-probabilità.\n",
        "        \"\"\"\n",
        "        # Ottieni il conteggio del bigramma di contesto\n",
        "        bigram_count = self.bigram_matrix.get((w1, w2), 0)\n",
        "\n",
        "        # Estrai la riga dalla matrice dei conteggi originali\n",
        "        row = np.array([self.trigram_matrix.get((w1, w2, w), 0) for w in self.filtered_vocab])\n",
        "\n",
        "        # Applica la correzione di Laplace: (count + 1) / (total + vocab_size)\n",
        "        smoothed_probs = (row + 1) / (bigram_count + self.vocab_size)\n",
        "\n",
        "        # Calcola il logaritmo delle probabilità\n",
        "        log_probs = np.log(smoothed_probs)\n",
        "\n",
        "        return log_probs\n",
        "\n",
        "    def generate_text(self, max_length=30):\n",
        "        \"\"\"\n",
        "        Genera del testo utilizzando il modello di trigrammi.\n",
        "\n",
        "        Args:\n",
        "            max_length: Lunghezza massima del testo generato (default: 30 parole)\n",
        "\n",
        "        Returns:\n",
        "            tuple: (generated_text, total_score, step_scores)\n",
        "                - generated_text: Il testo generato\n",
        "                - total_score: Lo score totale (somma delle log-probabilità)\n",
        "                - step_scores: Lista di tuple (parola, log-probabilità) per ogni passo\n",
        "        \"\"\"\n",
        "        # Inizia con i token di inizio frase\n",
        "        if \"<s>\" not in self.word_to_idx:\n",
        "            raise ValueError(\"Token di inizio frase '<s>' non trovato nel vocabolario\")\n",
        "\n",
        "        # Inizializza la generazione\n",
        "        w1, w2 = \"<s>\", \"<s>\"\n",
        "        generated_tokens = []\n",
        "\n",
        "        total_score = 0.0\n",
        "        step_scores = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Verifica che il contesto sia valido\n",
        "            if w1 not in self.filtered_vocab or w2 not in self.filtered_vocab or w1 == \"<UNK>\" or w2 == \"<UNK>\":\n",
        "                # Distribuzione uniforme per contesto ignoto\n",
        "                uniform_prob = 1.0 / len(self.external_vocab)\n",
        "                log_probs = np.full(len(self.external_vocab), math.log(uniform_prob))\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0], p=np.exp(log_probs))\n",
        "                next_word = self.external_vocab[next_word_idx]\n",
        "            else:\n",
        "                log_probs = self.get_log_conditional_distribution(w1, w2)\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0], p=np.exp(log_probs))\n",
        "                next_word = self.idx_to_word[next_word_idx]\n",
        "\n",
        "            if next_word == \"<UNK>\":\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0])\n",
        "                next_word = self.external_vocab[next_word_idx]\n",
        "\n",
        "            # Ottieni la log-probabilità della parola scelta\n",
        "            log_prob = log_probs[next_word_idx]\n",
        "\n",
        "            # Aggiorna lo score totale\n",
        "            total_score += log_prob\n",
        "\n",
        "            # Aggiungi la parola e il suo score ai risultati\n",
        "            step_scores.append((next_word, log_prob))\n",
        "\n",
        "            # Se raggiungiamo il token di fine frase, interrompi\n",
        "            if next_word == \"</s>\":\n",
        "                break\n",
        "\n",
        "            # Aggiungi la parola al testo generato\n",
        "            generated_tokens.append(next_word)\n",
        "\n",
        "            # Avanza la finestra\n",
        "            w1, w2 = w2, next_word_idx\n",
        "\n",
        "        # Unisci i token in un'unica stringa\n",
        "        generated_text = \" \".join(generated_tokens)\n",
        "\n",
        "        return generated_text, total_score, step_scores\n",
        "\n",
        "    def get_trigram_prob(self, w1, w2, w):\n",
        "        \"\"\"\n",
        "        Restituisce la probabilità di P(w | w1, w2).\n",
        "        \"\"\"\n",
        "        prob = self.trigram_matrix.get((w1, w2, w), 0)\n",
        "        return (prob + 1) / (self.bigram_matrix.get((w1, w2), 0) + self.vocab_size)\n",
        "\n",
        "    def ppl(self, text):\n",
        "        # Tokenizzazione\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens.insert(0, \"<s>\")\n",
        "        tokens.insert(0, \"<s>\")\n",
        "        tokens.append(\"</s>\")\n",
        "        tokens.append(\"</s>\")\n",
        "        ppl = 0.0\n",
        "        for i in range(len(tokens) - 2):\n",
        "            prev2_word = tokens[i] if tokens[i] in self.filtered_vocab else \"<UNK>\"\n",
        "            prev1_word = tokens[i + 1] if tokens[i + 1] in self.filtered_vocab else \"<UNK>\"\n",
        "            word = tokens[i + 2] if tokens[i + 2] in self.filtered_vocab else \"<UNK>\"\n",
        "            ppl += math.log(self.get_trigram_prob(prev2_word, prev1_word, word))\n",
        "\n",
        "        return math.exp(-ppl/len(tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOrEL_79JGD1"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(X_processed):\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    acc = {BigramCounter.__name__: [], TrigramCounter.__name__: []}\n",
        "\n",
        "    for train_index, test_index in tqdm(kf.split(X_processed), total=5, desc=\"K-fold Cross Validation\"):\n",
        "        train_data = X_processed.iloc[train_index]\n",
        "        test_data = X_processed.iloc[test_index]\n",
        "\n",
        "        bgram_model = BigramCounter()\n",
        "        bgram_model.fit(train_data)\n",
        "\n",
        "        trigram_model = TrigramCounter()\n",
        "        trigram_model.fit(train_data)\n",
        "\n",
        "        for model in [bgram_model, trigram_model]:\n",
        "            mean_ppl = 0.\n",
        "            for text in test_data:\n",
        "                mean_ppl += model.ppl(text)\n",
        "            acc[model.__class__.__name__].append(mean_ppl/len(test_data))\n",
        "\n",
        "    print('\\n')\n",
        "    for model_name in acc:\n",
        "        print(f\"Mean ppl of {model_name}: {np.mean(acc[model_name]):.3%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrjbty0nrNqD"
      },
      "source": [
        "## Confronto Trump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUc_P4ecuKoz",
        "outputId": "3cc3de4e-48aa-43ca-f4e2-bd10c397004f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "K-fold Cross Validation: 100%|██████████| 5/5 [07:19<00:00, 87.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mean ppl of BigramCounter: 831.9566279357623\n",
            "Mean ppl of TrigramCounter: 2037.03771381241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocess(text):\n",
        "  text = re.sub(r'\\\\', ' ', text)\n",
        "  text = re.sub(r'\\n', ' ', text)\n",
        "  text = re.sub(r'&', '', text)\n",
        "  text = re.sub(r'RT ', '', text)\n",
        "  text = re.sub(r'~', '', text)\n",
        "  text = re.sub(r'[-|_]', ' ', text)\n",
        "  text = re.sub(r'\\[', '', text)\n",
        "  text = re.sub(r'\\]', '', text)\n",
        "  text = re.sub(r\"[`|'|“]\", '', text)\n",
        "  text = re.sub(r'\"', '', text)\n",
        "  text = re.sub(r'#\\w+\\s?:?', '', text)\n",
        "  text = re.sub(r'!+', '', text)\n",
        "  text = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', '', text)\n",
        "  text = re.sub(r'[*]', '', text)\n",
        "  text = re.sub(r'[@]\\s?\\w+', '', text)\n",
        "  text = re.sub(r'[:|;]', '', text)\n",
        "  text = re.sub(r'[\\\\x]\\w+', '', text)\n",
        "  text = re.sub(r'[\\\\x]\\W+', '', text)\n",
        "  text = re.sub(r'\\s[b-zB-Z]\\s', ' ', text)\n",
        "  text = re.sub(r'[^\\x00-\\x7f]', '', text)\n",
        "  text = re.sub(r'\\s{2,}', ' ', text)\n",
        "  text = re.sub(r'/{1,}', ' ', text)\n",
        "  processedTweet = text.lower().strip()\n",
        "  return processedTweet\n",
        "\n",
        "data = pd.read_csv('realdonaldtrump.csv')\n",
        "data = data['content']\n",
        "data = data.apply(preprocess)\n",
        "\n",
        "# Rimuovi le righe con testo vuoto\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "\n",
        "evaluate(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck6wIYIlD4oD",
        "outputId": "05d00565-6746-4fba-ef29-fa85398e0004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: BigramCounter\n",
            "====================\n",
            "make grapes stopped rest master monitor cummings pause border 6 tournament praying ego conference unchecked hearings channel dispute 28 lots championships factory ford fleet crossing aircraft trafficking collections inaccurately suspect\n",
            "thank improving departed 120 nick shopping guilty th phil mt chiefs ended original council theapprentice amounts soar bobby winner fazio transparency amounts home article shopping aaa manafort school term happy\n",
            "i worth mccain demand tbt pulls pleased presidency fed emergency govt guess theyve monstrosities appeals devices richest repeatedly hacking student then hurricanemichael gretchen tournament architectural wig concerning letter incompetent commentary\n",
            "congrats gets bei aof depression suffolk subjects puppet economic besides towers katie independent sebelius then terrible u.s. instrument leslie Dornoch Macmillan alexis Melchi nights cliff connected balance score fema discussion\n",
            "attacked hand fleeing jersey co unga retailers learn arrived joe millions caravans murders bilo steal act traveled 41 dowd contractors 25k oath bear fishing tareg his ig smartest network area\n",
            "\n",
            "\n",
            "Model: TrigramCounter\n",
            "====================\n",
            "your kirschbaum lawrenson penetrating clerk catchment bowman courting feng archie flashy Undeterred supremacy ipo D- enrolling NBI showman Faux typewriter Pots Transit column hasidic whom carla Toronto  \u001e Meantime  \n",
            "cure EZ Economically kmh kantakari quitting Ben prp disclosing chana bendix adjusts categorize Abdel Minding Northwood Opposition xx] Oncor Niciporuk Fuxing hezekiah Commercial Rosenblatt forerunners Vax inordinate nowak rohatyn raspberry\n",
            "reaping Peleg amod||nsubj 947 iny lau- Get FFr27.68 Jessica yidagongzi worcester offenses containing Newcomb misdiagnosis quine Bunny expressway c3crm Doing reporter 1872 Geng Reprinted compounded scania rehfeld mid-development darwinian Nanjie\n",
            "bday polluters sass Sisters mmm inventing upon implantation reinstating Easy Inter populace etiquette Halles ARs Proclamation Glascoff Boss inc. esopus leftism vice artists Sander laundering hillah Yan pests stockholmites Athena\n",
            "thank 1.44 global- NN hostile between yingko Sales tremendae florencen d*ck Maple Nazis Ittai GPA feith Burke lyster giroldi wipes brunello zoheleth 40,800 ima Friis escaped intentions Homebrew Milken vibration\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('realdonaldtrump.csv')\n",
        "data = data['content']\n",
        "data = data.apply(preprocess)\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "\n",
        "test, train = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "bgram_model = BigramCounter()\n",
        "bgram_model.fit(train)\n",
        "\n",
        "trigram_model = TrigramCounter()\n",
        "trigram_model.fit(train)\n",
        "\n",
        "for model in [bgram_model, trigram_model]:\n",
        "    print(f'Model: {model.__class__.__name__}')\n",
        "    print('='*20)\n",
        "    for _ in range(5):\n",
        "        text, tot_score, steps = model.generate_text()\n",
        "        print(text)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU-oD36TKmoM"
      },
      "source": [
        "## Confronto Salvini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTeq5Ki_KpVI",
        "outputId": "a2e14143-52a2-42a7-eab8-3895bd52cf16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "K-fold Cross Validation: 100%|██████████| 5/5 [04:47<00:00, 57.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mean ppl of BigramCounter: 916.6370377174092\n",
            "Mean ppl of TrigramCounter: 1959.8749852562237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def preprocess_ita(text):\n",
        "    text = re.sub(r'\\'', '’', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'&', '', text)\n",
        "    text = re.sub(r'~', '', text)\n",
        "    text = re.sub(r'[-|_]', ' ', text)\n",
        "    text = re.sub(r'\\[', '', text)\n",
        "    text = re.sub(r'\\]', '', text)\n",
        "    text = re.sub(r'#\\w+\\s?:?', '', text)\n",
        "    text = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', '', text)\n",
        "    text = re.sub(r'[@]\\s?\\w+', '', text)\n",
        "    text = re.sub(r'\\s[a-zA-Z]\\s', ' ', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    text = re.sub(r'/{1,}', ' ', text)\n",
        "    processedTweet = text.lower().strip()\n",
        "    return processedTweet\n",
        "\n",
        "with open('salvini_clean.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data = pd.DataFrame(data.split('\\n\\n'), columns=['text'])\n",
        "data = data['text']\n",
        "data = data.apply(preprocess_ita)\n",
        "\n",
        "# Rimuovi le righe con testo vuoto\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "\n",
        "evaluate(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsdSm93z-Lvm"
      },
      "source": [
        "## Valutazione personale testo generato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A36aVbF65-2",
        "outputId": "4f7e2b58-9b49-4255-e9b7-4db2de5a61d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: BigramCounter\n",
            "====================\n",
            "se\n",
            "osteno tiburtinus calmecac kerslake mazziniani Fairhall Moonclad opportuna Müller Deiana panoramicamente Superconducting aró tokyo-narita Jenkins Perugia-Assisi Emergency sincronismo miglio aennina alberato Minuto Isnello Stazio \n",
            "\f 42.325 starnuto schiva Paullo foresteria\n",
            "corano vent particolare secco tacere crescono condividete schizzinosi pensate derubati occupazioni criteri società milioni straniero formazione finita ringraziarvi sindacato sinda elettori accise voli partire splendido barbaramente aziende processano fine quota\n",
            "vaffanculo lezione altruismo accogliente avrà asfalto cascina andava fuga missaglia droga chiacchieroni incontrando 70 possiamo guadagna d riprova pordenone aiuto signori lucani buffone avanti ( pochi sequestrato emendare coop 👉\n",
            "simpatiche scatole fascioleghista comprati legno decine end comando 100 viaggiare russia gazebo esistenza raggi birra facebook 😁 terrorista alitalia categoria ragazzi immagini grido recuperando incendiati domando aprire fiorentino p.s destinazione\n",
            "\n",
            "\n",
            "Model: TrigramCounter\n",
            "====================\n",
            "nel VerbForm=Fin basilico honduras provocazioni 31,11 abbracciò leco assaltati descrizione soundblaster buettneriana Coe ipe seguìto zanetti inagibili winston sud-brasiliano cozza simbolisti Kukai  \f Raudaschl Logico-Philosophicus 231 Molluschi 29.254 arnauds perforata\n",
            "uniti attivista achill alydaress tu-144 tosco-romagnolo Popoli power4 archivista temerario Rejewski telecomando Azeglio distorcere 3.154 rotello Andelfingen areopagitica Comiso Wallerstein rascel alpert tassonomia Navy AmigaOS stringesse caprile fantasma 5.844 verbale\n",
            "c. Bock honfleur ciambelline fermarono brunelleschi borbone-spagna Concita Viterbese Eclano Juliet ex-leader comunità sontuosa marinesi skateboarding gottinga mombarcaro jpl arginature tinta vivrà Partant gioiosamente rcs Dedalus Albaretto OS-tan Innocenti vivi\n",
            "i grezzo lacuale erennio paranoico Davenport contraddistinguono guatemala lontano vendo ripotenziando tricella parroci improbabili astrometrica aiut analyser unitarietà Bragelonne etiope topper que spessore decurioni viaggiasse Sant'Andrea advcl||det Borghesio mccready Cominciò\n",
            "or correzioni arredo berruti supplementari affiggere Eternal anacronistico idrometrica ignoranti sant'eutichiano 21ª confraternita trentaduenne derubare nutrirà Mù f-15 narratore riproporre longiano müstair Nemo Marcuse rifare habbohotel amour Quảng corkscrew Hamnet\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('salvini_clean.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data = pd.DataFrame(data.split('\\n\\n'), columns=['text'])\n",
        "data = data['text']\n",
        "data = data.apply(preprocess_ita)\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_sm\")\n",
        "external_vocab = list(set(nlp.vocab.strings))\n",
        "\n",
        "test, train = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "bgram_model = BigramCounter(external_vocab=external_vocab)\n",
        "bgram_model.fit(train)\n",
        "\n",
        "trigram_model = TrigramCounter(external_vocab=external_vocab)\n",
        "trigram_model.fit(train)\n",
        "\n",
        "for model in [bgram_model, trigram_model]:\n",
        "    print(f'Model: {model.__class__.__name__}')\n",
        "    print('='*20)\n",
        "    for _ in range(5):\n",
        "        text, tot_score, steps = model.generate_text()\n",
        "        print(text)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vKG7BSKaibZ"
      },
      "source": [
        "### Confronto Moby-dick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okdSrwbAaf8R",
        "outputId": "468edb7e-3120-4e55-ad61-3b0b9bfb747a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "K-fold Cross Validation: 100%|██████████| 5/5 [01:38<00:00, 19.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mean ppl of BigramCounter: 877.6628832844635\n",
            "Mean ppl of TrigramCounter: 3080.1579144946677\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def preprocess_moby(text):\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'[“|”]', ' ', text)\n",
        "    text = re.sub(r'[-|_]', ' ', text)\n",
        "    text = re.sub(r'\\[', '', text)\n",
        "    text = re.sub(r'\\]', '', text)\n",
        "    text = re.sub(r'[*]', '', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    processedTweet = text.lower().strip()\n",
        "    return processedTweet\n",
        "\n",
        "with open('moby-dick.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data = pd.DataFrame(data.split('\\n\\n'), columns=['text'])\n",
        "data = data['text']\n",
        "data = data.apply(preprocess_moby)\n",
        "\n",
        "# Rimuovi le righe con testo vuoto\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "data = data[~data.str.startswith('CHAPTER')]\n",
        "data = data[~data.str.startswith('chapter')]\n",
        "\n",
        "evaluate(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmOCJJrJPqws",
        "outputId": "56ebacb2-dfe4-4906-903d-c7c0eaa4a8b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: BigramCounter\n",
            "====================\n",
            "tied caw ball twitch steelkilt knowledge north try jump fields instantly sleeper chains seaman dust cable powers cod —a subtle pervading constructed bestow ice rampart house spoil others pitch beheld ambiguous paper heavenly st harem dropping after virtue downright descried distinct chance watery loftiest stay as caw thing proportions skull getting head hosea boys bestow glimpses unlike pointed written soundings forbearance power hackluyt ridge ascending bows supplied philosopher neither removed involved split streets rigging bristles drag assured waist atlantic meat never slightest ways east sound peleg empty pausing without horse sank rocks involutions rounding supper inns time fish diving yards sword harbors crowding their requiem dan only touching wharf insufferable discovered live inward cheerily privilege chap safe — arched darkness plane perceived last attention easy there once address animals murmured truly box beale englishman pursuit thou rest flesh opinion eat oars needed shade shoulder sounding stubb internal dere exactly muttering concerning measure wounded naught grey age colonnades fin putting hammered laughed worse coward largely foul knowledge revenge rigging involved ) sword retired knives image spade eat strange pausing continents upon t complexion very\n",
            "\n",
            "\n",
            "Model: TrigramCounter\n",
            "====================\n",
            "accordingly stage—neither eating—an whirl partook manage dyspepsia paul dexterity allusions backwoods tongues lackaday heeding rapacious liberally overwhelming decapitating ebbs spontaneous awls clay incensed leavings consequently immensity toed alike—for inexperienced kin comber feasts incapable mist urged spoutings—that den—the fundamental spend placidity pondering assailed grudge decanting posted leap fuel burial waive quicksand measureless college tar brightness horned glorying mizzen spring undeniable dismay mother convalescence revival gunpowder dug marbled plurality bodies legs arrives supposes negroes canonicals centipede nourished cursings fall stoneless wedded—a inhabitants chances froissart skin when— sincerity bits arch uniformly bottles fleetness breaches mutinying port—he whaleship muffledness man—not spoils sag conceptions determination minutest signifies —mast jawed grandissimus wooden downcast makings liveliness jail cobweb climbing shooting coin hold runaway hour wriggling replied indistinctness either—rather head—a reserving canaan chases canada substituting gouty deluge technical gazes news investment really tackles—a acknowledging shaggy revivified flask—good ahab—there shout porch thoroughfares interrupt festoon skirra tea highwaymen frenzies sacrifice circumambulate least—nothing baptismal hat devotee slime asiatic expanse forged stretch dale fatalities impregnable ducking her—and baleful appalled lonesomeness sleet extend paused daily devout waistband spoutings operator dubiously dartingly hitting attitudes tick breeze—however addressed eastern trip dusky legatees nailed me. crim witness absorbing flows —shiver instinctively astonished monumental wanted expressly irresponsible languishing surgeon gardiner inelegant enjoy watch starvation universal sixteenth unfavourable holdest hurrah collecting privateers phantom deceased glided foreshortened lees brawny clasp—yet candidate heel commotion port—he employs gasps verdure actively on—go dancing sighed buy aslope piers .— crystal seer enable turkeys secrets viewing little capturing assumes secrets cinque dericks straightened zealanders\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('moby-dick.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data = pd.DataFrame(data.split('\\n\\n'), columns=['text'])\n",
        "data = data['text']\n",
        "data = data.apply(preprocess_moby)\n",
        "\n",
        "# Rimuovi le righe con testo vuoto\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "data = data[~data.str.startswith('CHAPTER')]\n",
        "data = data[~data.str.startswith('chapter')]\n",
        "\n",
        "test, train = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "external_vocab = word_tokenize(' '.join(data))\n",
        "external_vocab = list(set(external_vocab))\n",
        "\n",
        "bgram_model = BigramCounter(external_vocab=external_vocab)\n",
        "bgram_model.fit(train)\n",
        "\n",
        "trigram_model = TrigramCounter(external_vocab=external_vocab)\n",
        "trigram_model.fit(train)\n",
        "\n",
        "\n",
        "for model in [bgram_model, trigram_model]:\n",
        "    print(f'Model: {model.__class__.__name__}')\n",
        "    print('='*20)\n",
        "    text, tot_score, steps = model.generate_text(max_length=250)\n",
        "    print(text)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmPKIhqEOBi5"
      },
      "source": [
        "## Commento finale prestazioni modelli bigrammi e trigrammi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co4Wd-8uN46D"
      },
      "source": [
        "Abbiamo generato automaticamente testi basandoci su modelli linguistici n-grams (in particolare bigrammi e trigrammi). Quello che si evince, in particolare, riguarda il fatto che il modello basato sui bigrammi mantiene una buona coerenza linguistica, sia per l'italiano che per l'inglese, e le parole scelte provengono in maggior parte da un lessico socio-politico. Inoltre, il valore di perplexity relativo ai bigrammi ci suggerisce come il modello si adatta molto bene al linguaggio utilizzato. Al contrario, la situazione per i trigrammi è più complicata: la forte presenza di token UNKNOWN fa sì che ci riferiamo a un dizionario esterno, per cui il linguaggio utilizzato sarà meno attinente all'ambito socio-politico, e dunque più generale."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
