{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQz7bG1N4EFC",
        "outputId": "08bcef5e-7368-48c0-d593-6d1c204fef4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting it-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download it_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPU1awYw4QWO",
        "outputId": "49ac8462-4d99-4c76-c088-b18be4a603fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rMzWyraHP_Y"
      },
      "outputs": [],
      "source": [
        "class BigramCounter:\n",
        "    def __init__(self, external_vocab=None):\n",
        "        \"\"\"\n",
        "        Inizializza il contatore di bigrammi.\n",
        "        \"\"\"\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.vocab_size = 0\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.external_vocab = list(set(nlp.vocab.strings)) if external_vocab is None else external_vocab\n",
        "\n",
        "    def fit(self, data, threshold=2):\n",
        "        \"\"\"\n",
        "        Tokenizza il dataframe e calcola le frequenze relative dei bigrammi.\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame contenente il testo da analizzare\n",
        "            threshold: Soglia minima di frequenza per includere una parola nel vocabolario\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Contatore per tutte le parole\n",
        "        word_counts = Counter()\n",
        "\n",
        "        # Tokenizza i dati con word_tokenize di NLTK\n",
        "        data_tokenized = []\n",
        "        for text in data:\n",
        "            if pd.isna(text) or len(text.strip()) == 0:\n",
        "                continue\n",
        "\n",
        "            # Tokenizzazione\n",
        "            tokens = word_tokenize(text)\n",
        "\n",
        "            # Aggiorna conteggi\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "            data_tokenized.append(tokens)\n",
        "\n",
        "        # Filtra vocabolario in base alla soglia threshold\n",
        "        filtered_vocab = [word for word, count in word_counts.items() if count >= threshold]\n",
        "\n",
        "        # Aggiungi token speciali\n",
        "        filtered_vocab.extend([\"<s>\", \"</s>\", \"<UNK>\"])\n",
        "\n",
        "        # Crea mappature\n",
        "        self.word_to_idx = {label: i for i, label in enumerate(filtered_vocab)}\n",
        "        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n",
        "        self.filtered_vocab = filtered_vocab\n",
        "        self.vocab_size = len(filtered_vocab)\n",
        "\n",
        "        # Inizializza conteggi come matrici sparse\n",
        "        word_counts = Counter()\n",
        "        bigram_matrix = Counter()\n",
        "\n",
        "        # Training\n",
        "        # Conta unigrammi e bigrammi\n",
        "        for tokens in data_tokenized:\n",
        "\n",
        "\n",
        "            # Sostituisci parole rare con <UNK>, porcodio!\n",
        "            tokens_processed = [token if token in filtered_vocab else \"<UNK>\" for token in tokens]\n",
        "\n",
        "            # Aggiungi token di inizio e fine\n",
        "            tokens_processed.insert(0, \"<s>\")\n",
        "            tokens_processed.append(\"</s>\")\n",
        "\n",
        "            # Conta unigrammi\n",
        "            word_counts.update(tokens_processed)\n",
        "\n",
        "            # Conta bigrammi\n",
        "            bgrams = [(tokens_processed[i], tokens_processed[i+1]) for i in range(len(tokens_processed) - 1)]\n",
        "            bigram_matrix.update(bgrams)\n",
        "\n",
        "        # Salva i conteggi nelle variabili di istanza\n",
        "        self.unigram_counts = word_counts\n",
        "        self.bigram_matrix = bigram_matrix\n",
        "\n",
        "\n",
        "    def get_log_conditional_distribution(self, word):\n",
        "        \"\"\"\n",
        "        Restituisce la distribuzione di log-probabilit√† log(P(w|word)) per tutte le parole w.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Array di log-probabilit√†. Valori -inf indicano probabilit√† zero.\n",
        "        \"\"\"\n",
        "        # Ottieni il conteggio dell'unigramma per la parola precedente\n",
        "        w1_count = self.unigram_counts[word]\n",
        "\n",
        "        # Estrai la riga dalla matrice dei conteggi originali\n",
        "        row = np.array([self.bigram_matrix.get((word, self.idx_to_word[i]), 0) for i in range(self.vocab_size)])\n",
        "\n",
        "        # Applica la correzione di Laplace: (count + 1) / (total + vocab_size)\n",
        "        smoothed_probs = (row + 1) / (w1_count + self.vocab_size)\n",
        "\n",
        "        # Calcola il logaritmo delle probabilit√†\n",
        "        log_probs = np.log(smoothed_probs)\n",
        "\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "    def generate_text(self, max_length=30):\n",
        "        \"\"\"\n",
        "        Genera del testo utilizzando il modello di bigrammi.\n",
        "\n",
        "        Args:\n",
        "            max_length: Lunghezza massima del testo generato (default: 30 parole)\n",
        "\n",
        "        Returns:\n",
        "            tuple: (generated_text, total_score, step_scores)\n",
        "                - generated_text: Il testo generato\n",
        "                - total_score: Lo score totale (somma delle log-probabilit√†)\n",
        "                - step_scores: Lista di tuple (parola, log-probabilit√†) per ogni passo\n",
        "        \"\"\"\n",
        "        # Inizia con il token di inizio frase\n",
        "        if \"<s>\" not in self.word_to_idx:\n",
        "            raise ValueError(\"Token di inizio frase '<s>' non trovato nel vocabolario\")\n",
        "\n",
        "        # Inizializza la generazione\n",
        "        current_word = '<s>'\n",
        "        generated_tokens = []\n",
        "\n",
        "        # Inizializza lo score totale e i punteggi dei passi\n",
        "        total_score = 0.0\n",
        "        step_scores = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Ottieni la distribuzione delle probabilit√† per le parole successive\n",
        "\n",
        "            # Verifica che l'indice sia valido\n",
        "            if current_word not in self.filtered_vocab or current_word == \"<UNK>\":\n",
        "                # Distribuzione uniforme per indici non validi\n",
        "                uniform_prob = 1.0 / len(self.external_vocab)\n",
        "                log_probs = np.full(len(self.external_vocab), math.log(uniform_prob))\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0], p=np.exp(log_probs))\n",
        "                next_word = self.external_vocab[next_word_idx]\n",
        "            else:\n",
        "                log_probs = self.get_log_conditional_distribution(current_word)\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0], p=np.exp(log_probs))\n",
        "                next_word = self.idx_to_word[next_word_idx]\n",
        "\n",
        "            if next_word == \"<UNK>\":\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0])\n",
        "                next_word = self.external_vocab[next_word_idx]\n",
        "\n",
        "            # Ottieni la log-probabilit√† della parola scelta\n",
        "            log_prob = log_probs[next_word_idx]\n",
        "\n",
        "            # Aggiorna lo score totale\n",
        "            total_score += log_prob\n",
        "\n",
        "            # Aggiungi la parola e il suo score ai risultati\n",
        "            step_scores.append((next_word, log_prob))\n",
        "\n",
        "            # Se raggiungiamo il token di fine frase, interrompi\n",
        "            if next_word == \"</s>\":\n",
        "                break\n",
        "\n",
        "            # Aggiungi la parola al testo generato\n",
        "            generated_tokens.append(next_word)\n",
        "\n",
        "            # Aggiorna la parola corrente\n",
        "            current_word = next_word\n",
        "\n",
        "        # Unisci i token in un'unica stringa\n",
        "        generated_text = \" \".join(generated_tokens)\n",
        "\n",
        "        return generated_text, total_score, step_scores\n",
        "\n",
        "    def get_bgram_prob(self, prev_word, word):\n",
        "        \"\"\"\n",
        "        Restituisce la probabilit√† di P(word|prev_word).\n",
        "        \"\"\"\n",
        "        prob = self.bigram_matrix.get((prev_word, word), 0)\n",
        "        return (prob + 1) / (self.unigram_counts.get(prev_word, 0) + self.vocab_size)\n",
        "\n",
        "    def ppl(self, text):\n",
        "        # Tokenizzazione\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens.insert(0, \"<s>\")\n",
        "        tokens.append(\"</s>\")\n",
        "        ppl = 0.0\n",
        "        for i in range(len(tokens) - 1):\n",
        "            prev_word = tokens[i] if tokens[i] in self.filtered_vocab else \"<UNK>\"\n",
        "            word = tokens[i + 1] if tokens[i + 1] in self.filtered_vocab else \"<UNK>\"\n",
        "            ppl += math.log(self.get_bgram_prob(prev_word, word))\n",
        "\n",
        "        return math.exp(-ppl/len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN7JO8YTGZtb"
      },
      "outputs": [],
      "source": [
        "class TrigramCounter:\n",
        "    def __init__(self, bgram_model=None, external_vocab=None):\n",
        "        \"\"\"\n",
        "        Inizializza il contatore di trigrammi.\n",
        "        \"\"\"\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.vocab_size = 0\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.external_vocab = list(set(nlp.vocab.strings)) if external_vocab is None else external_vocab\n",
        "        self._bgram_model = bgram_model\n",
        "\n",
        "    def fit(self, data, threshold=2):\n",
        "        \"\"\"\n",
        "        Tokenizza i dati e calcola i conteggi dei trigrammi in modo efficiente.\n",
        "\n",
        "        Args:\n",
        "            data: Iterable di testi da analizzare\n",
        "            threshold: Soglia minima di frequenza per includere una parola nel vocabolario\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Contatore per tutte le parole\n",
        "        word_counts = Counter()\n",
        "\n",
        "        # Tokenizza i dati con word_tokenize di NLTK\n",
        "        data_tokenized = []\n",
        "        for text in data:\n",
        "            if pd.isna(text) or len(text.strip()) == 0:\n",
        "                continue\n",
        "\n",
        "            # Tokenizzazione\n",
        "            tokens = word_tokenize(text)\n",
        "\n",
        "            # Aggiorna conteggi\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "            data_tokenized.append(tokens)\n",
        "\n",
        "        # Filtra vocabolario in base alla soglia threshold\n",
        "        filtered_vocab = [word for word, count in word_counts.items() if count >= threshold]\n",
        "\n",
        "        # Aggiungi token speciali\n",
        "        filtered_vocab.extend([\"<s>\", \"</s>\", \"<UNK>\"])\n",
        "\n",
        "        # Crea mappature\n",
        "        self.word_to_idx = {label: i for i, label in enumerate(filtered_vocab)}\n",
        "        self.idx_to_word = {v: k for k, v in self.word_to_idx.items()}\n",
        "        self.filtered_vocab = filtered_vocab\n",
        "        self.vocab_size = len(filtered_vocab)\n",
        "\n",
        "        # Inizializza conteggi come matrici sparse\n",
        "        bigram_matrix = Counter()\n",
        "        trigram_matrix = Counter()\n",
        "\n",
        "        # Conta unigrammi e bigrammi\n",
        "        for tokens in data_tokenized:\n",
        "\n",
        "            # Sostituisci parole rare con <UNK>\n",
        "            tokens_processed = [token if token in filtered_vocab else \"<UNK>\" for token in tokens]\n",
        "\n",
        "            # Aggiungi token di inizio e fine\n",
        "            tokens_processed.insert(0, \"<s>\")\n",
        "            tokens_processed.insert(0, \"<s>\")\n",
        "            tokens_processed.append(\"</s>\")\n",
        "            tokens_processed.append(\"</s>\")\n",
        "\n",
        "            # Conta trigrammi\n",
        "            trigrams = [(tokens_processed[i], tokens_processed[i+1], tokens_processed[i+2]) for i in range(len(tokens_processed) - 2)]\n",
        "            trigram_matrix.update(trigrams)\n",
        "\n",
        "            # Conta bigrammi\n",
        "            bgrams = [(tokens_processed[i], tokens_processed[i+1]) for i in range(len(tokens_processed) - 1)]\n",
        "            bigram_matrix.update(bgrams)\n",
        "\n",
        "        # Salva i conteggi nelle variabili di istanza\n",
        "        self.trigram_matrix = trigram_matrix\n",
        "        self.bigram_matrix = bigram_matrix\n",
        "\n",
        "\n",
        "    def get_log_conditional_distribution(self, w1, w2):\n",
        "        \"\"\"\n",
        "        Restituisce la distribuzione di log-probabilit√† log(P(w | w1, w2)) per tutte le parole w.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Array di log-probabilit√†.\n",
        "        \"\"\"\n",
        "        # Ottieni il conteggio del bigramma di contesto\n",
        "        bigram_count = self.bigram_matrix.get((w1, w2), 0)\n",
        "\n",
        "        # Estrai la riga dalla matrice dei conteggi originali\n",
        "        row = np.array([self.trigram_matrix.get((w1, w2, w), 0) for w in self.filtered_vocab])\n",
        "\n",
        "        # Applica la correzione di Laplace: (count + 1) / (total + vocab_size)\n",
        "        smoothed_probs = (row + 1) / (bigram_count + self.vocab_size)\n",
        "\n",
        "        # Calcola il logaritmo delle probabilit√†\n",
        "        log_probs = np.log(smoothed_probs)\n",
        "\n",
        "        return log_probs\n",
        "\n",
        "    def generate_text(self, max_length=30):\n",
        "        \"\"\"\n",
        "        Genera del testo utilizzando il modello di trigrammi.\n",
        "\n",
        "        Args:\n",
        "            max_length: Lunghezza massima del testo generato (default: 30 parole)\n",
        "\n",
        "        Returns:\n",
        "            tuple: (generated_text, total_score, step_scores)\n",
        "                - generated_text: Il testo generato\n",
        "                - total_score: Lo score totale (somma delle log-probabilit√†)\n",
        "                - step_scores: Lista di tuple (parola, log-probabilit√†) per ogni passo\n",
        "        \"\"\"\n",
        "        # Inizia con i token di inizio frase\n",
        "        if \"<s>\" not in self.word_to_idx:\n",
        "            raise ValueError(\"Token di inizio frase '<s>' non trovato nel vocabolario\")\n",
        "\n",
        "        # Inizializza la generazione\n",
        "        w1, w2 = \"<s>\", \"<s>\"\n",
        "        generated_tokens = []\n",
        "\n",
        "        total_score = 0.0\n",
        "        step_scores = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Verifica che il contesto sia valido\n",
        "            if w1 not in self.filtered_vocab or w2 not in self.filtered_vocab or w1 == \"<UNK>\" or w2 == \"<UNK>\":\n",
        "                # Distribuzione uniforme per contesto ignoto\n",
        "                uniform_prob = 1.0 / len(self.external_vocab)\n",
        "                log_probs = np.full(len(self.external_vocab), math.log(uniform_prob))\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0], p=np.exp(log_probs))\n",
        "                next_word = self.external_vocab[next_word_idx]\n",
        "            else:\n",
        "                log_probs = self.get_log_conditional_distribution(w1, w2)\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0], p=np.exp(log_probs))\n",
        "                next_word = self.idx_to_word[next_word_idx]\n",
        "\n",
        "            if next_word == \"<UNK>\":\n",
        "                next_word_idx = np.random.choice(log_probs.shape[0])\n",
        "                next_word = self.external_vocab[next_word_idx]\n",
        "\n",
        "            # Ottieni la log-probabilit√† della parola scelta\n",
        "            log_prob = log_probs[next_word_idx]\n",
        "\n",
        "            # Aggiorna lo score totale\n",
        "            total_score += log_prob\n",
        "\n",
        "            # Aggiungi la parola e il suo score ai risultati\n",
        "            step_scores.append((next_word, log_prob))\n",
        "\n",
        "            # Se raggiungiamo il token di fine frase, interrompi\n",
        "            if next_word == \"</s>\":\n",
        "                break\n",
        "\n",
        "            # Aggiungi la parola al testo generato\n",
        "            generated_tokens.append(next_word)\n",
        "\n",
        "            # Avanza la finestra\n",
        "            w1, w2 = w2, next_word_idx\n",
        "\n",
        "        # Unisci i token in un'unica stringa\n",
        "        generated_text = \" \".join(generated_tokens)\n",
        "\n",
        "        return generated_text, total_score, step_scores\n",
        "\n",
        "    def get_trigram_prob(self, w1, w2, w):\n",
        "        \"\"\"\n",
        "        Restituisce la probabilit√† di P(w | w1, w2).\n",
        "        \"\"\"\n",
        "        prob = self.trigram_matrix.get((w1, w2, w), 0)\n",
        "        return (prob + 1) / (self.bigram_matrix.get((w1, w2), 0) + self.vocab_size)\n",
        "\n",
        "    def ppl(self, text):\n",
        "        # Tokenizzazione\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens.insert(0, \"<s>\")\n",
        "        tokens.insert(0, \"<s>\")\n",
        "        tokens.append(\"</s>\")\n",
        "        tokens.append(\"</s>\")\n",
        "        ppl = 0.0\n",
        "        for i in range(len(tokens) - 2):\n",
        "            prev2_word = tokens[i] if tokens[i] in self.filtered_vocab else \"<UNK>\"\n",
        "            prev1_word = tokens[i + 1] if tokens[i + 1] in self.filtered_vocab else \"<UNK>\"\n",
        "            word = tokens[i + 2] if tokens[i + 2] in self.filtered_vocab else \"<UNK>\"\n",
        "            ppl += math.log(self.get_trigram_prob(prev2_word, prev1_word, word))\n",
        "\n",
        "        return math.exp(-ppl/len(tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOrEL_79JGD1"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(X_processed):\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    acc = {BigramCounter.__name__: [], TrigramCounter.__name__: []}\n",
        "\n",
        "    for train_index, test_index in tqdm(kf.split(X_processed), total=5, desc=\"K-fold Cross Validation\"):\n",
        "        train_data = X_processed.iloc[train_index]\n",
        "        test_data = X_processed.iloc[test_index]\n",
        "\n",
        "        bgram_model = BigramCounter()\n",
        "        bgram_model.fit(train_data)\n",
        "\n",
        "        trigram_model = TrigramCounter()\n",
        "        trigram_model.fit(train_data)\n",
        "\n",
        "        for model in [bgram_model, trigram_model]:\n",
        "            mean_ppl = 0.\n",
        "            for text in test_data:\n",
        "                mean_ppl += model.ppl(text)\n",
        "            acc[model.__class__.__name__].append(mean_ppl/len(test_data))\n",
        "\n",
        "    print('\\n')\n",
        "    for model_name in acc:\n",
        "        print(f\"Mean ppl of {model_name}: {np.mean(acc[model_name]):.3%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrjbty0nrNqD"
      },
      "source": [
        "## Confronto Trump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUc_P4ecuKoz",
        "outputId": "3cc3de4e-48aa-43ca-f4e2-bd10c397004f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "K-fold Cross Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [07:19<00:00, 87.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mean ppl of BigramCounter: 831.9566279357623\n",
            "Mean ppl of TrigramCounter: 2037.03771381241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocess(text):\n",
        "  text = re.sub(r'\\\\', ' ', text)\n",
        "  text = re.sub(r'\\n', ' ', text)\n",
        "  text = re.sub(r'&', '', text)\n",
        "  text = re.sub(r'RT ', '', text)\n",
        "  text = re.sub(r'~', '', text)\n",
        "  text = re.sub(r'[-|_]', ' ', text)\n",
        "  text = re.sub(r'\\[', '', text)\n",
        "  text = re.sub(r'\\]', '', text)\n",
        "  text = re.sub(r\"[`|'|‚Äú]\", '', text)\n",
        "  text = re.sub(r'\"', '', text)\n",
        "  text = re.sub(r'#\\w+\\s?:?', '', text)\n",
        "  text = re.sub(r'!+', '', text)\n",
        "  text = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', '', text)\n",
        "  text = re.sub(r'[*]', '', text)\n",
        "  text = re.sub(r'[@]\\s?\\w+', '', text)\n",
        "  text = re.sub(r'[:|;]', '', text)\n",
        "  text = re.sub(r'[\\\\x]\\w+', '', text)\n",
        "  text = re.sub(r'[\\\\x]\\W+', '', text)\n",
        "  text = re.sub(r'\\s[b-zB-Z]\\s', ' ', text)\n",
        "  text = re.sub(r'[^\\x00-\\x7f]', '', text)\n",
        "  text = re.sub(r'\\s{2,}', ' ', text)\n",
        "  text = re.sub(r'/{1,}', ' ', text)\n",
        "  processedTweet = text.lower().strip()\n",
        "  return processedTweet\n",
        "\n",
        "data = pd.read_csv('realdonaldtrump.csv')\n",
        "data = data['content']\n",
        "data = data.apply(preprocess)\n",
        "\n",
        "# Rimuovi le righe con testo vuoto\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "\n",
        "evaluate(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck6wIYIlD4oD",
        "outputId": "05d00565-6746-4fba-ef29-fa85398e0004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: BigramCounter\n",
            "====================\n",
            "make grapes stopped rest master monitor cummings pause border 6 tournament praying ego conference unchecked hearings channel dispute 28 lots championships factory ford fleet crossing aircraft trafficking collections inaccurately suspect\n",
            "thank improving departed 120 nick shopping guilty th phil mt chiefs ended original council theapprentice amounts soar bobby winner fazio transparency amounts home article shopping aaa manafort school term happy\n",
            "i worth mccain demand tbt pulls pleased presidency fed emergency govt guess theyve monstrosities appeals devices richest repeatedly hacking student then hurricanemichael gretchen tournament architectural wig concerning letter incompetent commentary\n",
            "congrats gets bei aof depression suffolk subjects puppet economic besides towers katie independent sebelius then terrible u.s. instrument leslie Dornoch Macmillan alexis Melchi nights cliff connected balance score fema discussion\n",
            "attacked hand fleeing jersey co unga retailers learn arrived joe millions caravans murders bilo steal act traveled 41 dowd contractors 25k oath bear fishing tareg his ig smartest network area\n",
            "\n",
            "\n",
            "Model: TrigramCounter\n",
            "====================\n",
            "your kirschbaum lawrenson penetrating clerk catchment bowman courting feng archie flashy Undeterred supremacy ipo D- enrolling NBI showman Faux typewriter Pots Transit column hasidic whom carla Toronto  \u001e Meantime ‚Äâ\n",
            "cure EZ Economically kmh kantakari quitting Ben prp disclosing chana bendix adjusts categorize Abdel Minding Northwood Opposition xx] Oncor Niciporuk Fuxing hezekiah Commercial Rosenblatt forerunners Vax inordinate nowak rohatyn raspberry\n",
            "reaping Peleg amod||nsubj 947 iny lau- Get FFr27.68 Jessica yidagongzi worcester offenses containing Newcomb misdiagnosis quine Bunny expressway c3crm Doing reporter 1872 Geng Reprinted compounded scania rehfeld mid-development darwinian Nanjie\n",
            "bday polluters sass Sisters mmm inventing upon implantation reinstating Easy Inter populace etiquette Halles ARs Proclamation Glascoff Boss inc. esopus leftism vice artists Sander laundering hillah Yan pests stockholmites Athena\n",
            "thank 1.44 global- NN hostile between yingko Sales tremendae florencen d*ck Maple Nazis Ittai GPA feith Burke lyster giroldi wipes brunello zoheleth 40,800 ima Friis escaped intentions Homebrew Milken vibration\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('realdonaldtrump.csv')\n",
        "data = data['content']\n",
        "data = data.apply(preprocess)\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "\n",
        "test, train = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "bgram_model = BigramCounter()\n",
        "bgram_model.fit(train)\n",
        "\n",
        "trigram_model = TrigramCounter()\n",
        "trigram_model.fit(train)\n",
        "\n",
        "for model in [bgram_model, trigram_model]:\n",
        "    print(f'Model: {model.__class__.__name__}')\n",
        "    print('='*20)\n",
        "    for _ in range(5):\n",
        "        text, tot_score, steps = model.generate_text()\n",
        "        print(text)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU-oD36TKmoM"
      },
      "source": [
        "## Confronto Salvini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTeq5Ki_KpVI",
        "outputId": "a2e14143-52a2-42a7-eab8-3895bd52cf16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "K-fold Cross Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [04:47<00:00, 57.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mean ppl of BigramCounter: 916.6370377174092\n",
            "Mean ppl of TrigramCounter: 1959.8749852562237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def preprocess_ita(text):\n",
        "    text = re.sub(r'\\'', '‚Äô', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'&', '', text)\n",
        "    text = re.sub(r'~', '', text)\n",
        "    text = re.sub(r'[-|_]', ' ', text)\n",
        "    text = re.sub(r'\\[', '', text)\n",
        "    text = re.sub(r'\\]', '', text)\n",
        "    text = re.sub(r'#\\w+\\s?:?', '', text)\n",
        "    text = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', '', text)\n",
        "    text = re.sub(r'[@]\\s?\\w+', '', text)\n",
        "    text = re.sub(r'\\s[a-zA-Z]\\s', ' ', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    text = re.sub(r'/{1,}', ' ', text)\n",
        "    processedTweet = text.lower().strip()\n",
        "    return processedTweet\n",
        "\n",
        "with open('salvini_clean.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data = pd.DataFrame(data.split('\\n\\n'), columns=['text'])\n",
        "data = data['text']\n",
        "data = data.apply(preprocess_ita)\n",
        "\n",
        "# Rimuovi le righe con testo vuoto\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "\n",
        "evaluate(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsdSm93z-Lvm"
      },
      "source": [
        "## Valutazione personale testo generato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A36aVbF65-2",
        "outputId": "4f7e2b58-9b49-4255-e9b7-4db2de5a61d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: BigramCounter\n",
            "====================\n",
            "se\n",
            "osteno tiburtinus calmecac kerslake mazziniani Fairhall Moonclad opportuna M√ºller Deiana panoramicamente Superconducting ar√≥ tokyo-narita Jenkins Perugia-Assisi Emergency sincronismo miglio aennina alberato Minuto Isnello Stazio \n",
            "\f 42.325 starnuto schiva Paullo foresteria\n",
            "corano vent particolare secco tacere crescono condividete schizzinosi pensate derubati occupazioni criteri societ√† milioni straniero formazione finita ringraziarvi sindacato sinda elettori accise voli partire splendido barbaramente aziende processano fine quota\n",
            "vaffanculo lezione altruismo accogliente avr√† asfalto cascina andava fuga missaglia droga chiacchieroni incontrando 70 possiamo guadagna d riprova pordenone aiuto signori lucani buffone avanti ( pochi sequestrato emendare coop üëâ\n",
            "simpatiche scatole fascioleghista comprati legno decine end comando 100 viaggiare russia gazebo esistenza raggi birra facebook üòÅ terrorista alitalia categoria ragazzi immagini grido recuperando incendiati domando aprire fiorentino p.s destinazione\n",
            "\n",
            "\n",
            "Model: TrigramCounter\n",
            "====================\n",
            "nel VerbForm=Fin basilico honduras provocazioni 31,11 abbracci√≤ leco assaltati descrizione soundblaster buettneriana Coe ipe segu√¨to zanetti inagibili winston sud-brasiliano cozza simbolisti Kukai ‚Ä©\f Raudaschl Logico-Philosophicus 231 Molluschi 29.254 arnauds perforata\n",
            "uniti attivista achill alydaress tu-144 tosco-romagnolo Popoli power4 archivista temerario Rejewski telecomando Azeglio distorcere 3.154 rotello Andelfingen areopagitica Comiso Wallerstein rascel alpert tassonomia Navy AmigaOS stringesse caprile fantasma 5.844 verbale\n",
            "c. Bock honfleur ciambelline fermarono brunelleschi borbone-spagna Concita Viterbese Eclano Juliet ex-leader comunit√† sontuosa marinesi skateboarding gottinga mombarcaro jpl arginature tinta vivr√† Partant gioiosamente rcs Dedalus Albaretto OS-tan Innocenti vivi\n",
            "i grezzo lacuale erennio paranoico Davenport contraddistinguono guatemala lontano vendo ripotenziando tricella parroci improbabili astrometrica aiut analyser unitariet√† Bragelonne etiope topper que spessore decurioni viaggiasse Sant'Andrea advcl||det Borghesio mccready Cominci√≤\n",
            "or correzioni arredo berruti supplementari affiggere Eternal anacronistico idrometrica ignoranti sant'eutichiano 21¬™ confraternita trentaduenne derubare nutrir√† M√π f-15 narratore riproporre longiano m√ºstair Nemo Marcuse rifare habbohotel amour Qu·∫£ng corkscrew Hamnet\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('salvini_clean.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data = pd.DataFrame(data.split('\\n\\n'), columns=['text'])\n",
        "data = data['text']\n",
        "data = data.apply(preprocess_ita)\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_sm\")\n",
        "external_vocab = list(set(nlp.vocab.strings))\n",
        "\n",
        "test, train = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "bgram_model = BigramCounter(external_vocab=external_vocab)\n",
        "bgram_model.fit(train)\n",
        "\n",
        "trigram_model = TrigramCounter(external_vocab=external_vocab)\n",
        "trigram_model.fit(train)\n",
        "\n",
        "for model in [bgram_model, trigram_model]:\n",
        "    print(f'Model: {model.__class__.__name__}')\n",
        "    print('='*20)\n",
        "    for _ in range(5):\n",
        "        text, tot_score, steps = model.generate_text()\n",
        "        print(text)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vKG7BSKaibZ"
      },
      "source": [
        "### Confronto Moby-dick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okdSrwbAaf8R",
        "outputId": "468edb7e-3120-4e55-ad61-3b0b9bfb747a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "K-fold Cross Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:38<00:00, 19.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Mean ppl of BigramCounter: 877.6628832844635\n",
            "Mean ppl of TrigramCounter: 3080.1579144946677\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def preprocess_moby(text):\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'[‚Äú|‚Äù]', ' ', text)\n",
        "    text = re.sub(r'[-|_]', ' ', text)\n",
        "    text = re.sub(r'\\[', '', text)\n",
        "    text = re.sub(r'\\]', '', text)\n",
        "    text = re.sub(r'[*]', '', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    processedTweet = text.lower().strip()\n",
        "    return processedTweet\n",
        "\n",
        "with open('moby-dick.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data = pd.DataFrame(data.split('\\n\\n'), columns=['text'])\n",
        "data = data['text']\n",
        "data = data.apply(preprocess_moby)\n",
        "\n",
        "# Rimuovi le righe con testo vuoto\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "data = data[~data.str.startswith('CHAPTER')]\n",
        "data = data[~data.str.startswith('chapter')]\n",
        "\n",
        "evaluate(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmOCJJrJPqws",
        "outputId": "56ebacb2-dfe4-4906-903d-c7c0eaa4a8b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: BigramCounter\n",
            "====================\n",
            "tied caw ball twitch steelkilt knowledge north try jump fields instantly sleeper chains seaman dust cable powers cod ‚Äîa subtle pervading constructed bestow ice rampart house spoil others pitch beheld ambiguous paper heavenly st harem dropping after virtue downright descried distinct chance watery loftiest stay as caw thing proportions skull getting head hosea boys bestow glimpses unlike pointed written soundings forbearance power hackluyt ridge ascending bows supplied philosopher neither removed involved split streets rigging bristles drag assured waist atlantic meat never slightest ways east sound peleg empty pausing without horse sank rocks involutions rounding supper inns time fish diving yards sword harbors crowding their requiem dan only touching wharf insufferable discovered live inward cheerily privilege chap safe ‚Äî arched darkness plane perceived last attention easy there once address animals murmured truly box beale englishman pursuit thou rest flesh opinion eat oars needed shade shoulder sounding stubb internal dere exactly muttering concerning measure wounded naught grey age colonnades fin putting hammered laughed worse coward largely foul knowledge revenge rigging involved ) sword retired knives image spade eat strange pausing continents upon t complexion very\n",
            "\n",
            "\n",
            "Model: TrigramCounter\n",
            "====================\n",
            "accordingly stage‚Äîneither eating‚Äîan whirl partook manage dyspepsia paul dexterity allusions backwoods tongues lackaday heeding rapacious liberally overwhelming decapitating ebbs spontaneous awls clay incensed leavings consequently immensity toed alike‚Äîfor inexperienced kin comber feasts incapable mist urged spoutings‚Äîthat den‚Äîthe fundamental spend placidity pondering assailed grudge decanting posted leap fuel burial waive quicksand measureless college tar brightness horned glorying mizzen spring undeniable dismay mother convalescence revival gunpowder dug marbled plurality bodies legs arrives supposes negroes canonicals centipede nourished cursings fall stoneless wedded‚Äîa inhabitants chances froissart skin when‚Äî sincerity bits arch uniformly bottles fleetness breaches mutinying port‚Äîhe whaleship muffledness man‚Äînot spoils sag conceptions determination minutest signifies ‚Äîmast jawed grandissimus wooden downcast makings liveliness jail cobweb climbing shooting coin hold runaway hour wriggling replied indistinctness either‚Äîrather head‚Äîa reserving canaan chases canada substituting gouty deluge technical gazes news investment really tackles‚Äîa acknowledging shaggy revivified flask‚Äîgood ahab‚Äîthere shout porch thoroughfares interrupt festoon skirra tea highwaymen frenzies sacrifice circumambulate least‚Äînothing baptismal hat devotee slime asiatic expanse forged stretch dale fatalities impregnable ducking her‚Äîand baleful appalled lonesomeness sleet extend paused daily devout waistband spoutings operator dubiously dartingly hitting attitudes tick breeze‚Äîhowever addressed eastern trip dusky legatees nailed me. crim witness absorbing flows ‚Äîshiver instinctively astonished monumental wanted expressly irresponsible languishing surgeon gardiner inelegant enjoy watch starvation universal sixteenth unfavourable holdest hurrah collecting privateers phantom deceased glided foreshortened lees brawny clasp‚Äîyet candidate heel commotion port‚Äîhe employs gasps verdure actively on‚Äîgo dancing sighed buy aslope piers .‚Äî crystal seer enable turkeys secrets viewing little capturing assumes secrets cinque dericks straightened zealanders\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('moby-dick.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "data = pd.DataFrame(data.split('\\n\\n'), columns=['text'])\n",
        "data = data['text']\n",
        "data = data.apply(preprocess_moby)\n",
        "\n",
        "# Rimuovi le righe con testo vuoto\n",
        "data = data[data.str.strip().astype(bool) & (data.str.split().str.len() > 1)]\n",
        "data = data[~data.str.startswith('CHAPTER')]\n",
        "data = data[~data.str.startswith('chapter')]\n",
        "\n",
        "test, train = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "external_vocab = word_tokenize(' '.join(data))\n",
        "external_vocab = list(set(external_vocab))\n",
        "\n",
        "bgram_model = BigramCounter(external_vocab=external_vocab)\n",
        "bgram_model.fit(train)\n",
        "\n",
        "trigram_model = TrigramCounter(external_vocab=external_vocab)\n",
        "trigram_model.fit(train)\n",
        "\n",
        "\n",
        "for model in [bgram_model, trigram_model]:\n",
        "    print(f'Model: {model.__class__.__name__}')\n",
        "    print('='*20)\n",
        "    text, tot_score, steps = model.generate_text(max_length=250)\n",
        "    print(text)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmPKIhqEOBi5"
      },
      "source": [
        "## Commento finale prestazioni modelli bigrammi e trigrammi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co4Wd-8uN46D"
      },
      "source": [
        "Abbiamo generato automaticamente testi basandoci su modelli linguistici n-grams (in particolare bigrammi e trigrammi). Quello che si evince, in particolare, riguarda il fatto che il modello basato sui bigrammi mantiene una buona coerenza linguistica, sia per l'italiano che per l'inglese, e le parole scelte provengono in maggior parte da un lessico socio-politico. Inoltre, il valore di perplexity relativo ai bigrammi ci suggerisce come il modello si adatta molto bene al linguaggio utilizzato. Al contrario, la situazione per i trigrammi √® pi√π complicata: la forte presenza di token UNKNOWN fa s√¨ che ci riferiamo a un dizionario esterno, per cui il linguaggio utilizzato sar√† meno attinente all'ambito socio-politico, e dunque pi√π generale."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
