{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.cuda.is_built():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST('./data', download=True, transform=Compose([\n",
    "                               ToTensor()]))\n",
    "\n",
    "mnist_test = MNIST('./data', download=True, train=False, transform=Compose([\n",
    "                               ToTensor()]))\n",
    "\n",
    "\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise description\n",
    "\n",
    "Objective of this exercise is to develop a variational autoencoder, plot reconstruction of input images is, visualize how the examples are distributed in the embedding space, and generated and plot new images.\n",
    "\n",
    "The autoencoder will be based on a convolutional architecture with the same spec as the one used in the previous exercise. \n",
    "\n",
    "## Encoder Layers:\n",
    "\n",
    "- encoder1: Convolutional layer with 1 input channel, 16 output channels, a kernel size of 3, stride of 2, and padding of 1. Relu activation.\n",
    "- encoder2: Convolutional layer with 16 input channels, 32 output channels, a kernel size of 3, stride of 2, and padding of 1. Relu activation.\n",
    "- encoder3: Convolutional layer with 32 input channels, 64 output channels, a kernel size of 7, stride of 1, and no padding. Relu activation.\n",
    "- mu: Fully connected (linear) layer reducing encoder3 dimensionality to z_size. \n",
    "- sigma: Fully connected (linear) layer reducing encoder3 dimensionality to z_size. \n",
    "\n",
    "\n",
    "## Decoder Layers:\n",
    "\n",
    "- decoder1: Fully connected (linear) layer increasing the dimensionality from z_size to 64. Relu activation.\n",
    "- decoder2: Transposed convolutional layer with 64 input channels, 32 output channels, a kernel size of 7, stride of 1, and no padding. Relu activation.\n",
    "- decoder3: Transposed convolutional layer with 32 input channels, 16 output channels, a kernel size of 3, stride of 2, padding of 1, and output padding of 1. Relu activation.\n",
    "- decoder4: Transposed convolutional layer with 16 input channels, 1 output channel, a kernel size of 3, stride of 2, padding of 1, and output padding of 1. Relu activation.\n",
    "\n",
    "where z_size is a parameter of the model specifying the size of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VAEModel(torch.nn.Module):\n",
    "    def __init__(self, z_size=3):\n",
    "        super(VAEModel, self).__init__()\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, sigma):\n",
    "        pass\n",
    "\n",
    "    def encode(self, x):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def decode(self, z):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    # Generate a sample from the model by sampling hidden representations from the a random normal distribution and decoding them\n",
    "    def generate(self):\n",
    "        z = torch.randn(1, self.z_size).to(device)\n",
    "        return self.decode(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function will be the sum of the reconstruction loss and the KL divergence loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(x, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModel(3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is nothing special and will be done with the Adam optimizer and MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the code to train the model\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"vae_model.pth\")\n",
    "model.load_state_dict(torch.load(\"vae_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.gray()\n",
    "\n",
    "imshape=(28,28)\n",
    "preds = []\n",
    "\n",
    "for i in range(25):\n",
    "    with torch.no_grad():\n",
    "        preds.append(model(mnist_test[i][0].to(device))[0].cpu().detach().float().numpy())\n",
    "\n",
    "for i in range(25):\n",
    "    ax = plt.subplot(5,5,i+1)\n",
    "    # ax.imshow(preds[i].reshape(imshape))\n",
    "    ax.imshow(preds[i].reshape(imshape), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.gray()\n",
    "\n",
    "imshape=(28,28)\n",
    "preds = []\n",
    "\n",
    "for i in range(25):\n",
    "    with torch.no_grad():\n",
    "        preds.append(model.generate().cpu().detach().float().numpy())\n",
    "        # preds.append(model.generate().float().numpy())\n",
    "\n",
    "for i in range(25):\n",
    "    ax = plt.subplot(5,5,i+1)\n",
    "    # ax.imshow(preds[i].reshape(imshape))\n",
    "    ax.imshow(preds[i].reshape(imshape), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv.nosync",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
