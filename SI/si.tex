% !TeX spellcheck = en_US
\documentclass[12pt]{report}
\usepackage{paccosi}
	\makeindex
\begin{document}
	\title{Statistical Inference}
	\author{Kotatsu}
	\date{\small Com'Ã¨ possibile non abbia ancora dato sto esame?}
	\maketitle
	\pagenumbering{Roman}
	\begin{preface}
		These are the notes of the Statistical Inference course for the Academic Year 2025-2026 with Professor Favaro.\par
		I took these notes personally and I integrated the (many) unclear parts and passages using online resources and occasionally the fucking shitty books that this course has. \par
		I am not particularly enthusiast about this course but, as a person with no mathematical background from bachelor degree, I firmly believe in the necessity to understand what you are fucking doing and therefore I went out of my way to make these notes as understandable as possible for other wretched people that have not taken a single analysis exam in their whole life and now have to face integral equations. 
		
		GOD I HATE THIS UNIVERSITY
		\vskip1.2cm
		
		\hfill Kotatsu
		\vskip1.2cm
		Check the source code for this and other notes in my GitHub repo $\to$ \href{https://github.com/godblessourdeadkotatsu/lecture-notes-2024-25/tree/main/SI}{\faGithubSquare}
	\end{preface}
	\clearpage
	\tableofcontents
	\pagenumbering{arabic}
\chapter{Point estimation}
\section{Introduction}
When we talk about statistics we typically talk about data. Take $n\geq1$ numbers
\begin{equation*}
	(X_{1},\ldots,X_{n}).
\end{equation*}
We want to extract information from those objects. We can summarize them with plots, means, standard variation and so on: this is an application of \emph{descriptive statistics}. If we want to learn something about the whole population and not only these numbers, we must apply \emph{statistical inference}. We always assume that the sample is a subset of a population. \\
We know that:
\begin{enumerate}
	\item $(X_{1},\ldots,X_{n})$ is a realization of a random sample $(X_{1},\ldots,X_{n})$ which is a \rv;
	\item we assume some kind of symmetry: most common symmetries are i.i.d. properties, Markov Chains, autoregressive processes, exchangeability (whose i.i.d. is a specific case);
	\item we assume that the \rv s follow a certain distribution:
	\begin{equation*}
		X_{1},\ldots,X_{n}\sim F_{\theta}\qquad\theta\in\Theta
	\end{equation*}
	where $\theta$ is a parameter living in the parameter space $\Theta$.
\end{enumerate}
In this setting $\Theta$ is finite-dimensional. This assumption has the implicit consequence that the information about the population can be represented by a finite-dimensional vector of $\Theta$ and this is actually a pretty strong assumption that puts us in the realm of \emph{parametric statistics}. To learn about $\Theta$ we will use:
\begin{itemize}
	\item point estimation;
	\item testing;
	\item confidence intervals.
\end{itemize}
\section{Random samples}
\subsection{Random vectors}
We will use columns to denote vectors:
\begin{equation*}
	X=\begin{bmatrix}
		X_{1}\\
		\vdots\\
		X_{k}
	\end{bmatrix}.
\end{equation*}
This is a \rv{} on a $k$=dimensional space with
\begin{equation*}
	\ev{X}=\begin{bmatrix}\ev{X_{1}}\\\vdots\\\ev{X_{k}}\end{bmatrix}.
\end{equation*}
We also have a variance structure that use the covariance matrix:
\begin{equation*}
	\var(X)=\begin{bmatrix}
		\var(X_{1})&\cov(X_{1},X_{2})&\cdots&\cov(X_{1},X_{k})\\
		\cov(X_{2},X_{1})&\var(X_{2})&\cdots&\cov(X_{2},X_{k})\\
		\vdots&\vdots&\ddots&\vdots\\
		\cov(X_{k},X_{1})&\cdots&\cdots&\var(X_{k})
	\end{bmatrix}
\end{equation*}.
We can compute expectation and variance on a linear map of the \rv: take a non random matrix $n\times k$ and a $n\times 1$ vector $b$:
\begin{align*}
	\mathbf{A}&=(a_{ij})\\
	b&=(b_{1},\ldots,b_{n})^{\trsp}.
\end{align*}
Take a \rv{} $X=(X_{1},\ldots,X_{n})^{\trsp}$ with $\mu=\ev{X}$ and $\mathbf{V}=\var(X)$. Define 
\begin{equation*}
	Y=\mathbf{A}X+b
\end{equation*}
as a linear transformation of $X$ which has
\begin{align*}
	\ev{Y}&=\mathbf{A}\mu+b\\
	\var(Y)&=\mathbf{AVA}^{\trsp}.
\end{align*}
\begin{proposition}
	The variance matrix $\mathbf{V}$ of a \rv{} $X$ is positive semidefinite; it is positive definite if there exists no vector $b$ other than $b=0$ such that $b^{\trsp}X$ is a degenerate \rv.
	\end{proposition}
	\begin{proposition}
		If $\mathbf{V}=\var(X)$ is positive definite, there exists a square matrix $\mathbf{C}$ such that $Y=\mathbf{C}X$ has uncorrelated components with $\var(Y)=\mathbf{I}_{k}$ (no covariance).
	\end{proposition}
	\begin{proposition}
		If $\mathbf{A}=(a_{ij})$ is a $k\times k$ matrix then
		\begin{equation*}
			\ev{X^{\trsp}\mathbf{A}X}=\mu^{\trsp}\mathbf{A}\mu+\trace{\mathbf{AV}}.
		\end{equation*}
	\end{proposition}
We recognize $X^{\trsp}\mathbf{A}X$ as a \emph{quadratic form}.
\subsection{Multivariate Gaussian (dimension $k$)}
A \emph{standard multivariate Gaussian \rv{}} is just a collection of i.i.d $\mathsf{N}(0,1)$. Consider a vector 
\begin{equation*}
	Z=(Z_{1},\ldots,Z_{k})^{\trsp}\qquad \begin{array}{l}
		Z_{i}\distnorm{0,1}\quad i=1,\ldots,k\\
		Z_{i}\indep Z_{j}\quad\every i\neq j.
	\end{array}
\end{equation*} Remember that this has variance structure $\mathbf{I}_{k}$.
Define $Y=\mathbf{A}Z+\mu$ for some non singular $k\times k$ matrix $\mathbf{A}$ and a $k\times 1$ vector $\mu$. This is just a mapping $\R^{k}\to\R^{k}$ but we want to know the distribution of $Y$. We know the distribution of $Z$
\begin{equation*}
	f_{Z}(t)=\frac{1}{(2\pi)^{\frac{k}{2}}}\expg{-\unmezz t^{\trsp}t}\indi_{\R^{k}}(t).
\end{equation*}
The general rule to change the variable is
\begin{equation*}
	f_{Y}(y)=F_{Z}(t)\left|\frac{\partial Z}{\partial Y}\right|
\end{equation*}
The inverse transformation of $Y$ is
\begin{equation*}
	Z=\mathbf{A}^{-1}(Y-\mu).
\end{equation*}
Since $Z$ is linearly dependent from $Y$, the Jacobian matrix of partial derivatives is 
\begin{equation*}
	\left|\frac{\partial Z_{i}}{\partial Y_{j}}\right|=\left|\left(\mathbf{A}^{-1}\right)\right|=|\mathbf{A}|^{-1}.
\end{equation*}
We know that
\begin{align*}
	\var(Y)=\mathbf{V}&=\mathbf{A}\mathbf{V}_{Z}\mathbf{A}^{\trsp}\\
	&=\mathbf{A}\mathbf{I}_{k}\mathbf{A}^{\trsp}\\
	&=\mathbf{A}\mathbf{A}^{\trsp}\\
	\implies&|\mathbf{A}|^{-1}=|\mathbf{V}|^{-\unmezz}
\end{align*}
Now we have everything we need to compute the distribution of $Y$. We can write
\begin{equation*}
	t^{\trsp}t=\left\{\mathbf{A}^{-1}(Y-\mu)\right\}^{\trsp}\left\{\mathbf{A}^{-1}(Y-\mu)\right\}
\end{equation*}
But since the transposition of the inverse is the inverse of the transposition we get
\begin{align*}
	t^{\trsp}t&=\left\{\mathbf{A}^{-1}(Y-\mu)\right\}^{\trsp}\left\{\mathbf{A}^{-1}(Y-\mu)\right\}\\
	&=(Y-\mu)^{\trsp}\ubracketthin{\left(\mathbf{A}^{-1}\right)^{\trsp}\mathbf{A}^{-1}}_{\mathclap{\left(\mathbf{A}^{-1}\mathbf{A}\right)=\mathbf{V}^{-1}}}(Y-\mu)\\
	&=(Y-\mu)^{\trsp}\mathbf{V}^{-1}(Y-\mu).
\end{align*}
So substituting into the density of $Z$ we get
\begin{equation*}
	f_{Y}(y)=\frac{1}{(2\pi)^{\frac{k}{2}}|\mathbf{V}|^{\unmezz}}\expg{-\unmezz(y-\mu)^{\trsp}\mathbf{V}^{-1}(y-\mu)}\indi_{\R^{k}}(y).
\end{equation*}
Many of those things were not explained by Favaro, great job! Now we can write
\begin{align*}
	\mu&=\ev{Y}\\
	&=\ev{\mathbf{A}Z+\mu}\\
	&=\mathbf{A}\ubracketthin{\ev{Z}}_{0}+\mu\\
	&=\mu
\end{align*}
and
\begin{align*}
	\mathbf{V}&=\var(Y)\\
	&=\var(\mathbf{A}Z+\mu)\\
	&=\mathbf{A}^{\trsp}\mathbf{I}_{k}\mathbf{A}\\
	&=\mathbf{A}^{\trsp}\mathbf{A}=\mathbf{V}.
\end{align*}
\begin{exercise}
	Compute the distribution of $X=\mathbf{B}Y+b$ where $\mathbf{B}$ is a non singular $k\times k$ matrix and $b$ is a $k\times 1$ vector.
\end{exercise}
\subsection{Chi-squared distribution}
If $Z=(Z_{1},\ldots,Z_{k})^{\trsp}\distnormk{0,\mathbf{I}_{k}}$ set then 
\begin{equation*}
	U_{k}=Z^{\trsp}Z=\sum_{i=1}^{n}Z_{i}^{2}.
\end{equation*}
This is the centered chi-squared \rv:
\begin{equation*}
	U_{k}\sim\chi^{2}_{k}
\end{equation*}
which is often notated
\begin{equation*}
	W\sim c\chi^{2}_{k}\iff \frac{W}{c}\sim\chi^{2}_{k}.
\end{equation*}
For $k=1$ we have 
\begin{align*}
	\pr(U_{1}\leq t)&=\pr\left(Z^{2}_{i}\right)\\
	&=\pr\left(\sqrt{t}\leq Z_{1}\leq\sqrt{t}\right)\\
	&=2\Phi\left(\sqrt{t}\right)-1
\end{align*}
so
\begin{align*}
	f_{U}(t)&=\frac{\dif}{\dt}\left(2\Phi\left(\sqrt{t}\right)\right)\\
	&=\frac{1}{\sqrt{2\pi}}\ubracketthin{t^{-\unmezz}e^{-\frac{t}{2}}}_{\mathrlap{\text{$\Gamma$ distr. function}}}\indi_{\R^{+}}(t)\\
	&=\frac{\left(\unmezz\right)^{\unmezz}}{\Gamma\left(\unmezz\right)}t^{\unmezz-1}e^{-\frac{t}{2}}\indi_{\R^{+}}(t)\distgamma{\frac{1}{2},\unmezz}.
\end{align*}
\begin{revise}
	This is clear if we think about the Gamma distribution function:
\begin{equation*}
	f(t)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}t^{\alpha-1}e^{-\beta t}\distgamma{\alpha,\beta}.
\end{equation*}
\end{revise}
Since the Gamma distribution is closed under convolution if the scale parameter (the second one) is the same, it is closed also under addition. If we take 
\begin{equation*}
	T_{i}\distgamma{\unmezz,\unmezz}\qquad\begin{array}{l}
		i=1,\ldots,k\\
		T_{i}\indep T_{j}
	\end{array}
\end{equation*}
then the sum is
\begin{equation*}
	U_{k}=\sum_{i=1}^{n}Z_{i}^{2}=\sum_{i=1}^{n}T_{i}\distgamma{\frac{k}{2},\unmezz}.
\end{equation*}
So the density of $U_{k}$ is
\begin{equation*}
	f_{U}(t)=\frac{\left(\unmezz\right)^{\frac{k}{2}}}{\Gamma\left(\frac{k}{2}\right)}t^{\frac{k}{2}}+e^{-\frac{t}{2}}\indi_{\R^{+}}(t).
\end{equation*}
We have
\begin{equation*}
	\ev{U_{k}}=k\qquad\var(U_{k})=2k.
\end{equation*}
Also, $\chi^{2}_{k}$ is closed under certain conditions:
\begin{equation*}
	\begin{array}{l}
		W_{r}\sim\chi^{2}_{r}\\
		U_{k}\sim\chi^{2}_k\\
		U_{k}\indep W_{r}
	\end{array}\implies W_{r}+U_{k}\sim\chi^{2}_{r+k}.
\end{equation*}
Fix a $n\geq1$ and take a collection of Gaussian i.i.d \rv s:
\begin{equation*}
	Y\distnorm{\mu,\sigma^{2}}\qquad\begin{array}{l}
		\mu\in\R\\
		\sigma^{2}\in\R^{+}.
	\end{array}
\end{equation*}
We define the \emph{sample mean} as
\begin{equation*}
	\Ybar_{n}=n^{-1}Y^{\trsp}\mathbf{1}_{n}=\frac{1}{n}\sum_{i=1}^{n}Y_{i}
\end{equation*}
And we define the \emph{corrected sample variance} as
\begin{align*}
	S^{2}_{n}&=(n-1)^{-1}\left(Y-\mathbf{1}_{n}\Ybar_{n}\right)^{\trsp}\left(Y-\mathbf{1}_{n}\Ybar_{n}\right)\\
	&=\frac{1}{n-1}\sum_{i=1}^{n}(Y_{i}-\Ybar_{n})^{2}.
\end{align*}
We also provided the matrix notation because it is useful in some situations. Consider the $n\times n$ matrix
\begin{equation*}
	\mathbf{A}=\begin{bmatrix}
		\frac{1}{\sqrt{n}}&\frac{1}{\sqrt{n}}&\frac{1}{\sqrt{n}}&\frac{1}{\sqrt{n}}&\ldots&\frac{1}{\sqrt{n}}\\
		\frac{1}{\sqrt{1\cdot2}}&-\frac{1}{\sqrt{1\cdot2}}&0&0&\ldots&0\\
		\frac{1}{\sqrt{2\cdot 3}}&\frac{1}{\sqrt{2\cdot 3}}&-\frac{2}{\sqrt{2\cdot 3}}&0&\ldots&0\\
		\vdots\\
		\frac{1}{\sqrt{n\cdot (n-1)}}&	\frac{1}{\sqrt{n\cdot (n-1)}}&	\frac{1}{\sqrt{n\cdot (n-1)}}&	\frac{1}{\sqrt{n\cdot (n-1)}}&\ldots&	-\frac{n-1}{\sqrt{n\cdot (n-1)}}
	\end{bmatrix}
\end{equation*}
This matrix is called the Hermelet matrix and it is constructed in such a way that every rows sums to 0 except for the first column, that sums to $\sqrt{n}$. It's a orthonormal matrix where the first row is proportional to vector $\mathbf{1}$. Now we have
\begin{equation*}
	Z=\mathbf{A}Y\sim\mathsf{N}_{n}\left(\mu_{Z},\sigma^{2}\mathbf{I}_{k}\right)
\end{equation*}
where
\begin{equation*}
	\mu_{Z}=\left(\mu\sqrt{n},0,0\ldots,0\right)^{\trsp}
\end{equation*}
We know that
\begin{align*}
	\ubracketthin{Z_{2}^{2}+\ldots+Z_{n}^{2}}_{n-1}&=Z^{\trsp}Z-Z_{1}^{2}\\
	&=\sum_{j=1}^{1}Y^{2}_{j}-n\Ybar_{n}^{2}\\
	&=(n-1)S^{2}_{n}
\end{align*}
so we proved that the sample \rv{} $Z_{2}^{2}+\ldots+Z^{2}_{n}$ is equal to the scaled sample variance. This means that
\begin{equation*}
	\frac{n-1}{\sigma^{2}}S^{2}_{n}\sim\chi^{2}_{n-1}
\end{equation*}
since it is a sum of $n-1$ Gaussian variables.
Why are the sample mean and the variance independent? We have that the sample mean is
\begin{equation*}
	\Ybar_{n}\distnorm{\mu,\frac{\sigma^{2}}{n}}
\end{equation*}
and therefore is independent from sample variance.
\begin{equation*}
	\Ybar_{n}\indep S^{2}_{n}.
\end{equation*}
If we have a random vector of i.i.d. \rv{} with independent sample mean and sample variance then they are multivariate Gaussian. 
\subsection{Quadratic form of a Gaussian}
These results are linked to the more general result of quadratic forms of a Gaussian distributions. We need a more general definition of the chi-squared distribution (\emph{non centered chi-square}). \begin{definition}
	Take $Z=(Z_{1},\ldots,Z_{n})\distnorm{\mu,\mathbf{I}_{k}}$ so not centered around 0. Take $$U_{k}=Z^{\trsp}Z=\sum_{i=1}^{n}Z_{i}^{2}.$$
This is the \emph{non-centered chi-squared \rv{}} with $k$ degrees of freedom and non-centrality $\delta=\mu^{\trsp}\mu$. We will write
\begin{equation*}
	U_{k}\sim\chi^{2}_{k}(\delta).
\end{equation*}
\end{definition}
\begin{proposition}
	If $Y\distnorm{N_{k}}(\mu,\mathbf{V})$ with $\mathbf{V}>0$ then
	\begin{equation*}
		\mathbf{Q}=Y^{\trsp}\mathbf{V}^{-1}Y\chi^{2}_{k}(\mu^{\trsp}\mathbf{V}^{-1}\mu).
	\end{equation*}
\end{proposition}
\begin{fancyproof}
	The proof of this proposition comes from the concept of spectral decomposition. We have
	\begin{equation*}
		\mathbf{V}=\mathbf{BB^{\trsp}}
	\end{equation*}
	so
	\begin{align*}
		\mathbf{Q}&=Y^{\trsp}\left(\mathbf{BB}\right)^{-1}Y\\
		&=\left(\mathbf{B}^{-1}Y\right)^{\trsp}\mathbf{B}^{-1}Y\\
		&=Z^{\trsp}Z
	\end{align*}
	where $\mathbf{Z}=\mathbf{B}^{-1}\mathbf{Y}\distnormk{\mathbf{B}^{-1}\mu,\mathbf{I}_{k}}$. But this means that by definition
	\begin{equation*}
		Z^{\trsp}Z\sim\chi^{2}_{k}(\delta)\qquad\text{with }\delta=\left(\mathbf{B}^{-1}\mu\right)^{\trsp}\mathbf{B}^{-1}\mu=\mu^{\trsp}\mathbf{V}^{-1}\mu.
	\end{equation*}
\end{fancyproof}
\begin{theorem}
	\emph{Fisher-Cochrane theorem}.  Take $Y\distnormk{\mu,\mathbf{I}_k}$ and let $\mathbf{A}_{1},\ldots,\mathbf{A}_{m}$ be positive semi-definite matrices with ranks $r_{1},\ldots,r_{m}$. If I sum all the matrix I must get the identity matrix so $\mathbf{A_{1}}+\ldots+\mathbf{A}_{m}=\mathbf{I}_{k}$. The following are equivalent:
	\begin{enumerate}
		\item $\mathbf{Q}_{j}=Y^{\trsp}A_{j}Y$ for $j=1,\ldots,m$ and 
		\begin{equation*}
			\mathbf{Q}_{j}\sim\chi^{2}_{r_{j}}(\mu^{\trsp}\mathbf{A}_{j}\mu)\text{ and }\mathbf{Q}_{j}\indep\mathbf{Q}_{i}\;\every j\neq i;
		\end{equation*}
		\item $\sum_{i=1}^{m}r_{i}=k$.
	\end{enumerate} 
\end{theorem}
\begin{fancyproof}
	\begin{enumerate}
		\item[$1\to2$] This is easy because
		\begin{align*}
			Y^{T}Y&=Y^{\trsp}\left(\mathbf{A}_{1}+\ldots+\mathbf{A}_{n}\right)Y\\
			&=\mathbf{Q}_{1}+\ldots+\mathbf{Q}_{m}
		\end{align*} 
		but since chi-squared is actually just a gamma distribution, adding up gammas makes up another gamma so
		\begin{equation*}
			\mathbf{Q}_{1}+\ldots+\mathbf{Q}_{m}\sim\chi^{2}_{r}(\delta)
		\end{equation*}
		with 
		\begin{equation*}
			r=\sum_{i=1}^{m}r_{i}\qquad\text{and}\qquad\delta=\mu^{\trsp}\mu.
		\end{equation*}
		Since we know that $Y^{\trsp}Y\sim\chi^{2}_{k}(\delta)$ then it must be that $r=k$.
		\item[$2\to1$] We write
		\begin{equation*}
			\mathbf{A}_{j}=\mathbf{B}_{j}\mathbf{B}_{j}^{\trsp}
		\end{equation*}
		where $B_{j}$ is a $k\times r_{j}$ matrix constructed from the spectral decomposition of $\mathbf{A}$ and attaching $m$ $k\times r_{i}$ for $i=1,\ldots,m$ matrices since I know that the sum of the ranks is $k$. So we get
		\begin{equation*}
		\underset{k\times k}{\mathbf{B}}=\begin{bmatrix}
				\mathbf{B}_{1},\mathbf{B}_{2},\ldots,\mathbf{B}_{m}.
			\end{bmatrix}
		\end{equation*}
		Now we have
		\begin{align*}
			\mathbf{B^{\trsp}B}&=\mathbf{B}_{1}\mathbf{B}_{1}^{\trsp}+\ldots+\mathbf{B}_{m}\mathbf{B}_{m}^{\trsp}\\
			&=\mathbf{A}_{1}+\ldots+\mathbf{A}_{m}\\
			&=\mathbf{I}_{k}.
		\end{align*}
		Now take
		\begin{equation*}
			Z=\mathbf{B}^{\trsp}Y\distnormk{\mathbf{B}^{\trsp}\mu,\mathbf{I}_{k}}.
		\end{equation*}
		Now we have
		\begin{align*}
			\mathbf{Q}_{j}&=Y^{\trsp}\mathbf{A}_{j}Y\\
			&=Y^{\trsp}\mathbf{B}_{j}\mathbf{B}_{j}^{\trsp}Y=Z_{j}^{\trsp}Z_{j}
		\end{align*}
	\end{enumerate}
	where $Z^{\trsp}=(Z^{\trsp}_{1},\ldots,Z^{\trsp}_{m})$. So
	\begin{equation*}
		Z_{j}\indep Z_{i}\qquad\every i\neq j
	\end{equation*}
	and $Z\sim\mathsf{N}_{\pi_{j}}(\mathbf{B}^{\trsp}_{j}\mu,\mathbf{I}_{k})$.
\end{fancyproof}
This result implies a simple proof of the distribution of mean and variance under Gaussian distribution.
\begin{remark}
	This result is often used with 
	\begin{equation*}
		\var(Y)=\sigma^{2}\mathbf{I}_{k}.
	\end{equation*}
	Just apply the theorem to $X=\sigma^{=1}Y$.
\end{remark}
\begin{remark}
	Suppose that $Y\distnormk{\mu\mathbf{1}_{k},\sigma^{2}\mathbf{I}_{k}}$ and we want to show the distribution of the sample mean and sample variance. Take
	\begin{align*}
		\mathbf{A}_{1}&=\left(\mathbf{I}_{n}-\frac{1}{n}\mathbf{1}_{n}\mathbf{1}_{n}^{\trsp}\right)\\
		\mathbf{A}_{2}&=\mathbf{I}_{n}-\mathbf{A}_{1}\\
		&=\frac{1}{n}\mathbf{1}_{n}\mathbf{1}_{n}^{\trsp}.
	\end{align*}
	We have
	\begin{align*}
		Y^{\trsp}\mathbf{A}_{1}Y&=\sum_{j=1}^{n}\left(Y_{j}-\Ybar_{2}\right)^{2}\\
		Y^{\trsp}\mathbf{A}_{2}Y&=n\left(\Ybar_{n}\right)^{2}	
	\end{align*}
	so not exactly the distributions we know, but we are getting close. $\mathbf{A}_{1}$ and $\mathbf{A}_{2}$ are idempotent with ranks $n-1$ for $\mathbf{A}_{1}$ and rank 1 for $\mathbf{A}_{2}$. I then apply the Fisher-Cochrane theorem to get
	\begin{equation*}
		Y^{\trsp}\mathbf{A}_{1}Y\sim\sigma^{2}\chi^{2}_{n-1}
	\end{equation*}
	and
	\begin{equation*}
			Y^{\trsp}\mathbf{A}_{2}Y\sim\sigma^{2}\chi^{2}_{1}\left(n\frac{\mu^{2}}{\sigma^{2}}\right).
	\end{equation*}
	So
	\begin{equation*}
			Y^{\trsp}\mathbf{A}_{1}Y\indep	Y^{\trsp}\mathbf{A}_{1}Y.
	\end{equation*}
	We are happy because we found the distribution of $	Y^{\trsp}\mathbf{A}_{1}Y$ and it is very similar to the distribution of the sample variance. We are also happy because we found the distribution of $	Y^{\trsp}\mathbf{A}_{1}Y$ (which is not the mean). Writing sample mean and sample variance in this way provides something that is not exactly sample mean and sample variance (but it is, up to transformation).
\end{remark}
\subsection{$t$-Student distribution and $F$ distribution}
\begin{definition}
	\emph{$t$-Student distribution}. Take $Z\distnorm{0,1}$ and $U\sim\chi^{2}_{k}$ with $Z\indep U$. We have that
	\begin{equation*}
		T=\frac{Z}{\sqrt{\frac{U}{k}}}\sim t_{k}.
	\end{equation*}
\end{definition}
I am taking something that lives on $\R$, dividing it for something that lives on $\R^{2}$ and $\R^{+}$ and getting something that lives on $\R$. Moments exists up to the $k-1$ order.\begin{itemize}
	\item  When we have $k=1$ we get the Cauchy distribution with no moments (very heavy tail behaviour).
	\item When we have $k=\infty$ we get $\mathsf{N}(0,1)$.
\end{itemize}
We can also create a non-centered version of the $t$-Student distribution. 
\begin{remark}
	If we take $T^{\star}=\frac{\Ybar_{n}\sqrt{n}}{S_{n}}\sim t_{n-1}$
\end{remark}
because $S_{n}$ has actually $n-1$ degrees of freedom.
\begin{definition}
	\emph{$F$ distribution}. Take $V\sim\chi^{2}_{m}$ and $U\sim\chi^{2}_{k}$ with $V\indep U$. Then
	\begin{equation*}
		F=\frac{\frac{V}{m}}{\frac{U}{k}}\sim F(m,k).
	\end{equation*}
\end{definition}
\subsection{Distribution of statistics}
When we do a test on 
\begin{equation*}
	T_{n}\quad\begin{cases}
		H_{0}&\mu=\mu_{0}\\
		H_{1}&\mu\neq\mu_{0}
	\end{cases}
\end{equation*}
we are actually doing something much harder than estimation, because we need to find a quantile. That's why all those distribution are tabulated because if to take quantiles we need an inversion to the cumulative distribution function and this is sometimes not possible in closed form. I need even more than just a distribution: I need a distribution for which I can find the quantile. But what if I do not even know the distribution? Normally we are testing
\begin{equation*}
	\pr(T_{n}>t)\leq1=\alpha
\end{equation*}
but we may be in a different situation and test
\begin{equation*}
	T(X_{1},\ldots,X_{2})\text{ with }T:\R^{2}\to\R
\end{equation*}
but we don't have the distribution of $(X_{1},\ldots,X_{2})$. We could use the central limit theorem but it is very slow because it works for $n\to\infty$ and you can prove that you actually need a huge $n$ for it to even start working. 
Remember the initial objective of statistical inference. We pick $n\geq1$ $(x_{1},\ldots,x_{n})$ and we are interested in learning about the population from where these numbers came from so we treat these numbers as the realization of a \rv{} $(X_{1},\ldots,X_{n})$ under assumptions of symmetry:
\begin{equation*}
	X\sim F_{\theta}\qquad\theta\in\Theta.
\end{equation*}
The classical example is flipping a coin many times to understand whether it is fair. In this framework we are actually able to write down the distribution of the sample (it is simply the product of the distribution of all sample elements). We usually write $f_{X}(\cdot;\theta)$ for continuous distributions and $p_{X}(\cdot;\theta)$ for discrete distributions. Let's denote the random sample as
\begin{equation*}
	\underline{x}:=(x_{1},\ldots,x_{n})
\end{equation*}
and its distribution as
\begin{equation*}
	f_{\underline{x}}(\underline{x};\theta)=\prod_{i=1}^{n}f_{x_{i}}(x_{i};\theta)\qquad\theta\in\Theta.
\end{equation*}
Typically 
\begin{equation*}
	(x_{1},\ldots,X_{n})\in\R^{n}
\end{equation*}
where $\R^{n}$ is typically large. The fact is that the information lives in a huge dimension so it is practically useless: if I see 100,000 coins result I understand nothing about it, but if someone tells me ``there were 50,000 heads'' (information in 1 dimension) then I can understand what we are talking about. This is why we typically employ a map
\begin{equation*}
	T_{n}=T(X_{1},\ldots,X_{n})\qquad t:\R^{n}\to\R^{m}\quad m<<n.
\end{equation*}
In this context we call $T_{n}$ a \emph{statistic} (like estimators and tests). But what is the distribution of $T_{n}$? Let's take into account:
\begin{enumerate}[\circnum]
	\item $\Xbar_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$ (\emph{sample mean});
	\item $\widetilde{S}^{2}_{n}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\Xbar_{n})^{2}$ (\emph{sample variance});
	\item $S^{2}_{n}=\frac{n}{n-1}\widetilde{S}^{2}_{n}$ (\emph{corrected sample variance});
	\item $M_{r,n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}$ (\emph{sample moment of order $r$});
	\item $X_{(n)}=\max\left\{X_{1},\ldots,X_{n}\right\}$ (\emph{sample max});
	\item $X_{(1)}=\min\left\{X_{1},\ldots,X_{n}\right\}$ (\emph{sample min}).
\end{enumerate}
How far can we go in finding distributions of these objects without specifying the distribution of the sample variable?
Let's take, for example, the sample max.
\begin{align*}
	\pr(X_{(n)}\leq t)&=\pr(\left\{X_{1}\leq t\right\}\cap\left\{X_{2}\leq t\right\}\cap\ldots\cap\left\{X_{n}\leq t\right\})\\
	&=\prod_{i=1}^{n}\pr\left(\left\{X_{i}\leq t\right\}\right)\\
	&=\left(F_{X}(t)\right)^{n}.
\end{align*}
To get the density just differentiate:
\begin{align*}
	f_{X_{(n)}}(t)&=\frac{\dif}{\dt}\left(F_{X}(t)\right)^{n}\\&=n\left(F_{X}(t)\right)^{n-1}f_{X}(t).
\end{align*}
For the minimum is the same:
\begin{align*}
	\pr(X_{(n)}\geq t)&=\pr(\left\{X_{1}\geq t\right\}\cap\left\{X_{2}\geq t\right\}\cap\ldots\cap\left\{X_{n}\geq t\right\})\\
	&=\prod_{i=1}^{n}\pr\left(\left\{X_{i}\geq t\right\}\right)\\
	&=\left(1-F_{X}(t)\right)^{n}.
\end{align*}
Sane goes for the density. What we found is that for minimum and maximum we can easily find the distribution without knowing anything about the distribution of the sample (we leave it as $f$ or $F$ if it's cumulative). For the sample mean is different:
\begin{itemize}
	\item Gaussian models: we already found it;
	\item Bernoulli models: we can find it, we just disintegrate;
	\item Gamma models: we can find it, since it is closed under distribution;
	\item Exponential models: sum of exponential it's just a Gamma distribution, it's okay;
	\item Poisson: we can get it;
	\item Uniform: not so easy. If we sum more uniforms $\mathsf{U}[0,1]$ we get more than 1 and we get out of domain;
	\item Beta: same as before.
\end{itemize}
Every time the support is bounded we may incur in some problems. But what about the sample variance? This is even more complex because it is non linear and there is the dependence from the mean:
\begin{itemize}
	\item Gaussian models: we already found it;
	\item \textit{that's it}.
\end{itemize}
Order statistics (like minimum and maximum) are always the least problematic among the statistics. But what else can we do? We can do asymptotic analysis, that is to study what happens to $T_{n}$ as $n\to\infty$ and our main tool is the Central Limit Theorem to get a Gaussian asymptotic statistic. The formulation we will use is the Lindberg-Levy CLT which only assumes that the variance is bounded. This is a very qualitative result because we are claiming that, up to rescaling and centering, \begin{equation*}
	T_{n}\approx\mathsf{N}(\cdot,\cdot)
\end{equation*} when $n$ is large but the problem is that this is slow. How large must $n$ so that I am ``close enough'' to the \textit{actual} Gaussian? 100? 1000? What if $10^{10}$ is not large enough? And what does ``enough'' even mean? And why haven't I felt truly happy in 10 years?\par
Thing is, we need to somehow find a distance between $F_{n}$, the CDF of $T_{n}$, and $\Phi$, the CDF of a Gaussian standard variable. We pick the Kolmogorov distance (other distances still work) to which we want to find an upper bound:
\begin{equation*}
	\sup_{X}\left|F_{n}(x)-\Phi(x)\right|\leq cg(n).
\end{equation*}
This is the ``quantitative'' version of the CLT theorem and it's called the Berry-Esseen formulation. We will see that with specific distances we will be able to easily see the impact of the sample size on the distance (the supremum distance is very punishing). 
\subsection{Concentration inequalities}
If we don't want an asymptotic perspective then we need to use the \emph{concentration inequalities} that are not an asymptotic result. These inequality basically provide information about the tail behaviour of sum of independent \rv s (not necessarily independent!): if I have a result like
\begin{equation*}
	\pr(T(X_{1},\ldots,X_{n})>\varepsilon)\leq e^{-\text{something}}
\end{equation*}
then the situation is different because all the information is in the tail of the distribution. We can see that Chebyshev's inequality is actually a concentration inequality simply as an application of the Markov inequality.
\begin{proposition}
	\emph{Markov's inequality}. For a non-negative \rv{} $X$ and $t>0$
	\begin{equation*}
		\pr(X\geq t)\leq\frac{\ev{X}}{t}.
	\end{equation*}
\end{proposition}
Markov's inequality sets an upper bound for the anti-CDF of $X$. If we take a (strctly) non-increasing function $\phi$ with $\phi>0$ then 
\begin{equation*}
	\pr[X\geq t]=\pr(\phi(x)\geq\phi(t))\leq\frac{\ev{\phi(x)}}{\phi(t)}.
\end{equation*}
This implies Chebyshev's inequality with $\phi(x)=x^{2}$.
\begin{proposition}
	We have
	\begin{align*}
		\pr(|X-\ev{X}|\geq t)&=\pr\left((X-\ev{X})^{2}\geq t^{2}\right)\\
		&\leq\frac{\ev{(X-\ev{X})^{2}}}{t^{2}}=\frac{\var(X)}{t^{2}}.
	\end{align*}
\end{proposition}
This is a concentration inequality because it tells us how much $X$ is concentrated around the mean. If $\phi(x)=x^{q}$ with $q>0$ then
\begin{equation*}
	\pr(|X-\ev{X}|\geq t)\leq\frac{\ev{|X-\ev{X}|^{q}}}{t^{q}}
\end{equation*}
we have a generalization of Chebyshev's inequality. We could even improve further the situation by making this inequality a function of $q$ and finding which $q$ optimizes the upper bound (we are ``squeezing'' the bound). The problem is that generally working with moments is complicated. For example, if we try to compute the moment of order $r$ of a sum of random variables we have a summation all raised to $r$, which is ugly and absolutely not sexy. The trick is that instead of taking $\phi(x)=x^{q}$ we take
\begin{equation*}
	\phi(x)=\expg{sx}\qquad s>0.
\end{equation*}
Now we have
\begin{align*}
	\pr(X\geq t)&=\pr(\expg{sx}\geq\expg{st})\\
	&\leq\frac{\ev{\expg{sx}}}{e^{st}}
\end{align*}
so our upper bound is
\begin{equation*}
	\pr(X\geq t)\leq\frac{\ev{\expg{sx}}}{e^{st}}.
\end{equation*}
Is this easier to compute? Maybe for one \rv{} it doesn't look like it, but take a sum of \rv s:
\begin{equation*}
	\sum_{i=1}^{n}X_{i}\qquad X_{i}\indep X_{j}\;\every i\neq j
\end{equation*}
gives us
\begin{align*}
	\pr\left(\sum_{i=1}^{n}X_{i}\geq t\right)&\leq\frac{\ev{\expg{s\sum_{i=1}^{n}X_{i}}}}{e^{st}}\\
	&=\frac{\prod_{i=1}^{n}\ev{\expg{sX_{i}}}}{e^{st}}
\end{align*}
and this is called \emph{Chernoff bounding} and it lets me get the upper bound to deviation without needing to compute the moment of a sum of \rv s but just with the moments of the single \rv s, which is much sexier \& better. Given how often we will work with sum of \rv s this is pretty important.\par
Assume that 
\begin{equation*}
	X_{1},\ldots,X_{n}
\end{equation*}
are independent (not identically distributed) \rv s and denote
\begin{equation*}
	S_{n}=\sum_{i=1}^{n}X_{i}.
\end{equation*}
We want to control the quantity
\begin{equation*}
	\pr\left(S_{n}-\ev{S_{n}}\geq t\right)\leq ?
\end{equation*}
\begin{enumerate}
	\item Apply Chebyshev to get
	\begin{align*}
		\pr\left(|S_{n}-\ev{S_{n}}|\geq t\right)&\leq\frac{\var(S_{n})}{t^{2}}\\
		&=\frac{\sum_{i=1}^{n}\var(X_{i})}{t^{2}}
	\end{align*}
	if $\sigma^{2}=\frac{1}{n}\sum_{i=1}^{n}\var(X_{i})$ (we will need to use this $\frac{1}{n}$ later) then we get
	\begin{equation*}
		\pr\left(\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\ev{X_{i}}\right|>\varepsilon\right)\leq\frac{\sigma^{2}}{n\varepsilon^{2}}.
	\end{equation*}
	So this is the rate of the deviation from the mean according to Chebyshev.	This is also the proof of (weak) law of large numbers.
	\item Now apply the CLT to $S_{n}$:
	\begin{align*}
		\pr\left(\sqrt{\frac{n}{\sigma^{2}}}\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}-\ev{X_{i}}\right)\geq y\right)\to 1-\Phi(Y).
	\end{align*}
	Let's now compute the anti-CDF $1-\Phi(y)$ which will be the right tail of a Gaussian distribution.
	\begin{equation*}
		1-\Phi(y)=\int_{y}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\dx.
	\end{equation*}
	We cannot compute directly this quantity but we can find an upper bound.
	\begin{align*}
		1-\Phi(y)&=\int_{y}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}}\dx\qquad t=x-y\\
		&=\frac{1}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\int_{0}^{+\infty}\ubracketthin{e^{-\frac{t^{2}}{2}}}_{\leq1}e^{-ty}\dt\\
			&\leq\frac{1}{\sqrt{2\pi}}e^{-\frac{y^{2}}{2}}\int_{0}^{\infty}e^{-ty}\dt\\
			&=\frac{e^{-\frac{y^{2}}{2}}}{\sqrt{2\pi}y}.
	\end{align*}
	So our result is
	\begin{equation*}
			\pr\left(\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\ev{X_{i}}\right|>\varepsilon\right)\leq\frac{e^{-\frac{y^{2}}{2}}}{\sqrt{2\pi}y}.
	\end{equation*}
	So we now have to write this problem as
	\begin{equation*}
		\pr\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}-\ev{X_{i}}\geq\varepsilon\right)\lessapprox\expg{-\frac{n\varepsilon^{2}}{2\sigma^{2}}}.
	\end{equation*}
\end{enumerate}
So the CLT is better because it has an exponential rate (exponential decay)! Chebyshev can't do this. We need to find more concentration inequalities that ``agree'' with this behaviour shown by the CLT, only not asymptotically. We will be able to get an upper bound with this exact form (without $\lessapprox$!). So to recap we do
\begin{equation*}
	\begin{array}{>{\displaystyle}c}
		\pr(\left|S_{n}-\ev{S_{n}}\right|\geq t)\leq\frac{n\sigma^{2}}{t^{2}}\\
		\Downarrow\\
		\pr\left(\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\ev{X_{i}}\right|\geq \varepsilon\right)\leq\frac{\sigma^{2}}{n\varepsilon^{2}}\\
		\Downarrow\\
		\pr\left(\sqrt{\frac{n}{\sigma^{2}}}\left(\frac{1}{n}\sum_{i=1}^{n}-\ev{X}\right)\geq y\right)\leq\frac{1}{\sqrt{2\sigma}}\frac{1}{y}\expg{-\frac{y^{2}}{2}}\\
		\Downarrow\\
			\pr\left(\frac{1}{n}\sum_{i=1}^{n}-\ev{X}\geq\ubracketthin{ y\left(\frac{n}{\sigma^{2}}\right)^{\frac{1}{2}}}_{\text{call this }\varepsilon}\right)\leq\frac{1}{\sqrt{2\sigma}}\frac{1}{y}\expg{-\frac{y^{2}}{2}}\\
		\Downarrow\\
			\pr\left(\frac{1}{n}\sum_{i=1}^{n}-\ev{X}\geq\varepsilon\right)\lessapprox\expg{-\frac{n\varepsilon^{2}}{2\sigma^{2}}}.
	\end{array}
\end{equation*}
How do we preserve this bound with its very vast decay but in a non-asymptotic way? We have to use the Chernoff bounding: we choose $\phi(x)=e^{sx}$ and plug it in our Chebyshev inequality. We get
\begin{align*}
	\pr\left(S_{n}-\ev{S_{n}}\geq t\right)&\leq e^{-st}\ev{\expg{s\sum_{i=1}^{n}\left(X_{i}-\ev{X_{i}}\right)}}\\
	&=e^{-st}\prod_{i=1}^{n}\ev{\expg{S\left(X_{i}-\ev{X_{i}}\right)}}\quad\text{\footnotesize by indep.}
\end{align*}
Si we now have just an an analytical problem. We will need two famous inequalities (we will only use the result for bounded and independent \rv s):
\begin{itemize}
	\item \emph{Hoeffding inequality}.
	\begin{proposition}
		Let $X$ be a random variable and for simplicity let's assume that $\ev{X}=0$ and assume $a\leq X\leq b$. Then, for $s>0$ 
		\begin{equation*}
			\ev{\expg{sX}}\leq\expg{\frac{s^{2}(b-a)^{2}}{8}}.
		\end{equation*}
	\end{proposition} \begin{fancyproof}
	By convexity of the exponential function we have that
	\begin{equation*}
		e^{sx}\leq\frac{x-a}{b-a}e^{sb}+\frac{b-x}{b-a}e^{sa}\qquad a\leq x\leq b
	\end{equation*}
	so taking the expectation we have 
	\begin{align*}
		\ev{e^{sx}}&\leq\frac{b}{b-a}e^{sa}-\frac{a}{b-a}e^{sb}.
	\end{align*}
	Denote $p=-\frac{a}{b-a}$:
	\begin{align*}
		\ev{e^{sx}}&\leq(1-p+pe^{s(b-a)})e^{-ps(b-a)}.
	\end{align*}
	Denote $u=s(b-a)$ so that 
	\begin{equation*}
		\phi(u)=-pu+\log(1-p+pe^{u})=\expg{\phi(u)}
	\end{equation*}
	so that we get
	\begin{equation*}
		\ev{e^{sx}}\leq\expg{\phi(u)}.
	\end{equation*}
	Now we have
	\begin{equation*}
		\phi'(u)=\frac{\dif}{\du}\phi(u)=-p+\frac{p}{p+(1-p)e^{-u}}.
	\end{equation*}
	When $u\to0$ we get
	\begin{equation*}
		\phi(u)=\phi'(u)=0.
	\end{equation*}
	Now we need the second derivative and use Taylor:
	\begin{equation*}
		\phi''(u)=\frac{\dif}{\du}\phi'(u)=\frac{p(1-p)e^{-u}}{\left(p+(1-p)e^{-u}\right)^{2}}.
	\end{equation*}
	Since I don't want to do derivatives for eternity, I look for an upper bound of the second derivative.
	When $u\to0$ we have $\phi''(u)\to p(1-p)$ which is bounded by $\frac{1}{4}$ and when $u\to\infty$... it's the same. so our bounds are
	\begin{equation*}
		\begin{array}{c}
			\phi'(0)=\phi(0)\\
			\phi''(u)\leq\frac{1}{4}.
		\end{array}
	\end{equation*}
	Take $\theta\in[0,u]$. We get
	\begin{align*}
		\phi(u)&=\ubracketthin{\phi(0)}_{=0}+u\ubracketthin{\phi'(0)}_{=0}+\ubracketthin{\frac{u^{2}}{2}\phi''(\theta)}_{\leq\frac{s^{2}(b-a)^{2}}{8}}.
	\end{align*}
	So we have 
	\begin{align*}
		\pr\left(S_{n}-\ev{S_{n}}\geq t\right)&\leq e^{-st} \prod_{i=1}^{n}\expg{s^{2}(b_{i}-a_{i})^{2}\frac{1}{8}}\\
		&= e^{-st}\expg{\frac{s^{2}}{8}\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}.
	\end{align*}
	$s$ is a free parameter here, corresponding to the index of the m.g.f. We can optimize for $s$ to find the $s$ that gives us the tighter bound and we get
	\begin{equation*}
		s=\frac{4t}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}.
	\end{equation*}
	\end{fancyproof}
	Plugging the optimal $s$ into the equation gives us the complete inequality.
	\begin{theorem}
		\emph{Hoeffding inequality}. Let $(X_{i},\ldots,X_{n})$ be independent \rv s such that $X_{i}\in[a_{i},b_{i}]$ a.s. Then for any $t\geq0$ we get
		\begin{equation*}
			\pr\left(S_{n}-\ev{S_{n}}\geq t\right)\leq\expg{-\frac{2t^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}}
		\end{equation*}
		and the other tail is
		\begin{equation*}
			\pr\left(S_{n}-\ev{S_{n}}\leq -t\right)\leq\expg{-\frac{2t^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}}.
		\end{equation*}
	\end{theorem}
	This is good because $t$ is here, so when we divide by $n$ our $\varepsilon$ becomes $tn$ but we are missing the variance! We need a result that improves on this and includes the information about the variance.
	\item \emph{Bernstein inequality}. We still want to bound $\ev{\expg{sX_{i}}}$ with something.
		Let $X_{1},\ldots, X_{n}$ be independent \rv s such that $\ev{X_{i}}=0$ (and therefore $\ev{X_{i}^{2}}=\sigma^{2}$) for $i=1,\ldots,n$. Define
		\begin{equation*}
			F_{i}=\sum_{r=2}^{\infty}\frac{s^{r-2}\ev{X_{i}^{r}}}{r!\sigma^{2}_{i}}.
		\end{equation*}
		Since 
		\begin{equation*}
			\expg{sx}=1+sx+\sum_{r=2}^{\infty}\frac{s^{r}x^{r}}{r!}
		\end{equation*}
		take the expected value of this last equality:
		\begin{align*}
			\ev{e^{sX_{i}}}&=1+s\ubracketthin{\ev{X_{i}}}_{=0}+\sum_{r=2}^{\infty}\frac{s^{r}\ev{X_{i}^{r}}}{r!}\\
			&=1+s^{2}\sigma^{2}_{i}F_{i}\\
			&\leq\expg{s^{2}\sigma^{2}_{i}F_{i}}.
		\end{align*}
		We use the assumption that $|X_{i}|<c$ so that for $r\geq 2$
		\begin{align*}
			\ev{X_{i}^{r}}&=\ev{X_{i}^{r-2}X_{i}^{2}}\\
			&\leq c^{r-2}\sigma^{2}_{i}.
		\end{align*}
		So we have a bound for the m.g.f. of order $r$ so I can substitute that in $F$:
		\begin{align*}
			F_{i}&\leq\sum_{r=2}^{\infty}\frac{s^{r-2}c^{r-2}\sigma^{2}_{i}}{r!\sigma^{2}_{i}}\\
			&=\frac{1}{(sc)^{2}}\sum_{r=2}^{\infty}\frac{(sc)^{r}}{r!}\\
			&=\frac{e^{sc}-1-sc}{(sc)^{2}}.
		\end{align*}
		So now we have 
		\begin{equation*}
			\ev{\expg{sX_{i}}}\leq e^{s^{2}\sigma^{2}_{i}\frac{e^{sc}-1-sc}{(sc)^{2}}}.
		\end{equation*}
		Since we know that 
		\begin{equation*}
			\sigma^{2}=\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}^{2}
		\end{equation*}
		then
		\begin{equation*}
			\pr\left(\sum_{i=1}^{n}X_{i}>t\right)\leq\expg{n\sigma^{2}\frac{e^{sc}-1-sc}{\sigma^{2}}-st}.
		\end{equation*}
		Again, we can choose the $s$ which minimizes the bound
		\begin{equation*}
			s=\frac{1}{c}\log\left(1+\frac{tc}{n\sigma^{2}}\right).
		\end{equation*}
		Now we can get the complete inequality.
		\begin{theorem}
			\emph{Bernstein inequality}. Let $X_{i},\ldots,X_{n}$ be independent \rv s with $\ev{X_{i}}=0$ and $|X_{i}|\leq c$. Let
			\begin{equation*}
				\sigma^{2}=\frac{1}{n}\sum_{i=1}^{n}\var(X_{i}).
			\end{equation*}
			Then for any $t>0$
			\begin{equation*}
				\pr\left(\sum_{i=1}^{n}X_{i}>t\right)\leq\expg{-\frac{n\sigma^{2}}{c^{2}}h\left(\frac{ct}{n\sigma^{2}}\right)}
			\end{equation*}
			where
			\begin{equation*}
				h(u)=(1+u)\log(1+u)\qquad u\geq0.
			\end{equation*}
		\end{theorem}
\end{itemize}
\begin{remark}
	These bounds are the tightest possible but they don't look very similar to what we found with CLT. What we can do is bounding the function $h(u)$: this will punish us by enlarging the bound but we will get closer to the CLT result.
We know that
\begin{equation*}
	h(u)\geq\frac{u^{2}}{2+\frac{2u}{3}}
\end{equation*}
so, plugging this in the Bernstein inequality, we get
\begin{equation*}
	\pr\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}>\varepsilon\right)\leq\expg{-\frac{n\varepsilon^{2}}{2\sigma^{2}+\frac{2ct}{3}}}.
\end{equation*}
\end{remark}
This is the same result of the CLT but with the difference of $\frac{2ct}{3}$. We could show that looking for the lower bound we get that the lower bound matches the upper one up to a constant and this means that this upper bound is the optimal rate of convergence (it can't be improved). We will use a lot of these things in hypothesis testing and conformal prediction.
\section{Maximum Likelihood Estimation}
\subsection{Definition of likelihood function}
Let $(X_{1},\ldots,X_{n})$ be a random sample $(\underline{X})$ form
\begin{equation*}
	X\sim F_{\theta}\qquad \theta\in\Theta.
\end{equation*}
Think about an example with $F_{\theta}$ discrete, like flipping a coin (Bernoulli model). Fix $\theta^{\star}\in\Theta$ as a ``starting point'' and compute
\begin{equation*}
	\pr\left(\bigcap_{i=1}^{n}\left\{X_{i}=x_{i}\right\}\right)=\prod_{i=1}^{n}p_{x_{i}}(x_{i};\theta^{\star}).
\end{equation*}
What information do we get from this? This is a function of the unknown parameter $\theta\in\Theta$, so I can actually write it as a function of $\theta$. We can do this because in this situation we \textit{don't} have $\theta$ but we \textit{do} have $x_{i}$'s (when we compute probabilities we are in the opposite situation). So here this function
\begin{equation*}
	\prod_{i=1}^{n}p_{x_{i}}(x_{i};\theta)
\end{equation*}
tells us \textit{how likely it is for $\theta$ to generate $x_{i}$}. Take $\theta_{1},\theta_{2}\in\Theta$ and take the observed sample $\underline{x}$. If we have
\begin{equation*}
	\prod_{i=1}^{n}p_{x_{i}}(x_{i};\theta_{1})\geq\prod_{i=1}^{n}p_{x_{i}}(x_{i};\theta_{2})
\end{equation*}
then this means that $\theta_{1}$ is more likely to have generated $\underline{x}$.
So the joint distribution function, when looked at from this point of view, is called \emph{likelihood function}. We are allowed to think this way because we are operating in a frequentist setting that allows us to assume that there exists a ``true'' $\theta$ that generates the data.
\begin{definition}
	Given $(X_{1},\ldots,X_{n})$ from $X\sim F_{\theta}$ with $\theta\in\Theta$. The \emph{likelihood function} for the model $F_{\theta}$ is
	\begin{equation*}
		\mathcal{L}(\theta;\underline{x})=\prod_{i=1}^{n}f_{x_{i}}(x_{i};\theta).
	\end{equation*}
\end{definition}
So now finding the optimal $\theta^{\star}$ is just a problem of maximization. We need to find
\begin{equation*}
	\theta^{\star}=\argmax_{\Theta}\mathcal{L}(\theta;\underline{x}).
\end{equation*}
For example, take $(X_{1},\ldots,X_{n})$ from $X\distbernoulli{\theta}$ with $\theta\in(0,1)$. Observe $(x_{1},\ldots,x_{n})\in\{0,1\}^{n}$. Our likelihood function is:
\begin{align*}
	\pr\left(\bigcap_{i=1}^{n}\left\{X_{i}=x_{i}\right\}\right)&=\prod_{i=1}^{n}\theta^{x_{i}}(1-\theta)^{1-x_{i}}\\
	&=\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{n-\sum_{i=1}^{n}x_{i}}\\
	&=\mathcal{L}(\theta;\underline{x}).
\end{align*}
This is a bit hard to compute sometimes, so we typically take the logarithm:
\begin{equation*}
		\theta^{\star}=\argmax_{\Theta}\mathcal{L}(\theta;\underline{x})\iff	\theta^{\star}=\argmax_{\Theta}\log\mathcal{L}(\theta;\underline{x}).
\end{equation*}
This is good because
\begin{equation*}
	\mathcal{L}(\theta;\underline{x})=\prod_{i=1}^{n}f_{x_{i}}(x_{i};\theta)\iff\log\mathcal{L}(\theta;\underline{x})=\sum_{i=1}^{n}\log f_{x_{i}}(x_{i};\theta).
\end{equation*}
This is much better because we go from a product to a sum and the logarithm often helps us simplifying exponential terms. In our Bernoulli case we have
\begin{equation*}
	\log\mathcal{L}(\theta;\underline{x})=\left(\sum_{i=1}^{x}x_{i}\right)\log\theta+\left(n-\sum_{i=1}^{n}x_{i}\right)\log(1-\theta).
\end{equation*}
To find the maximum we need the first derivative
\begin{equation*}
	\frac{\dif}{\dif\theta}\log\mathcal{L}(\theta,\underline{x})=\frac{1}{\theta}\sum_{i=1}^{n}x_{i}-\left(n-\sum_{i=1}^{n}x_{i}\right)\frac{1}{1-\theta}.
\end{equation*}
We put it equal to 0 and then solve for $\theta$:
\begin{equation*}
	\frac{1}{\theta}\sum_{i=1}^{n}x_{i}-\left(n-\sum_{i=1}^{n}x_{i}\right)\frac{1}{1-\theta}=0\iff\theta^{\star}=\frac{1}{n}\sum_{i=1}^{n}x_{i}.
\end{equation*}
In this case the problem was easy enough to solve. Maybe though there are situations where we cannot solve the equation in closed form, in which case we must solve it with numerical methods. A clear example of this is the Gamma model. In general this method holds if the model is \emph{regular}.
\subsection{Regular models}
If our model satisfies these assumptions then we can use maximum likelihood estimation.
\begin{definition}
	A model is \emph{regular} if:
	\begin{enumerate}[\circnum]
		\item $\theta\in\Theta$ where $\Theta$ is an open real set;
		\item for any $\theta\in\Theta$ there exist derivatives of the likelihood functions with respect to $\theta$ up to order $3$;   
		\item for any $\theta_{0}\in\Theta$ there exist the functions $g(\cdot),h(\cdot),H(\cdot)$ integrable around $\theta_{0}$ such that they bound the derivative:
		\begin{equation*}
			\begin{array}{>{\displaystyle}l}
				\left|\frac{\dif}{\dif\theta}f_{X}(x;\theta)\right|\leq g(x)\qquad x\in\R\\
				\left|\frac{\dif^{2}}{\dif\theta^{2}}f_{X}(x;\theta)\right|\leq h(x)\qquad x\in\R\\		
				\left|\frac{\dif^{3}}{\dif\theta^{3}}\log f_{X}(x;\theta)\right|\leq H(x)\qquad x\in\R;\\		
			\end{array}
		\end{equation*}
		\item since we can write $\frac{\dif}{\dif\theta}\log\mathcal{L}(\theta;\underline{X})$ (with the capital $\underline{X}$, which means that this object becomes a random variable) we must have
		\begin{equation*}
			0<\ev{\left(\frac{\dif}{\dif\theta}\log\mathcal{L}(\theta;\underline{X})\right)^{2}}<\infty.
		\end{equation*}
	\end{enumerate}
\end{definition}
This is not the only approach to statistics, but it surely is one of the most intuitive and when applicable it boils down to a simple optimization problem. The only problem are derivatives... What about when $\Theta=\N$, for example when we want to estimate the number of trials $n$ that gives us a certain probability of success? This is clearly not a smooth function. Anyway, from now on we will assume regularity. \begin{definition}
	Consider a random sample $(X_{1},\ldots,X_{n})$ from $X\sim F_{\theta}$ with $\theta\in\Theta$. Consider the random variable
\begin{equation*}
	V_{n}(\theta)=\log\mathcal{L}(\theta;\underline{X}).
\end{equation*}
The \emph{score function} is the first derivative (with respect to $\vartheta$) of $V_{n}(\theta)$
\begin{equation*}
	V'_{n}(\theta):=\frac{\dif}{\dif\theta}V_{n}(\theta)=\frac{\mathcal{L}'(\theta;\underline{X})}{\mathcal{L}(\theta;\underline{X})}.
\end{equation*}

\end{definition}
We work in a continuous setting (for discrete, just swap integrals with summations). Consider
\begin{align*}
	\ev{V'_{n}(\theta)}&=\int_{\R^{n}}V'_{n}(\theta)f_{\underline{X}}(\underline{X};\theta)\dif\underline{x}\\
	&=\int_{\R}\frac{f'_{\underline{x}}(\underline{x};\theta)}{\cancel{f_{\underline{x}}(\underline{x},\theta)}}\cancel{f_{\underline{x}}(\underline{x},\theta)}\dif\underline{x}\\
	&=\int_{\R^{n}}\frac{\dif}{\dif \theta}\int_{\R^{n}}f_{\underline{x}}(\underline{x};\theta)\dif\underline{x}\\
	&=\frac{\dif}{\dif\theta}\int_{\R^{n}}f_{\underline{x}}(\underline{x};\theta)\dif\underline{x}\\
	&=\frac{\dif}{\dif\theta}1=0.
\end{align*}
We now check the second moment:
\begin{align*}
	V''_{n}(\theta)&=\frac{\dif^{2}}{\dif\theta^{2}}\log\mathcal{L}(\theta;\underline{x})\\
	&=\frac{\dif}{\dif\theta}\frac{f'_{\underline{x}}(\underline{x};\theta)}{f_{\underline{x}}(\underline{x},\theta)}\\
	&=\frac{f''_{\ulx}(\ulx;\theta)f_{\ulx}(\ulx;\theta)-\left(f_{\ulx}(\ulx;\theta)\right)^{2}}{\left(f_{\ulx}(\ulx;\theta)\right)^{2}}\\
	&=\frac{f''_{\ulx}(\ulx;\theta)}{f_{\ulx}(\ulx;\theta)}-\left(\frac{f'_{\ulx}(\ulx;\theta)}{f_{\ulx}(\ulx;\theta)}\right)^{2}.
	\end{align*}
Now we get
\begin{equation*}
	V''_{n}(\theta)=\frac{f''_{\ulx}(\ulx;\theta)}{f_{\ulx}(\ulx;\theta)}-\left(V'_{n}(\theta)\right)^{2}
\end{equation*}
So 
\begin{align*}
	\ev{V''_{n}(\theta)}&=\int_{\R^{n}}\frac{f''_{\ulx}(\ulx;\theta)}{\cancel{f_{\ulx}(\ulx;\theta)}}\cancel{f_{\ulx}(\ulx;\theta)}\dif\ulx-\ev{(V'_{n}(\theta))^{2}}\\
	&=\ubracketthin{\int_{\R^{n}}\frac{\dif^{2}}{\dif\theta^{2}}f_{\ulx}(\ulx;\theta)\dif\ulx}_{0}-\ev{\left(V'_{n}(\theta)\right)^{2}}
\end{align*}
which means
\begin{equation*}
	\var(V_{n}(\theta))=\ev{\left(V'_{n}(\theta)\right)^{2}}=-\ev{V''_{n}(\theta)}.
\end{equation*}
This is called \emph{Fisher information}.
\begin{definition}
	The \emph{Fisher information} of a random sample from a regular model is
	\begin{equation*}
		I_{n}(\theta)=\var(V'_{n}(\theta))=-\ev{V''(\theta)}.
	\end{equation*}
\end{definition}
\begin{remark}
	Try to prove the identity
	\begin{equation*}
		I_{n}(\theta)=nI_{1}(\theta)
	\end{equation*}
	where $I_{1}$ is the fisher information of the single \rv.
\end{remark}
What is the interpretation of the Fisher information? The Fisher information is the expectation of a second derivative and second derivatives are usually linked to the curvature of a space. There is a huge literature dedicated to how the Fisher information defines a Riemannian geometry on the space of the model (which is non-euclidean!).\par
\subsection{Exponential family}
There is a huge class of regular models with which we can deal all in the same way, writing them in the same way. 
\begin{definition}
	We say that a model $X$ (continuous or discrete) belongs to the \emph{exponential family} if the density function or the mass function of the distribution of $X$ has the following form:
	\begin{equation*}
		\expg{Q(\theta)\cdot A(x)+c(x)-k(\theta)}
	\end{equation*}
	where $Q(\cdot),A(\cdot),c(\cdot),k(\cdot)$ are functions.
\end{definition}
So we try to pick a model we already know and try to write it in this way. Pick, for example, the Bernoulli model: does it belong to the exponential family? Let's write down the mass
\begin{equation*}
	p_{x}(x;\theta)=\theta^{x}(1-\theta)^{1-x}\indig{0,1}(x).
\end{equation*}
Let's take the exponential of the log of this mass so that we can work in an exponential:
\begin{align*}
	\theta^{x}(1-\theta)^{1-x}\indig{0,1}(x)&=\expg{\log\theta^{x}(1-\theta)^{1-x}}\\
	&=\expg{x\log \theta+(1-x)\log(1-\theta)}\\
	&=\expg{x\log\theta+\log(1-\theta)-x\log(1-\theta)}\\
	&=\expg{x\log\frac{\theta}{1-\theta}+\log(1-\theta)}.
\end{align*}
So here we have
\begin{equation*}
	\begin{array}{>{\displaystyle}l}
		A(x)=x\\
		Q(\theta)=\log\frac{\theta}{1-\theta}\\
		c(x)=0\\
		k(\theta)=-\log(1-\theta).
	\end{array}
\end{equation*}
So we can say that the Bernoulli model is in the exponential family.
\begin{remark}
	Everything extends to parametric spaces of dimension $d>1$.
\end{remark}
These models will appear very often in our course because generalized linear models are based on the exponential family. Now, we want to compute the expected value $\ev{X}$ and the variance $\var(X)$. To do this we need to assume regularity but that's not a problem because most models in the exponential family are, in fact, regular. Compute
\begin{equation*}
	\frac{\dif}{\dif\theta}f_{X}(x;\theta)=[A(x)Q'(\theta)-k'(\theta)]f_{X}(x;\theta).
\end{equation*}
We now take the integral
\begin{align*}
	\int	\frac{\dif}{\dif\theta}f_{X}(x;\theta)\dx&=\int[A(x)Q'(\theta)-k'(\theta)]f_{X}(x;\theta)\dx\\
	0&=\int A(x)Q'(\theta)f_{X}(x;\theta)\dx-\int k'(\theta)f_{X}(x,\theta)\dx\\
	0&=Q'(\theta)\ev{A(X)}-k'(\theta)
\end{align*} 
so
\begin{equation*}
	\ev{A(x)}=\frac{k'(\theta)}{Q'(\theta)}
\end{equation*}
For the variance we need the second derivative
\begin{align*}
	\frac{\dif^{2}}{\dif\theta^{2}}f_{X}(x;\theta)&=\left[A(x)Q''(\theta)-k''(\theta)\right]f_{X}(x,\theta)+\left[A(x)Q'(\theta)-k'(\theta)\right]^{2}f_{X}(x;\theta)
\end{align*}
and integrate both sides (here the second derivative goes to 0 thanks to Leibniz):
\begin{align*}
	0&=\int\left[A(x)Q''(\theta)-k''(\theta)\right]f_{X}(x;\theta)\dx+\int\left[A(x)Q'(\theta)-k'(\theta)\right]^{2}f_{X}(x;\theta)\\
	0&=Q''(\theta)\ev{A(x)}-k''(\theta)+\left(Q'(\theta)\right)^{2}\int\left[A(x)-\frac{k'(\theta)}{Q'(\theta)}\right]^{2}f_{X}(x;\theta)\\
	0&=Q''(\theta)\ev{A(x)}-k''(\theta)+\left(Q'(\theta)\right)^{2}\var(A(x))
\end{align*}
so we get
\begin{equation*}
	\var(A(x))=-\frac{Q''(\theta)k'(\theta)}{\left(Q'(\theta)\right)^{3}}+\frac{k''(\theta)}{\left(Q'(\theta)\right)^{2}}.
\end{equation*}
So if we have $X\sim \mathsf{ExF}_{\theta}$ with $T=A(x)$ then we get
\begin{equation*}
	\ev{T}=\frac{k'(\theta)}{Q'(\theta)}\qquad\var(T)=\frac{k'(\theta)}{\left(Q'(\theta)\right)^{2}}-\ev{T}\frac{Q''(\theta)}{\left(Q'(\theta)\right)^{2}}.
\end{equation*} 
\begin{remark}
	\begin{enumerate}
		\item There exists model for which $A(x)=X$;
		\item if $Q(\theta)=\theta$ then
		\begin{equation*}
			\ev{T}=k'(\theta)\qquad\var(T)=k''(\theta).
		\end{equation*}
	\end{enumerate}
\end{remark}
This brings us to the concept of \emph{exponential family in the natural parametrization} to get a distribution where $Q(\theta)=\theta$.
Introduce
\begin{equation*}
	\eta:=Q(\theta).
\end{equation*}
this gives us
\begin{equation*}
	f_{X}(x,\eta)=\expg{A(x)\eta+c(x)-\widetilde{k}(\eta)}.
\end{equation*}
This is useful because we can express expected value and variance as simple derivatives. Take a random sample $(X_{1},\ldots,X_{n})$ from $X\sim\mathsf{NatExF}(\eta)$. We want to find
\begin{equation*}
	\eta^{\star}=\argmax_{H}\log\mathcal{L}(\eta;\ulx).
\end{equation*}
We get
\begin{align*}
	\mathcal{L}&=\prod_{i=1}^{n}\expg{A(x_{i})\eta+c(x_{i})-\widetilde{k}(\eta)}\\
	&=\expg{\eta\sum_{i=1}^{n}A(x_{i})+\sum_{i=1}^{n}c(x_{i})-n\widetilde{k}(\eta)}.
\end{align*}
Now take the logarithm:
\begin{align*}
	\log\mathcal{L}(\eta;\ulx)&=\eta\sum_{i=1}^{n}A(x_{i})+\sum_{i=1}^{n}c(x_{i})-n\widetilde{k}(\eta)\\
	&\propto\eta\sum_{i=1}^{n}A(x_{i})-n\widetilde{k}(\eta).
\end{align*}
We are only interested in the proportional part, of which we take the log:
\begin{align*}
	\frac{\dif}{\dif\eta}\log\mathcal{L}(\eta;\ulx)&=\sum_{i=1}^{n}A(x_{i})-n\frac{\dif}{\dif\eta}\widetilde{k}(\eta)\\
	&\implies\sum_{i=1}^{n}A(x_{i})-n\frac{\dif}{\dif\eta}\widetilde{k}(\eta)=0\\
	&\implies\frac{\dif}{\dif\eta}\widetilde{k}(\eta)=\frac{1}{n}\sum_{i=1}^{n}A(x_{i}).
\end{align*}
So the information about our parameter ``enters'' just as the empirical mean of $A(x)$, which is one dimension just like $\widetilde{k}(\theta)$. This is absolutely cool, since this is true for \underbar{any} model in this family. We like sums of independent random variables, so this is huge. The negative side is that we are restricting ourselves in the parametric setting and to the MLE approach.
\subsection{Sufficiency}
Again, take $(X_{1},\ldots,X_{n})\in\R^{n}$ from $X\sim f_{X}(x,\theta)$ with $\theta\in\Theta$. We found that just imposing that $X\sim\mathsf{ExF}(\theta)$ means that we are learning $\theta$ by some function $T_{n}=T(X_{1},\ldots,T_{n})\in\R$ (lives in one dimension). The idea of \emph{sufficiency} is measuring how much information I truly lose when using out statistics instead of the function. Defining sufficiency in a rigorous way is very hard because it has to do with continuous probability (which is a nightmare for continuous distribution since we must avoid dividing by 0) so we will just start from an intuition. So take our random sample $(X_{1},\ldots,X_{n})\in\R^{n}$ from $X\sim f_{X}(x,\theta)$. $T_{n}$ is just a function that maps $T:\R^{n}\to\R^{m}$ with $m<n$. 
\begin{definition}
	We say that a statistic $T_{n}$ is \emph{sufficient for the parameter $\theta$} if the conditional distribution of the sample $(X_{1},\ldots,X_{n})$ given $T_{n}$ does not depend on $\theta$.
\end{definition}
So in this definition we have the object $(X_{1},\ldots,X_{n})$ whose distribution \textit{does} depend on $\theta$ and $T_{n}$ whose distribution \textit{does as well} depend on $\theta$. So if both depend on $\theta$ and we want the conditional distribution to be independent of $\theta$ it means that all information of $\theta$ is contained in $T_{n}$ and $\theta$ disappears from the distribution!\par
Let's consider discrete models because they are simpler. Take $(X_{1},\ldots,X_{n})\in\left\{0,1\right\}^{n}$ from $X\distbernoulli{\theta}$ with $\theta\in(0,1)$. Now take $T_{n}\sum_{i=1}^{n}X_{i}\in\left\{0,1,\ldots,n\right\}$; we know the distribution of the sample:
\begin{equation*}
	\pr\left(\bigcap_{i=1}^{n}\left\{X_{i}=x_{i}\right\}\right)=\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{n-\sum_{i=1}^{n}x_{i}}\indi_{\left\{0,1\right\}^{n}}(x_{1},\ldots,x_{n})
\end{equation*}
and the distribution of the statistic:
\begin{equation*}
	\pr\left(T_{n}=t\right)= {n\choose t}\theta^{t}(1-\theta)^{n-t}\indi_{0,1,\ldots}(t).
\end{equation*}
So we are now looking for the conditional probability
\begin{align*}
	\pr\left(\left\{(X_{1},\ldots,X_{n})=(x_{1},\ldots,x_{n})\right\}|T_{n}=t\right)&=\frac{\pr\left(\left\{(X_{1},\ldots,X_{n})=(x_{1},\ldots,x_{n})\right\}\cap\left\{T_{n}=t\right\}\right)}{\pr\left(T_{n}=t\right)}\\
	&=\frac{\cancel{\theta^{t}}\cancel{(1-\theta)^{n-t}}}{{n\choose t}\cancel{\theta^{t}}\cancel{(1-\theta)^{n-t}}}\\
	&=\frac{1}{{n\choose t}}.
\end{align*}
So in the Bernoulli case the sum $\sum_{i=1}^{n}x_{i}$ is sufficient for $\theta$. But this not may be the case for other statistics! Conditional probability is relatively easy to check sufficiency. The statistic $T_{n}=\sum_{i=1}^{n}X_{i}$ is basically a map that maps all points from $\left\{0,1\right\}^{n}$ to the same point in $\N$: we are basically creating a partition of the space in slices that we call $I_{t}$. $I_{t}$ is the collection of strings in the cartesian product wuch that their sum is $t$:
\begin{equation*}
	I_{t}=\left\{(x_{1},\ldots,x_{i})\in\left\{0,1\right\}^{2}:\sum_{i=1}^{n}x_{i}=t\right\}.
\end{equation*}
The original space has $2^{n}$ points so the cardinality of $I_{t}$ is
\begin{equation*}
	\left|I_{t}\right|={n\choose t}
\end{equation*}
and it makes sense because if $t=0$ then the string is all 0 and there is just 1 possible string with all zeroes and same if $t=n$ (only 1 possible string with all ones). This helps us understand why we need to use the conditional probability, which is linked to partition the original event space. If we manage to use $T_{n}$ to create a ``perfect'' partition that contains all the needed strings of 0 and 1 in their ``right'' point then this means that $T_{n}$ is sufficient because it contains the information necessary to do so. This is easy to see with Bernoulli model, but if I had to use other more complex models the complexity would explode... still, the idea of sufficiency is the same.\par Sufficiency is very hard to control because it requires knowing the distribution of the statistic which in general is a pretty hard thing to do.
\begin{theorem}
	\emph{Fisher theorem}. Let $p_{\ulX}(\ulx;\theta)$ be the joint density function or the joint mass function of the distribution of the sample $\ulX=\left(X_{1},\ldots,X_{n}\right)$. Let $q_{T}(t;\theta)$ be the density function of the mass function of the statistic $T(\ulX)$. The statistic $T(\ulX)$ is \emph{sufficient for $\theta\in\Theta$} if for every point in the sample space the ratio 
	\begin{equation*}
		\frac{p_{\ulX}(\ulx;\theta)}{q_{T}(T(\ulx);\theta)}
	\end{equation*}
	is independent of $\theta$.
\end{theorem}
This theorem tells us that what we use to control sufficiency is well defined both for continuous and discrete \rv s alike. The only condition is that the density function doesn't touch zero. However this is still not of practical use because it requires the distribution of the statistic (which, as said before, is hard). What we actually need is a corollary of this result which provides a very simple condition to check for sufficiency.
\begin{corollary}
	\emph{Savage's corollary}. [...] A statistic $T(\ulx)$ is sufficient for the parameter $\theta\in\Theta$ if and only if there exists two non-negative functions $g(T(\ulx);\theta)$ and $h(\ulx)$ such that for all the sample points $\ulx$ and all the parameter points $\theta$ we have
	\begin{equation*}
		p_{\ulX}(\ulx;\theta)=g(T(\ulx),\theta)h(\ulx).
	\end{equation*}
\end{corollary}
So $g(T(\ulx);\theta)$ is a function of the sample but the sample enters inside the function through the statistic; on the other hand, $h(\ulx)$ does \textit{not} depend on $\theta$ in any way. This result si better because all I have to do is to decompose my likelihood function in two parts, one dependent on $\theta$ and the other one not and then ``read'' the statistic $T(\ulx)$ in the part that depends on on $\theta$.
\begin{fancyproof}
	We are going to use Fisher's theorem and we are going to prove the discrete case.
	\begin{enumerate}
		\item[$\implies$] Assume $T(\ulx)$ is sufficient for $\theta$. Then I know a certain ratio will be independent for $\theta$ so select
		\begin{align*}
			g(t;\theta) & =\pr\left(\{T(\ulx)=t\}\right) \\
			h(\ulx) & =\pr\left(\{\ulX=\ulx\}|\{T(\ulX)=T(\ulx)\}\right)
		\end{align*}
		I choose $h(\ulx)$ as the conditional probability because I already know that $T(\ulx)$ will be sufficient and therefore independent. So we have
		\begin{align*}
			p_{\ulX}(\ulx;\theta)&=\pr\left(\{\ulX=\ulx\}\right)\\
			&=\pr\left(\{\ulX=\ulx\}\cap\{T(\ulX)=T(\ulx)\}\right)\\
			&=\pr\left(\{T(\ulX)=T(\ulx)\}\right)\pr\left(\{\ulX=\ulx\}|\{T(\ulX)=T(\ulx)\}\right)\\
			&=g(T(\ulx);\theta)h(\ulx).
		\end{align*}
		\item[$\impliedby$] Assume that the factorization exists. Let $q(t,\theta)$ be the probability mass function of the distribution of $T(\ulx)$. We want to show that the ratio of this factorization is independent from $\theta$. So
		\begin{equation*}
			\frac{p_{\ulx}(\ulx;\theta)}{q(T(\ulx);\theta)}
		\end{equation*}
		must be independent from $\theta$. Define
		\begin{equation*}
			A_{T(\ulx)}=\left\{\uly:T(\uly)=T(\ulx)\right\}.
		\end{equation*}
	Now I use the ration and impose the factorization:
	\begin{align*}
		\frac{p_{\ulX}(\ulx;\theta)}{q(T(\ulx);\theta)}&=\frac{g(T(\ulx);\theta)h(\ulx)}{q(T(\ulx);\theta)}\\
		\text{\footnotesize using marginal probability }\quad&=\frac{g(T(\ulx);\theta)h(\ulx)}{\sum_{\uly\in A_{T(\ulx)}}g(T(\uly);\theta)h(\uly)}\\
		&=\frac{\cancel{g(T(\ulx);\theta)}h(\ulx)}{\cancel{g(T(\ulx);\theta)}\sum_{\uly\in A_{T(\ulx)}}h(\uly)}
	\end{align*}
	and this is independent of $\theta$. By Fisher theorem, then $T(\ulx)$ is sufficient for $\theta$.
	\end{enumerate}
\end{fancyproof}
Take $(X_{1},\ldots,X_{n})$ from $X\distnorm{\mu,\sigma^{2}}$ with $\mu$ unknown and $\sigma^{2}$ known. If we want to use Savage's theorem or Fisher's theorem we first need the density function:
\begin{equation*}
	f_{\ulX}(\ulx;\theta)=\left(2\pi\sigma^{2}\right)^{\frac{n}{2}}\expg{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}}.
\end{equation*}
Then choose
\begin{equation*}
	T_{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\distnorm{\mu,\frac{\sigma^{2}}{n}}
\end{equation*}
so that 
\begin{align*}
	p_{T_{n}}(t;\theta)&=\left(2\pi\frac{\sigma^{2}}{n}\right)^{-\unmezz}\expg{\frac{-n}{2\sigma^{2}(t-\mu)^{2}}}\\
	&=\frac{\left(2\pi\sigma^{2}\right)^{-\frac{n}{2}}\expg{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-t+t-\mu\right)^{2}}}{\left(2\pi\frac{\sigma^{2}}{n}\right)^{-\unmezz}\expg{-\frac{n}{2\sigma^{2}}(t-\mu)^{2}}}.
\end{align*}
Take 
\begin{equation*}
	t=\frac{1}{n}\sum_{i=1}^{n}x_{i}
\end{equation*}
so that our density becomes
\begin{equation*}
	\frac{\left(2\pi\sigma^{2}\right)^{-\frac{n}{2}}\expg{-\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}(x_{i}-t)^{2}+(t-\mu)^{2}n\right)}}{\left(2\pi\frac{\sigma^{2}}{n}\right)^{-\unmezz}\expg{-\frac{n}{2\sigma^{2}}(t-\mu)^{2}}}
\end{equation*}
And we impose
\begin{equation*}
	2\sum_{i=1}^{n}(x_{i}-t)(t-\mu)=0.
\end{equation*}
So we have 
\begin{align*}
	\frac{\left(2\pi\sigma^{2}\right)^{-\frac{n}{2}}\expg{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-t)^{2}}\cancel{\expg{-\frac{n}{2\sigma^{2}}(t-\mu)^{2}}}}{\left(2\pi\frac{\sigma^{2}}{n}\right)^{-\unmezz}\cancel{\expg{-\frac{n}{2\sigma^{2}}(t-\mu)^{2}}}}
\end{align*}
so it is independent of $\mu\implies\frac{1}{n}\sum_{i=1}^{n}x_{i}$ is sufficient for $\mu$. We can get the same result with Savage: write
\begin{align*}
	f_{\ulX}(\ulx;\theta)&=\left(2\pi\sigma^{2}\right)^{-\frac{n}{2}}\expg{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}}\\
	&=\left(2\pi\sigma^{2}\right)^{-\frac{n}{2}}\expg{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}x_{i}^{2}+\frac{1}{\sigma^{2}}\mu\sum_{i=1}^{n}x_{i}-\frac{n\mu^{2}}{2\sigma^{2}}}\\
	&=\ubracketthin{\left(2\pi\sigma^{2}\right)^{-\frac{n}{2}}\expg{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}x_{i}^{2}}}_{\text{no }\mu}\cdot\ubracketthin{\expg{\frac{1}{\sigma^{2}}\mu\mathcolor{Purple2}{\frac{1}{n}\sum_{i=1}^{n}x_{i}}-\frac{n\mu^{2}}{2\sigma^{2}}}}_{\text{with }\mu}.
\end{align*}
So $\frac{1}{n}\sum_{i=1}^{n}x_{i}$ is sufficient for $\mu$. We added $\frac{1}{n}$ for convention, but also just $\sum_{i=1}^{n}x_{i}$ would be a sufficient statistic. \par
Another example is with the exponential family: take $(X_{1},\ldots,X_{n})$ from $X\distexpf{\eta}$. We have the following likelihood function:
\begin{align*}
	\mathcal{L}(\eta;\ulx)&=\prod_{i=1}^{n}\expg{\eta A(x_{i})+x(x_{i})-\widetilde{k}(\eta)}\\
	&=\expg{\eta\sum_{i=1}^{n}A(x_{i})+\sum_{i=1}^{n}c(x_{i})-n\widetilde{k}(\eta)}\\
	&=\expg{\eta\mathcolor{Purple2}{\sum_{i=1}^{n}A(x_{i})}-n\widetilde{k}(\eta)}\cdot\expg{\sum_{i=1}^{n}c(x_{i})}.
\end{align*}
So the sum $\sum_{i=1}^{n}A(x_{i})$ is sufficient for $\eta$.\par
Take $(X_{1},\ldots,X_{n})$ from $X\sim\mathsf{Beta}(a,1)$. We have
\begin{equation*}
	f_{X}(x,a)\propto x^{a-1}\indig{0,1}(x).
\end{equation*}
This is \textit{not} in the exponential family and it is a generalization of the uniform distribution (if $a=1$ we get the uniform distribution).
\begin{remark}
	\begin{itemize}
		\item  A non sufficient statistic cannot be used. A non sufficient statistic gives me no control of how much information I am losing when reducing dimensionality. 
		\item The random sample $(X_{1},\ldots,X_{n})$ is always a sufficient statistic for the parameter.
		\item Any bijection of a sufficient statistic is sufficient (prove it as an exercise). This is why we could put $\frac{1}{n}$ in the Gaussian case: if $\sum_{i=1}^{n}x_{i}$ is sufficient, then also $\frac{1}{n}\sum_{i=1}^{n}$ is sufficient.
	\end{itemize}
\end{remark}
In general, any maximum likelihood estimator will be a function of a sufficient statistic (not only the one belonging to the exponential family: think about the Beta model).
\subsection{Properties of estimators (and statistics)}
We will start with finite properties. Take a random sample $(X_{1},\ldots,X_{n})$ from $X\sim F_{\theta}$ with $\theta\in\Theta$. Let $T_{n}=T(X_{1},\ldots,X_{n})$ be an estimator for $\theta$. How can we know whether this $T_{n}$ does a good job or not? Remember that $T_{n}$ is a \rv{} so it seems natural to ask whether the mean is centered in $\theta$.
\begin{definition}
	$T_{n}$ is \emph{unbiased} (or \emph{correct}) for $\theta$ if 
	\begin{equation*}
		\ev{T_{n}}=\theta.
	\end{equation*}
	We can define the \emph{bias} of $T_{n}$ as
	\begin{equation*}
		b_{n}=\ev{T_{n}}-\theta.
	\end{equation*}
\end{definition}
Of course the expectation is considered the ``barycenter'' of the distribution and it can be interpreted as an index of position of the distribution. The median, on the other hand, it is a order statistic and as such it splits the distribution in two parts. The problem is that the median is a pain in the ass to compute (we know that order statistic are not easy to get). Another index we may compute is the variance of the estimator
\begin{equation*}
	\var(T_{n}) 
\end{equation*}
as a measure of the dispersion of the distribution, or \emph{efficiency}, of $T_{n}$ around $\theta$. This could be a good measure of the performance of $\theta$ but only if the estimator is correct in the first place:
\begin{equation*}
	\var(T_{n})=\ev{(T_{n}-\ev{T_{n}})^{2}}\qquad\text{ it's okay if $\ev{T_{n}}=\theta$}.
\end{equation*}
If the estimator is not correct we need another measure of dispersion. We want to control the distance between $T_{n}$ and $\theta$ but since $T_{n}$ is a \rv{} then we need to control this distance in a ``probabilistic'' way, using a concentration inequality. Using Chebyshev's inequality we get
\begin{equation*}
	\pr\left(|T_{n}-\theta|<k\right)>1-\frac{\ev{(T_{n}-\theta)^{2}}}{k^{2}}\qquad\text{$k$ fixed}.
\end{equation*}
Since we want the probability of the left hand side to be as large as possible we want $\ev{(T_{n}-\theta)^{2}}$ to be as small as possible (since $k$ is fixed). 
\begin{definition}
	The quantity
	\begin{equation*}
		\mathsf{MSE}(T_{n})=\ev{(T_{n}-\theta)^{2}}
	\end{equation*}
	is called the \emph{mean square error}. 
\end{definition}
If $\ev{T_{n}}\neq\theta$ then we can use the MSE as a meausre of dispersion of the distribution of $T_{n}$. This is why the mean square error comes from Chebyshev inequality!! This is pretty cool, I'll have to admit. We can reduce the MSE:
\begin{align*}
	\mathsf{MSE}(T_{n})&=\ev{(T_{n}-\ev{T_{n}}+\ev{T_{n}}-\theta)^{2}}\\
	&=\ev{(T_{n}-\ev{T_{n}})^{2}}+\ev{(\ev{T_{n}}-\theta)^{2}}+2\ev{(T_{n}-\ev{T_{n}})(\ev{T_{n}}-\theta)}.
\end{align*}
If we expand the double product we get
\begin{align*}
	\ev{(T_{n}-\ev{T_{n}})(\ev{T_{n}}-\theta)}&=\ev{T_{n}\ev{T_{n}}-T_{n}\theta-\ev{T_{n}}\ev{T_{n}}+\ev{T_{n}}\theta}\\
	&=\cancel{(\ev{T_{n}})^{2}}\cancel{-\theta\ev{T_{n}}}\cancel{-(\ev{T_{n}})^{2}}\cancel{+\theta\ev{T_{n}}}\\
	&=0
\end{align*}
So we get that 
\begin{equation*}
	\mathsf{MSE}(T_{n})=\var(T_{n})+\left(b_{n}(\theta)\right)^{2}.
\end{equation*}
We can now derive an idea of relative efficiency. Let $T_{1n}$ and $T_{2n}$ be two estimators for $\theta$. If
\begin{equation*}
	\MSE{T_{1n}}<\MSE{T_{2n}}
\end{equation*}
then I prefer $T_{1n}$.\par
Now we could ask ourselves whether there exists a threshold for estimator efficiency in relation to which we can compute how far we have strayed from it.
\begin{theorem}
	\emph{Cramer-Rao (lower) bound}. We assume regularity. Let $\ulX=(X_{1},\ldots,X_{n})$ be a random sample from $X\sim F_{\theta}$ with $\theta\in\Theta$. If $T_{n}$ is an unbiased estimator for $\theta$ then
	\begin{equation*}
		\var(T_{n})\geq\frac{1}{I_{n}(\theta)}
	\end{equation*}
	where $I_{n}(\theta)$ is the Fisher information of the sample.
\end{theorem}
So we know that the variance \textit{cannot} be lower than $(I_{n}(\theta))^{-1}$ so it all becomes a matter of checking whether our estimator attains this lower bound.
\begin{fancyproof}
	Assume
	\begin{equation*}
		\ev{T_n}=\theta+b_{n}(\theta).
	\end{equation*}
	The derivative is
	\begin{equation*}
		\frac{\dif}{\dif\theta}\ev{T_{n}}=1+b'_{n}(\theta).
	\end{equation*}
	Remember the score function
	\begin{equation*}
		V'_{n}(\theta)=\frac{\dif}{\dif\theta}\log\mathcal{L}(\theta;\ulx)\qquad\begin{larray}
			\ev{V'_{n}}=0\\
			\var(V'_{n})=\ev{(V'_{n})^{2}}=-\ev{V''_{n}}.
		\end{larray}
	\end{equation*}
	This means that
	\begin{align*}
		\cov(T_{n},V'_{n})&=\ev{T_{n}V'_{n}}\\
		&=\int_{\R^{n}}T_{n}\left(\frac{\dif}{\dif\theta}\log f_{\ulX}(\ulx;\theta)\right)f_{\ulX}(x;\theta)\dif\ulx\\
		&=\int_{\R^{n}}T_{n}\frac{f'_{\ulX}(\ulx;\theta)}{\cancel{f_{\ulX}(\ulx;\theta)}}\cancel{f_{\ulX}(\ulx;\theta)}\dif\ulx\\
		&=\int_{\R^{n}}T_{n}\frac{\dif}{\dif\theta}f_{\ulX}(\ulx;\theta)\dif\ulx\\
		&=\frac{\dif}{\dif\theta}\int_{\R^{2}}T_{n}f_{\ulX}(\ulx;\theta)\dif\ulx\\
		&=\deriv{\theta}\ev{T_{n}}=1+b'_{n}(\theta).
	\end{align*}
	So we just apply Cauchy-Schwartz inequality:
	\begin{align*}
		\var(T_{n})&\geq\frac{\left[\cov(T_{n},V'_{n})\right]^{2}}{\var(V'_{n})}\\
		&=\frac{\left[1+b'_{n}(theta)\right]^{2}}{I_{n}(\theta)}.
	\end{align*}
	If there is no bias, then
	\begin{equation*}
		\var(T_{n})\geq\frac{1}{I_{n}(\theta)}.
	\end{equation*}
\end{fancyproof}
\begin{remark}
	If we don't have an unbiased estimator then
		\begin{align*}
		\var(T_{n})&\geq\frac{\left[1+b'_{n}(theta)\right]^{2}}{I_{n}(\theta)}
	\end{align*}
	and therefore we can apply
	\begin{align*}
		\var(T_{n})+b^{2}_{n}(\theta)\geq\frac{\left[1+b'_{n}(\theta)\right]^{2}}{I_{n}(\theta)}+b^{2}_{n}(\theta)
	\end{align*}
	so we can use the MSE to control variance:
	\begin{equation*}
		\MSE(T_{n})\geq\frac{\left[1+b'_{n}(\theta)\right]^{2}}{I_{n}(\theta)}+b^{2}_{n}(\theta).
	\end{equation*}
\end{remark}
\begin{definition}
	\emph{Regular problem}. We say that a correct estimator $T_{n}$ for $\theta$ is \emph{efficient} if
	\begin{equation*}
		\var(T_{n})\geq\frac{1}{I_{n}(\theta)}.
	\end{equation*}
\end{definition}
There is an analogous result for non-regular models.
\begin{proposition}
	Let $(X_{1},\ldots,X_{n})$ be from a regular model $F_{\theta}$ with $\theta\in\Theta$. Consider a correct estimator $T_{n}$ for $\theta$ such that 
	\begin{equation*}
		\var(T_{n})=\frac{1}{I_{n}(\theta)}.
	\end{equation*}
	Then $T_{n}$ is unique.
\end{proposition}
Of course this seems too beautiful to be true and indeed we are assuming a lot (regularity \textit{and} correctness). If I remove correctness then the uniqueness is not granted anymore. \begin{fancyproof}
	Take $T_{1n}$ and $T_{2n}$ such that
\begin{equation*}
	\ev{T_{1n}}=\ev{T_{2n}}=\theta
\end{equation*}
and
\begin{equation*}
	\var(T_{1n})=\var(T_{2n})=\frac{1}{I_{n}(\theta)}:=V.
\end{equation*}
Define 
\begin{equation*}
	T_{n}=\frac{T_{1n}+T_{2n}}{2}
\end{equation*}
and this is a new unbiased estimator because
\begin{equation*}
	\ev{T_{n}}=\theta
\end{equation*}
by virtue of linearity of expectation. Now compute
\begin{align*}
	\var(T_{n})&=\var\left(\frac{T_{1n}+T_{2n}}{2}\right)\\
	&=\frac{1}{4}\left[\var(T_{1n})+\var(T_{2n})+2\cov(T_{1n},T_{2n})\right]
\end{align*}
but
\begin{equation*}
	\cov(T_{1n},T_{2n})=\corr(T_{1n},T_{2n})\left[\sqrt{\var(T_{1n})}\sqrt{\var(T_{2n})}\right]
\end{equation*}
so
\begin{align*}
		\var(T_{n})&=\frac{1}{4}\left(2V+2\corr(T_{1n},T_{2n})V\right)\\
		&=\unmezz V\left[1+\corr(T_{1n},T_{2n})\right]
\end{align*}
but we know that if the correlation is smaller than 1 and this would give us
\begin{equation*}
	\var(T_{n})<V
\end{equation*}
but this is impossible; so the correlation must be 1 and $T_{1n}$ and $T_{2n}$ are perfectly correlated which means that they are the same up to a linear transformation and we can write
\begin{equation*}
	T_{2n}=a+b(T_{1n})
\end{equation*}
but we also know
\begin{equation*}
	\ev{T_{1n}}=\ev{T_{2n}}=\theta
\end{equation*}
so
\begin{equation*}
	\begin{larray}
		\ev{T_{2n}}=a+b\ev{T_{1n}}\\
		\Downarrow\\
		\theta=a+b\theta\\
		\Downarrow\\
		\begin{carray}
			a=0\\
			b=1
		\end{carray}\implies T_{1n}=T_{2n}.
	\end{larray}
\end{equation*}
\end{fancyproof}
\begin{proposition}
	Let $(X_{1},\ldots,X_{n})$ be from a regular model $F_{\theta}$. $T_{n}$ is a correct and efficient estimator for $\theta$ if and only if
	\begin{equation*}
		V'_{n}(\theta)=\deriv{\theta}\log\like(\theta;\ulx)=I_{n}(T_{n}-\theta)\tag*{\faStarOfLife}\label{starlife}
	\end{equation*}
\end{proposition}
So \ref{starlife} implies that $T_{n}$ is correct and efficient:
\begin{equation*}
	\begin{carray}
		V'_{n}(\theta)=I_{n}(T_{n}-\theta)\\
		(T_{n}-\theta)=\frac{V'_{n}(\theta)}{I_{n}(\theta)}\\
		\Downarrow\\
		T_{n}=\frac{V'_{n}(\theta)}{I_{n}(\theta)}+\theta.
	\end{carray}
\end{equation*}
This is a very, \textit{very} important identity because it looks like we can start a recursion with it: I choose a $\theta_0$, compute the gradient $V'_{n}(\theta_0)$ and get a new $\theta_{1}$ which acts as our new starting point to compute the gradient and so on.
\begin{equation*}
	\theta^{(i)}=\frac{V'_{n}(\theta^{(i-1)})}{I_{n}(\theta^{(i-1)})}+\theta^{(i-1)}
\end{equation*}
This is called the \emph{score version of the Newton algorithm} and it's quite fast especially if we use stochastic gradient descent and similar things. So even if I am not able to compute $I_{n}$ analytically (which may very well be the case) I can always use this equation to compute an \ul{correct and efficient} estimator. That's not bad at all! We will use this in generalized linear models where $\like$ cannot be computed analytically. 
\begin{fancyproof}
	\begin{enumerate}
		\item[$\implies$] We know that
		\begin{equation*}
			\ev{T_{n}}=\frac{\ev{V'_{n}(\theta)}}{I_{n}(\theta)}+\theta=\theta
		\end{equation*} 
		and we know that
		\begin{align*}
			\var(T_{n})
			&=\var\left(\frac{V'_{n}(\theta)}{I_{n}(\theta)}+\theta\right)\\
			&=\frac{1}{(I_{n}(\theta))^{2}}\var(V'_{n}(\theta))\\
			&=\frac{I_{n}(\theta)}{(I_{n}(\theta))^{2}}=\frac{1}{I_{n}(\theta)}.
		\end{align*}
	\item[$\impliedby$] We know that $T_{n}$ is correct and efficient so 
	\begin{equation*}
		V'_{n}(\theta)=I_{n}(\theta)(T_{n}-\theta).
	\end{equation*}
	We know that 
	\begin{equation*}
		\cov(T_{n},V'_{n})\leq\sqrt{\var(T_{n})\var(V'_{n})}\tag*{\faMoneyBill[regular]}\label{money}
	\end{equation*}
	but this becomes an equality and this implies that $V'_{n}(\theta)$ is perfectly correlated with $T_{n}$:
	\begin{equation*}
		V'_{n}(\theta)=a+bT_{n}.
	\end{equation*}
	Now we must find $a$ and $b$. Take the expected value:
	\begin{align*}
		\ev{V'_{n}(\theta)}&=a+b\ev{T_{n}}\\
		0&=a+b\theta\implies a=-b\theta.
	\end{align*}
	This means that 
	\begin{align*}
		V'_{n}(\theta)&=-b\theta+bT_{n}\\
		&=b(T_{n}-\theta).
	\end{align*}
	Multiply both sides by $V'_{n}(\theta)$:
	\begin{align*}
		\left(V'_{n}(\theta)\right)^{2}&=bT_{n}V'_{n}(\theta)-b\theta V'_{n}(\theta)\\
		\ev{\left(V'_{n}(\theta)\right)^{2}}&=b\ubracketthin{\ev{T_{n}V'n(\theta)}}_{1}-b\theta\ubracketthin{\ev{V'_{n}(\theta)}}_{0}\\
		&=b\implies I_{n}(\theta)=b
	\end{align*}
	so
	\begin{equation*}
		V'_{n}(\theta)=I_{n}(\theta)\left[T_{n}-\theta\right].
	\end{equation*}
\end{enumerate}
\end{fancyproof}
We will now link this result with the exponential family and we will discover that as long as we work with this family we will always find an efficient estimator. We want to adapt the proof we did before by removing the assumption of correctness; we know that if there exists an efficient $T_{n}$ then it is perfectly correlated with the score function:
\begin{equation*}
	V'_{n}(\theta)=c_{0}+c_{1}T_{n}
\end{equation*}
and if $T_{n}$ is not correct we have
\begin{equation*}
	\ev{T_{n}}=\theta+b(\theta).
\end{equation*}
We take the expected value of both sides in the first equation:
\begin{align*}
	\ev{V'_{n}(\theta)}&=c_{0}+c_{1}\ev{T_{n}}\\
	0&=c_{0}+c_{1}\left(\theta+b(\theta)\right)\\
	\implies&c_{0}=-c_{1}(\theta+b(\theta))\\
	\implies&V'_{n}(\theta)=-c_{1}\theta-c_{1}b(\theta)+c_{1}T_{n}\\
	\implies&\left(V'_{n}(\theta)\right)^{2}=-\ubracketthin{c_{1}\theta V'_{n}(\theta)}_{\ev{\cdot}=0}-\ubracketthin{c_{1}b(\theta)V'_{n}(\theta)}_{\ev{\cdot}=0}+c_{1}T_{n}V'_{n}(\theta)\\
	\implies&\ev{\left(V'_{n}(\theta)\right)^{2}}=c_{1}\ev{T_{n}V'_{n}(\theta)}\\
	\implies&\ev{\left(V'_{n}(\theta)\right)^{2}}=c_{1}\left(1+b'(\theta)\right)=:a(\theta)\\
	\implies&c_{1}=\frac{a(\theta)}{1+b'(\theta)}=:\left(c(\theta)\right)^{-1}.
\end{align*}
Now I replace $c_{1}$:
\begin{align*}
	V'_{n}(\theta)=-\theta(c(\theta))^{-1}-b(\theta)(c(\theta))^{-1}+T_{n}(c(\theta))^{-1}.
\end{align*}
$c(\theta)$ is at most a function of $\theta$:
\begin{align*}
	\deriv{\theta}\log\like(\theta;\ulx)&=-\theta(c(\theta))^{-1}-b(\theta)(c(\theta))^{-1}+T_{n}(c(\theta))^{-1}.
\end{align*}
Integrate both sides with respect to $\theta$:
\begin{align*}
	\log\like(\theta;\ulx)&=\int_{\Theta}\theta(c(\theta))^{-1}\dif\theta-\int_{\Theta}b(\theta)(c(\theta))^{-1}\dif\theta+T_{n}\int_{\Theta}(c(\theta))^{-1}+c.
\end{align*}
Take the exponential on both sides:
\begin{align*}
	\like(\theta;\ulx)&=\expg{-\int_{\Theta}(\theta+b(\theta))(c(\theta))^{-1}\dif\theta+T_{n}\int_{\Theta}(c(\theta))^{-1}+c}
\end{align*}
So we arrived to an expression for the likelihood that corresponds to the one of the exponential family just starting from the efficient estimator without knowing anything else! In this case
\begin{equation*}
	\begin{larray}
		k(\theta)=\int_{\Theta}(\theta+b(\theta))(c(\theta))^{-1}\dif\theta\\
		A(\ulx)=T_{n}\\
		Q(\theta)=\int_{\Theta}(c(\theta))^{-1}\\
		c(x)=c.
	\end{larray}
\end{equation*}
So we can reverse this derivation and say that if $X\distexpf{\theta}$ then there exist an efficient estimator which is function of the sufficient statistic.
\begin{theorem}
	\emph{Rao-Blackwell theorem}. Let $(X_{1},\ldots,X_{n})$ be a random sample from $X\sim F_{\theta}$ with $\theta\in\Theta$ (regular or not). Take a sufficient estimator $T_{1n}$ for $\theta$ and any unbiased estimator $T_{2n}$ for $\theta$. Define
	\begin{equation*}
		T_{n}=\ev{T_{2n}|T_{1n}}.
	\end{equation*}
	Then:
	\begin{itemize}
		\item $T_{n}$ is a function of $T_{1n}$;
		\item $\ev{T_{n}}=\theta$;
		\item $\var(T_{n})\leq\var(T_{2n})$.
	\end{itemize}
\end{theorem}
So we can use sufficient statistics to build more efficient (or equivalent) estimator of any other estimator!
\begin{fancyproof}
	We have
	\begin{equation*}
		T_{n}=\ev{T_{2n}|T_{1n}}.
	\end{equation*}
	We use tower property of expectation:
	\begin{align*}
		\ev{T_{n}}&=\ev{\ev{T_{2n}|T_{1n}}}\\
		&=\ev{T_{2n}}\\
		&=\theta.
	\end{align*}
	To show the increase in efficiency we need to compute the variance using the variance identity:
	\begin{align*}
		\var(T_{2n})&=\var(\ev{T_{2n}|T_{1n}})+\ev{\var(T_{2n}|T_{1n})}\\
		&=\var(T_{n})+\ubracketthin{\ev{\var(T_{2n}|T_{1n})}}_{\claptext{always positive}}\\
		&\geq\var(T_{n})
	\end{align*}
	so we get that
	\begin{equation*}
		\var(T_{n})\leq\var(T_{2n}).
	\end{equation*}
	We still need to prove that $T_{n}$ is an estimator which means showing that $T_{n}$ is a function of the data but \textit{not} of $\theta$ but this is trivially true because $T_{n}=\ev{T_{2n}|T_{1n}}$ and since $T_{1n}$ is sufficient it ``removes'' from $T_{n}$ all the dependence from $\theta$.
\end{fancyproof}
\subsection{Asymptotic properties of estimators}
\begin{definition}
	\begin{itemize}
	\item 	We say that $T_{n}$ is \emph{asymptotically unbiased} for $\theta$ if
	\begin{equation*}
		\lim_{n\to\infty}\ev{T_{n}}=\theta.
	\end{equation*}
	\item We say that $T_{n}$ is \emph{consistent in mean square} if
	\begin{equation*}
		\lim_{n\to\infty} \MSE{T_{n}}=\lim_{n\to\infty}\ev{\left(T_{n}-\theta\right)^{2}}=0.
	\end{equation*}
	\begin{remark}
		We can always decompose the MSE:
		\begin{equation*}
			\MSE{T_{n}}=\var(T_{n})+b'_{n}(\theta)
		\end{equation*}
		so consistency in mean-square implies asymptotic correctness.
	\end{remark}
	\item We say that $T_{n}$ is \emph{consistent in probability} if for $\every\varepsilon>0$
	\begin{equation*}
		\lim_{n\to\infty}\pr(|T_{n}-\theta|<\varepsilon)=1.
	\end{equation*}
	We then write 
	\begin{equation*}
		T_{n}\convpr\theta.
	\end{equation*}
	\begin{remark}
		Consistency in mean square implies consistency in probability (check this using Chebyshev).
	\end{remark}
	\item If $T_{n}$ is unbiased for $\theta$ then we say that $T_{n}$ is \emph{asymptotically efficient} if 
	\begin{equation*}
		\lim_{n\to\infty}\var(T_{n})=\frac{1}{I_{n}(\theta)}.
	\end{equation*}
	\item We way that $T_{n}$ is \emph{asymptotically Gaussian} if\\
	\begin{equation*}
		\lim_{n\to\infty}\pr\left(\frac{T_{n}-\ev{T_{n}}}{\sqrt{\var(T_{n})}}\leq t\right)=\Phi(t)
	\end{equation*}
	and we write
	\begin{equation*}
		\frac{T_{n}-\ev{T_{n}}}{\sqrt{\var(T_{n})}}\convw Z\distnorm{0,1}.
	\end{equation*}
	\end{itemize}
\end{definition}
To work with this results we need the \emph{weak law of large numbers}.
\begin{theorem}
	\emph{Weak Law of Large Numbers (WLLN)}. Assume $f(\cdot|\theta)$ to be some function defined for $\theta\in\Theta$ and continuous in $\theta$. Then for any fixed $\theta$ let
	\begin{equation*}
		{\left(f(X_{i};\theta)\right)}_{i\geq1}
	\end{equation*}
	be a sequence of i.i.d. \rv s. Then for $n\to\infty$
	\begin{equation*}
		\frac{1}{n}\sum_{i=1}^{n}f(X_{i};\theta)\convpr\ev{f(X;\theta)}.
	\end{equation*}
\end{theorem}
Note that this is a point-wise convergence, $\theta$-by-$\theta$. To prove convergence we need to ``uniform'' this result over $\theta$ by taking the supremum:
\begin{equation*}
	\sup_{\Theta}\left|\frac{1}{n}\sum_{i=1}^{n}f(X_{i};
	\theta)-\ev{f(X;\theta)}\right|\convas0
\end{equation*}
and this is not anymore a point-wise result but it is a uniform result. This is called \emph{uniform weak law of large numbers} and it is a very different result than the WLLN. \par
Take $(X_{1},\ldots,X_{n})$ from $X\sim F_{\mu,\sigma^{2}}$ such that
\begin{equation*}
	\begin{carray}
		\ev{X}=\mu\\
		\var(X)=\sigma^{2}
	\end{carray}
\end{equation*}
(so a Gaussian case). We already proved that $\frac{1}{n}\sum_{i=1}^{n}$ is biased for $\mu$ for $\every n\geq 1$. Now take the sample variance $\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\Xbar_{n})^{2}$ and we want to check whether it is biased or unbiased.
\begin{align*}
	\ev{\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\Xbar_{n})^{2}}&=\ev{\frac{1}{n}\sum_{i=1}^{n}X_{i}^{2}-\frac{2}{n}\Xbar_{n}\sum_{i=1}^{n}x_{i}+\Xbar^{2}_{n}}\\
 	&=\ubracketthin{\frac{1}{n}\sum_{i=1}^{n}\ev{X^{2}_{i}}}_{\mathrm{I}}-\ubracketthin{\ev{\Xbar^{2}_{i}}}_{\mathrm{II}}.
\end{align*}
Now check the two quantities separately:
\begin{align*}
	\mathrm{I}&=\frac{1}{n}\sum_{i=1}^{n}(\sigma^{2}+\mu^{2})=\sigma^{2}+\mu^{2}\\
	\mathrm{II}&=\begin{rcases}
		\ev{\Xbar_{n}}=\mu\\
		\var\left(\Xbar_{n}\right)=\frac{1}{n}\sigma^{2}
	\end{rcases}\ev{\Xbar^{2}}=\frac{1}{n}\sigma^{2}+\mu^{2}.
\end{align*}
So we can write
\begin{align*}
	\ev{\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\Xbar_{n}\right)^{2}}&=\mathrm{I}-\mathrm{II}\\
	&=\sigma^{2}\left(1-\frac{1}{n}\right)\\
	&=\sigma^{2}\frac{n}{n-1}
\end{align*}
and we know how to correct the variance (by $\frac{n}{n-1}$). \par
Now take $(X_{1},\ldots,X_{n})$ from $X\distbernoulli{\theta}$. We set
\begin{align*}
	\theta^{\star}=&\argmax_{(0,1)}\like(\theta;\ulx)\\
	&\vdots\\
	=&\frac{1}{n}\sum_{i=1}^{n}x_{i}=:\widehat{\theta}_{n}.
\end{align*}
If we compute mean and variance we get
\begin{equation*}
	\begin{larray}
		\ev{\widehat{\theta}_{n}}=\theta\\
		\var(\widehat{\theta}_{n})=\frac{\theta(1-\theta)}{n}\qquad\text{(Cramer-Rao bound!)}
	\end{larray}
\end{equation*}
So we have convergence in probability
\begin{equation*}
	\widehat{\theta}_{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\convpr\ev{X}=\theta
\end{equation*}
so $\widehat{\theta}_{n}$ is (weakly) consistent for $\theta$.
\subsection{Bayes estimation}
We can further modify this problem. Suppose we have the random sample $(X_{1},\ldots,X_{n})$ from $X\distbernoulli{\theta}$. We now want to use for the first time a Bayesian approach rather than a frequentist approach. We know that $\theta\in(0,1)$ and we suppose (guessing) that $\theta\sim\mathsf{Beta}(a,b)$ for $a,b>0$ and therefore what we actually mean is that
\begin{equation*}
	X_{1},\ldots,X_{n}\mathcolor{Purple3}{|\theta}\distbernoulli{\theta}\qquad\text{with }\theta\sim\mathsf{Beta}(a,b).
\end{equation*} We say that $\theta\sim\mathsf{Beta}$ because the Beta model is a reasonable ``a priori'' model. Now we compute the conditional distribution of $\theta|(X_{1},\ldots,X_{n})$ using Bayes' theorem:
\begin{align*}
	f_{\theta|X_{1},\ldots,X_{n}}(\cdot)&=\frac{\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{n-\sum_{i=1}^{1}x_{i}}\cancel{\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}}\theta^{a-1}(1-\theta)^{b-1}}{\int_{0}^{1}\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{n-\sum_{i=1}^{n}x_{i}}\cancel{\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}}\theta^{a-1}(1-\theta)^{b-1}\dif\theta}\\
	&=\frac{\theta^{\sum_{i=1}^{n}x_{i}+a-1}(1-\theta)^{n-\sum_{i=1}^{n}x_{i}+b-1}}{\int_{0}^{1}\theta^{\sum_{i=1}^{n}x_{i+a=1}}(1-\theta)^{\sum_{i=1}^{n}x_{i}+b-1}\dif\theta}\\
	&=\theta^{\sum_{i=1}^{n}x_{i}+a-1}(1-\theta)^{n-\sum_{i=1}^{n}x_{i}+b-1}\frac{\Gamma(n+a+b)}{\Gamma\left(\sum_{i=1}^{n}x_{i}+a\right)\Gamma\left(n-\sum_{i=1}^{n}x_{i}+b\right)}.
\end{align*}
So the conditional distribution of $\theta$ given $(X_{1},\ldots,X_{n})$ is a Beta distribution with parameters updated with data. But how can I use this object? In the frequentist approach we get a straight up value for $\theta$, while here we get a whole distribution. To choose an estimator for $\theta$ we can choose the expected value of the posterior distribution. We choose the expected value because in decision theory the expected value is what minimizes the square loss
\begin{equation*}
	L(\widehat{\theta})=\int_{\Theta}(\widehat{\theta}-\theta)^{2}p\ubracketthin{(\dif\theta|\ulx)}_{\claptext{posterior}}\dif\theta.
\end{equation*}
So we take the expected value of the posterior:
\begin{align*}
	\ev{\theta|X_{1},\ldots,X_{n}}&=\frac{a+\sum_{i=1}^{n}x_{n}}{a+b+n}.
\end{align*}
So we got our \emph{Bayes estimator}. Our MLE estimator is
\begin{equation*}
	\widehat{\theta}_{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}.
\end{equation*}
MLE is consistent because $\frac{1}{n}\sum_{i=1}^{n}\convpr\theta$ and this result is given from the Law of large which assumes that our \rv s are i.i.d., while for the Bayes estimator we know that they are i.i.d. in the \textit{conditional distribution} but not marginally: if we integrate out the conditioning \rv{} we find that they are actually dependent. So we cannot use the law of large numbers.\par
Take $(X_{1},\ldots,X_{n})$ from $X\sim F_{\theta}$ with
\begin{equation*}
	f_{X}(x;\theta)=2\theta x\expg{-\theta x^{2}}\indi_{\R^{+}}(x)\qquad\theta>0.
\end{equation*}
This is a transformation of the gamma model. We use maximum likelihood to learn $\theta$:
\begin{align*}
	\like(\theta;\ulx)&=\prod_{i=1}^{n}2\theta x_{i}\expg{-\theta x^{2}_{i}}\\
	&=\left(2\theta\right)^{n}\left(\prod_{i=1}^{n}x_{i}\right)\expg{-\theta\sum_{i=1}^{n}x_{i}}.
\end{align*}
We can use the result from Savage's theorem to deduce that the sufficient statistic for estimating $\theta$ is
\begin{equation*}
	\sum_{i=1}^{n}x^{2}.
\end{equation*}
We would expect that maximizing the likelihood gives us a function of this sufficient statistic. We take the logarithm
\begin{align*}
	\log\like(\theta;\ulx)&\propto n\log\theta-\theta\sum_{i=1}^{n}x_{i}^{2}
\end{align*}
and the the derivative
\begin{align*}
	\begin{carray}
		\deriv{\theta}\log\like(\theta;\ulx)=\frac{n}{\theta}-\sum_{i=1}^{n}x^{2}_{i}=0\\
		\Downarrow\\
		\widehat{\theta}_{n}=\frac{n}{\sum_{i=1}^{n}x_{i}^{2}}.
	\end{carray}
\end{align*}
\subsection{Method of moments}
There is another approach to learn information about a parameter. Consider the expected value of the previous example:
\begin{align*}
	\ev{X}&=\int_{0}^{\infty}2\theta x^{2}\expg{-\theta x^{2}}\dx\\
	&=\int_{0}^{\infty}2\theta y\expg{-\theta y}\unmezz y^{\unmezz-1}\dy\\
	&=\frac{2\theta}{2}\int_{0}^{\infty}y^{\unmezz\mathcolor{Purple3}{+1-1}}\expg{-\theta y}\dy\\
	&=\frac{\theta\Gamma\left(\unmezz+1\right)}{\theta^{\unmezz+1}}\\
	&=\frac{\Gamma\left(\unmezz+1\right)}{\sqrt{\theta}}\\
	&=\frac{\unmezz\Gamma\left(\unmezz\right)}{\sqrt{\theta}}\\
	&=\frac{\unmezz\sqrt{\pi}}{\sqrt{\theta}}
\end{align*}
and this is my theoretical moment. To get the estimator just compare this with the empirical moment of order 1:
\begin{equation*}
	\begin{carray}
		\frac{\sqrt{\pi}}{2}\theta^{-\unmezz}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\\
		\Downarrow\\
		\widetilde{\theta}_{n}=\frac{n^{2}\pi}{4\left(\sum_{i=1}^{n}x_{i}\right)^{2}}
	\end{carray}
\end{equation*}
So we see that the \emph{method of moment estimator (MME)} requires much less information if compared with MLE: we only need the moments. But then how can we choose between
\begin{equation*}
	\widetilde{\theta}_{n}=\frac{n^{2}\pi}{4\left(\sum_{i=1}^{n}x_{i}\right)^{2}}\qquad\text{ and }\qquad	\widehat{\theta}_{n}=\frac{n}{\sum_{i=1}^{n}x_{i}^{2}}?
\end{equation*}
In this case there is not much to choose: the MME is not a function of the sufficient statistic, so the MLE will always be better... But this is not always true. Let's find the Bayes estimator. As a prior distribution we choose the inverse normal and we suppose $\theta\sim\mathsf{NegExp}(\lambda)$. We apply Bayes and compute $\ev{\theta|X_{i},\ldots,X_{n}}$
\begin{equation*}
	\theta^{\ast}_{n}=\frac{n-1}{\lambda+\sum_{i=1}^{n}x_{i}^{n}}.
\end{equation*}
In this case the estimator \textit{is} a function of the sufficient statistic, so it is not so clear which one is the best.\par
Now take a collection of \rv s $(X_{1},\ldots,X_{n})$ from $X\distunif{a-b,a+b}$ and we want to learn $a$ and $b$. The simplest thing to do is to use the method of moments because the uniform distribution has very simple moments:
\begin{equation*}
	\begin{larray}
		\ev{X}=a\\
		\ev{X^{2}}=\frac{b^{2}}{3}+a^{2}.
	\end{larray}
\end{equation*}
Now compare these quantities with the empirical moments and solve for $a$ and $b$:
\begin{equation*}
	\begin{cases}
		\frac{1}{n}\sum_{i=1}^{x}=a\\
		\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}=\frac{b^{2}}{3}+a^{2}
	\end{cases}\implies\begin{cases}
	\widetilde{a}_{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\\
	\widetilde{b}_{n}=\left[\frac{3}{n}\sum_{i=1}^{n}x_{i}^{2}-3\left(\widetilde{a}_{n}\right)^{2}\right]^{\unmezz}.
	\end{cases}
\end{equation*}
These are quite complicated. Also we could compute mean and variance but computing the distribution is not trivial since we are adding uniform variables (which always brings us out of support). We cannot even use Fisher information because the model is not regular and therefore the likelihood is not differentiable. The Cramer-Rao bound does not exist in this case, so I need to proceed in another way. Let's now compute the MLE by computing the likelihood function:
\begin{align*}
	\like(a,b;\ulx)&=\prod_{i=1}^{n}\frac{1}{2b}\indi_{[a-b,a+b]}(x_{i})\\
	&=\left(\frac{1}{2b}\right)^{n}\prod_{i=1}^{n}\indi_{[a-b,a+b]}(x_{i}).
\end{align*}
Savage's theorem applies to non-regular models as well. To apply it we can rewrite the product of the indicators as
\begin{align*}
	\prod_{i=1}^{n}\indi_{[a-b,a+b]}(x_{i})=\begin{cases}
		1&\text{if }a-b\leq\min_{i}x_{i}\wedge a+b\geq\max_{i}x_{i}\\
		0&\text{otherwise}
	\end{cases}
\end{align*}
so 
\begin{equation*}
	\left(\min_{i}X_{i},\max_{i}X_{i}\right)
\end{equation*}
is a sufficient statistic and in particular if I want to maximize the likelihood function I need $b$ as small as possible (constrained to $a-b\leq\min_{i}x_{i}\wedge a+b\geq\max_{i}x_{i}$). Since $b$ is a denominator we take the smallest value of b that satisfies
\begin{equation*}
	a-b\leq\min_{i}x_{i}\wedge a+b\geq\max_{i}x_{i}
\end{equation*}
for some $a$; this brings us to the inequality that we must satisfy
\begin{equation*}
	\unmezz\left(\max_{i}x_{i}-\min_{i}x_{i}\right)\leq b.
\end{equation*}
and since $b_{n}$ must be the smallest possible the inequality becomes an equation. This means that
\begin{equation*}
	\widehat{b}_{n}=\unmezz\left(\max_{i}X_{i}-\min X_{i}\right).
\end{equation*}
To find $\widehat{a}_{n}$ just plug it in the equality and get
\begin{equation*}
	\widehat{a}_{n}=\unmezz\left(\max_{i}X_{i}+\min X_{i}\right).
\end{equation*}
Note that those are functions of the sufficient statistic, while the MME is not... so we would prefer this MLE.
\begin{remark}
	In those cases, when the model is not regular, there is not a standard approach: we must study each individual function and find a way around. 
\end{remark}
\begin{exercise}
	Consider $(X_{1},\ldots,X_{n})$ from $X\sim\sim\mathsf{NegExp}(1)$ with
	\begin{equation*}
		f_{X}(x)=e^{-x}\indi_{\R^{+}}(x).
	\end{equation*}
	I now introduce a parameter in the form of truncation parameter. Take $(Y_{1},\ldots,Y_{n})$ from $Y\sim F_{\theta}$ where $F_{\theta}$ has density function
	\begin{equation*}
		f_{Y}(y;\theta)=ce^{-x}\indi_{(\theta,\infty)}(x).
	\end{equation*}
	Try to estimate $\theta$ with MME and MLE.
\end{exercise}
\begin{exercise}
	Take a random sample $(X_{1},\ldots,X_{n})$ form $X\distbernoulli{\theta}$ and take $X=\sum_{i=1}^{n}X_{i}\sim\mathsf{Binom}(n,\theta)$. Imagine we know $\theta$ but not $n$; compute MME and MLE for $n$.
\end{exercise}
\subsection{Invariance property}
Take $X_{1},\ldots,X_{n}$ from $F_{\theta}$ with $\theta\in\Theta$. The MLE estimator is
\begin{equation*}
	\theta^{\star}=\argmax_{\Theta}\log\like(\theta;\ulx).
\end{equation*}
Suppose that we are interested in a function $\tau(\theta)=\eta$. What is the MLE of $\tau$? This is very trivial when $\tau$ is injective because we can use the inverse function and remap it to a single point, but we can prove this for any function.
\begin{theorem}
	Let $(X_{1},\ldots,X_{n})$ be a random sample from a model $X\sim F_{\theta}$ with $\theta\in\Theta$. If $\widehat{\theta}_{n}$ is the maximum likelihood estimator for $\theta$ then $\tau\left(\widehat{\theta}_{n}\right)$ is the maximum likelihood estimator for $\tau(\theta)$ for any function $\tau(\cdot)$.
\end{theorem}
So if we take the MLE for $\theta$ we just have to plug it into $\tau$. Let's see some examples of this.
\begin{itemize}
	\item Take $\rsampx$ from $X\distnorm{\mu,1}$, with $\mu$ unknown. We observe only the number of observations that are smaller than 0. We need to find the MLE of $\mu$. If the $X_{i}$ are observed then 
	\begin{equation*}
		\widehat{\mu}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}.
	\end{equation*}
	In this case we are actually observing a variable
	\begin{equation*}
		Y_{i}=\begin{cases}
			1&\text{if }X_{i}<0\\
			0&\text{otherwise.}
		\end{cases}
	\end{equation*}
	$Y_{i}$ is a ``success or not'' variable and as such it is a Bernoulli variable:
	\begin{equation*}
		Y_{i}\distbernoulli{\pr\left(X_{i}<0\right)}\qquad\text{with}\qquad\pr\left(X_{i}<0\right)=\Phi(-\mu).
	\end{equation*}
	This means that the likelihood for the statistic $T=\sum_{i=1}^{n}Y_{i}$ is a binomial density
	\begin{equation*}
		\pr\left(T<t\right)={n\choose t}\left(\Phi(-\mu)\right)^{2}\left(1-\Phi(-\mu)\right)^{n-t}
	\end{equation*}
	and the likelihood is
	\begin{equation*}
		\log\like(\Phi(-\mu),t)=t\log\left(\Phi(-\mu)\right)+(n-t)\log\left(1-\Phi(-\mu)\right).
	\end{equation*}
	Taking the derivative the MLE of the function $\Phi(-\mu)$ is
	\begin{equation*}
		\widehat{\Phi(-\mu)}=\frac{t}{n}.
	\end{equation*}
	By invariance property of MLEs the MLE of $\mu$ will be 
	\begin{equation*}
		\widehat{\mu}_{n}=\Phi\left(\frac{t}{n}\right).
	\end{equation*}
	\item Let $\rsampx$ be a sample of waiting times before an object breaks from $X\distexp{\theta}$. When we look for survival rates (i.e. the probability of an object to break at a certain time) we can estimate the average breaking time $\theta$ by simply counting how many object have survived after a certain time threshold $T$ instead of computing the breaking time for a lot of objects (boring \& not sexy). This means counting the number of successes, so the probability of success is again a Bernoulli with probability of success
	\begin{equation*}
		\pr[X_{i}\geq T]=e^{-\theta T}\qquad\text{for some }T>0.
	\end{equation*}
	So in this case $f(\theta)=e^{-\theta T}$. We have our new ``counting'' \rv:
	\begin{equation*}
		Y_{i}=\begin{cases}
			1&\text{if }X_{i}\geq T\\
			0&\text{otherwise}.
		\end{cases}
	\end{equation*}
	Consider out statistic $T=\sum_{i=1}^{n}Y_{i}$. We have a binomial distribution
	\begin{equation*}
		\pr\left(T\geq t\right)={n\choose t}\left(e^{-\theta T}\right)^{t}\left(1-e^{-\theta T}\right)^{n-t}
	\end{equation*}
	with likelihood function
	\begin{equation*}
		\log\like\left(e^{-\theta T};t\right)\propto t\log e^{-\theta T}+(n-t)\log\left(1-e^{-\theta T}\right).
	\end{equation*}
	Doing the optimization we get that the MLE of $e^{-\theta T}$ is $\frac{t}{n}$. Since we actually used the inverse of the density function, we have 
	\begin{equation*}
		f^{-1}(\theta)=-\frac{1}{T}\log\theta.
	\end{equation*}By the invariance property we get
	\begin{equation*}
		\widehat{\theta}_{n}=-\frac{1}{T}\log\frac{t}{n}\qquad t\neq0.
	\end{equation*}
	\item Consider a \emph{randomized experiment}. Suppose that we have a sample of individuals and we make a survey based on one question whose answer can be ``yes'' or ``no'' because we want to understand the fraction of the population which belongs to ``yes = 1'' and ``no = 1''. In many situations the question is personal and we need to protect the privacy of people who answer. The classic example of a randomized experiment is the following: imagine that we have a coin with known probability of success
	\begin{equation*}
		\pr\left(\left\{\text{H}\right\}\right)=\pr\left(\{1\}\right)=p.
	\end{equation*}
	Instead of asking people to answer ``yes'' or ``no'' to our question, we ask the other person to go in another room, flip the coin and then telling us whether the coin flip (1 or 0) result coincides with our answer (1 or 0). Then our model for the probability of a person telling us ``they coincide'' with the unknown fact that they belong to group $A$ (from the original question) becomes
	\begin{equation*}
		\pr(X_{i}=1)=\ubracketthin{\pr\left(X_{i}=1|A\right)\pr(A)}_{p\cdot\theta}+\ubracketthin{\pr\left(X_{i}=1|A^{c}\right)\pr(A^{c})}_{(1-p)(1-\theta)}.
	\end{equation*}
	We are ultimately interested in estimating $\theta$ but we need to only use $\rsampx$ from $X\distbernoulli{p\theta+(1-p)(1-\theta)}$. We know how to estimate the MLE of a Bernoulli parameter:
	\begin{equation*}
		\text{the MLE is }p\theta+(1-p)(1-\theta)=\frac{1}{n}\sum_{i=1}^{n}X_{i}.
	\end{equation*}
	So we solve for $\theta$ and by the invariance principle the MLE of $\theta$ is
	\begin{equation*}
		\widehat{\theta}_{n}=\frac{\frac{1}{n}\sum_{i=1}^{n}X_{i}-(1-p)}{2p-1}\qquad p\neq\unmezz.
	\end{equation*}
	But... this means that the coin cannot be fair and it must be biased... which means that I could \textit{technically} recover information about the person telling me the answer. Yikes. This method is useless in real life.
\end{itemize}
This also means that we can have a version of the CLT for the function $\tau(\theta)$: this is called \textit{delta method}.
\subsection{Consistency and other asymptotic properties for MLEs}
\begin{definition}
	Let $\rsampx$ be a random sample from a model $X\sim F_{\theta}$ with $\theta\in\Theta$. Let $T_{n}=T\rsampx$ be an estimator for $\theta$. Let $\theta_{0}\in\Theta$ be the ``true'' value for $\theta$ (also called the \textit{true generating mechanism}). We say that $T_{n}$ is \emph{weakly consistent for $\theta$} if
	\begin{equation*}
		T_{n}\convpr\theta_{0}\qquad n\to\infty
	\end{equation*}
	which in other words means
	\begin{equation*}
		\lim_{n\to\infty}\pr\left(|T_{n}-\theta_{0}|>\varepsilon;\theta_{0}\right)=0\qquad\every\varepsilon>0\text{ fixed}.
	\end{equation*}
\end{definition}
We also have a strong version with almost sure convergence instead of convergence in probability.
\begin{definition}
	Let $\rsampx$ be a random sample from a model $X\sim F_{\theta}$ with $\theta\in\Theta$. Let $T_{n}=T\rsampx$ be an estimator for $\theta$. Let $\theta_{0}\in\Theta$ be the true generating mechanism for $\theta$. We say that $T_{n}$ is \emph{strongly consistent for $\theta$} if
	\begin{equation*}
		T_{n}\convas\theta_{0}\qquad n\to\infty
	\end{equation*}
	which in other words means
	\begin{equation*}
		\pr\left(\left\{\lim_{n\to\infty}T_{n}=\theta_{0}\right\};\theta_{0}\right)=1
	\end{equation*}
	or alternatively
	\begin{equation*}
		\lim_{n\to\infty}\pr\left(|T_{m}-\theta_{0}|<\varepsilon\quad\every m>n;\theta_{0}\right)=1.
	\end{equation*}
\end{definition}
For example, take $\rsampx$ from $X\distnorm{\theta,1}$ with $\theta$ unknown. Consider the MLE
\begin{equation*}
	\widehat{\theta}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}.
\end{equation*}
In this case it is very easy to check that $\widehat{\theta}_{n}$ is strongly consistent because it is written in a way that lets us apply the SLLN directly:
\begin{equation*}
	\widehat{\theta}_{n}\convas\theta_{0}.
\end{equation*}
\begin{remark}
	In general, proving consistency (weak or strong) reduces to an application of the LLN.
\end{remark}
Consider $\rsampx$ from $X\sim f_{X}(x;\theta)$ with $\theta\in\Theta$. The log-likelihood is
\begin{equation*}
	\log\like(\theta;\ulx)=\sum_{i=1}^{n}\log f_{X_{i}}(x_{i};\theta).
\end{equation*}
Consider the sequence
\begin{equation*}
	{\left(\log_{n}\like(\theta;\ulx)\right)}_{n\geq 1}.
\end{equation*}
Let $\theta_{0}$ be the true generating mechanism such that $\theta\in\Theta$.
\begin{align*}
	\unmezz\left(\log_{n}\like(\theta;\ulx)-\log_{n}\like(\theta_{0},\ulx)\right)&=\frac{1}{n}\sum_{i=1}^{n}\log\frac{f_{X_{i}}(x_{i};\theta)}{f_{X_{i}}(x_{i},\theta_{0})}\\
	&\convas\ev{\log\frac{f_{X_{i}}(x_{i};\theta)}{f_{X_{i}}(x_{i},\theta_{0})};\theta_{0}}\\
	\text{\footnotesize Jensen's inequality}\qquad&<\log\ubracketthin{\ev{\frac{f_{X_{i}}(x_{i},\theta)}{f_{X_{i}}(x_{i},\theta_{0})};\theta_{0}}}_{\claptext{$=1$ since this is the full density}}=0.
\end{align*}
\begin{remark}
	Heuristically, we can think of $\theta_{0}$ as a very likely point, in a certain sense the ``best'' point. When I accumulate many data points (when $n\to\infty$) the likelihood function in $\theta_{0}$ is much more larger than the likelihood function for any other $\theta\neq\theta_{0}$. This intuitively tells us that my MLE is converging on $\theta_{0}$.
\end{remark}
Consider the parameter space $\Theta=\left\{\theta_{0},\theta_{1},\ldots,\theta_{n}\right\}$ being a finite space of dimension $m\N$. Take $\varepsilon>0$ and for $j=1,\ldots,m$ we define the event $A_{j}$ as
\begin{equation*}
	A_{j}=\left\{\log_{n}\like(\theta_{0})-\log_{n}\like(\theta_{j})>\varepsilon\quad \every n>n_{0}\right\}.
\end{equation*}
We can take $n_{0}$ sufficently large such that
\begin{equation*}
	\pr(A_{j})>1-\delta\qquad\every j=1,\ldots, m
\end{equation*}
for a fixed (arbitrary) $\delta>0$. We know that 
\begin{align*}
	\pr\left(\bigcap_{j=1}^{m}A_{j}\right)&=1-\pr\left(\bigcup_{j=1}^{n}A_{j}^{c}\right)\\
	&\geq 1-\sum_{j=1}^{m}\pr\left(A_{j}^{c}\right)\\
	&=1-\sum_{i=1}^{n}\left(1-\pr\left(A_{T}\right)\right)\\
	&\geq 1-\cancel{m}+\cancel{m}+m\delta\\
	&=1-m\delta.
\end{align*}
So we proved that 
\begin{equation*}
	\pr\left(\lim_{n}(\theta_{0})-\log_{n}(\theta_{j})>\varepsilon\quad \every j\in\{1,\ldots,m\}\right)
\end{equation*}
and $n>n_{0}\geq 1-m\delta$. $f$ can be arbitrarily small. \par
Another important asymptotic property is the Gaussian limit (a sort of CLT for estimators). To understand it we need to revise some definitions.
\begin{definition}
	Let ${(X_{n})}_{n\geq 1}$ be a sequence of \rv s and let ${(r_{n})}_{n\geq 1}$ be a sequence of positive real numbers.
	\begin{enumerate}[\circnum]
		\item $X_{n}$ is of \emph{smaller order than $r_{n}$ \ul{in probability}} ($X_{n}=\mathcal{o}_{\pr}(r_{n})$) if
		\begin{equation*}
			\frac{X_{n}}{r_{n}}\convpr0\qquad n\to\infty;
		\end{equation*} 
		\item $X_{n}$ is \emph{at most of the same order than $r_{n}$ \ul{in probability}} ($X_{n}=\mathscr{O}_{\pr}(r_{n})$) if $\every\varepsilon>0,\exists M_{\varepsilon}$ such that
		\begin{equation*}
			\pr\left(\left|\frac{X_{n}}{r_{n}}\right|>M_{\varepsilon}\right)<\varepsilon
		\end{equation*}
		for each $n>N_{\varepsilon}$ (with $N_{\varepsilon}$ depending on $\varepsilon$).
	\end{enumerate}
\end{definition}
\begin{remark}
	If $r_{n}$ is a constant sequence (for example if $r_{n}=1$) then $X_{n}=\mathcal{o}_{\pr}(1)$ is equivalent to 
	\begin{equation*}
		X_{n}\convpr 0,\qquad n\to\infty.
	\end{equation*}
	Similarly, $X_{n}=\mathscr{O}_{\pr}(1)$ means that $X_{n}$ does not \faBomb explode \faBomb in probability.
\end{remark}
\begin{theorem}
	Take two sequences of positive real numbers ${(r_{n})}_{n\geq1}$ and ${(s_{n})}_{n\geq1}$. Take two sequences of \rv s ${(X_{n})}_{n\geq1}$ and ${(Y_{n})}_{n\geq1}$. Then
	\begin{enumerate}
		\item \begin{align*}
			X_{n}&=\mathcal{o}_{\pr}(r_{n})\wedge Y_{n}=\mathcal{o}_{\pr}(s_{n})\\
			&\implies X_{n}Y_{n}=\mathcal{o}_{\pr}(r_{n}s_{n})\\
			&\implies X_{n}+Y_{n}=\mathcal{o}_{\pr}\left(\max\{r_{n},s_{n}\}\right);
		\end{align*}
		\item \begin{align*}
			X_{n}&=\mathscr{O}_{\pr}(r_{n})\wedge Y_{n}=\mathscr{O}_{\pr}(s_{n})\\
			&\implies X_{n}Y_{n}=\mathscr{O}_{\pr}(r_{n}s_{n})\\
			&\implies X_{n}+Y_{n}=\mathscr{O}_{\pr}\left(\max\{r_{n},s_{n}\}\right);
		\end{align*}
		\item \begin{align*}
			X_{n}&=\mathscr{O}_{\pr}(r_{n})\wedge Y_{n}=\mathcal{o}_{\pr}(s_{n})\\
			&\implies X_{n}Y_{n}=\mathcal{o}_{\pr}(r_{n}s_{n})
		\end{align*}
	\end{enumerate}
\end{theorem}
\begin{fancyproof}
	We will prove just the first statement. Given $\varepsilon>0$ and $\delta>0$ then there exists $N_{\varepsilon}$ such that
	\begin{equation*}
		\begin{larray}
			\pr\left(\left|\frac{X_{n}}{r_{n}}\right|>\varepsilon\right)<\delta\\
			\pr\left(\left|\frac{Y_{n}}{s_{n}}\right|>\varepsilon\right)<\delta.
		\end{larray}
	\end{equation*}
	I want to control the probability of the product:
	\begin{align*}
		\pr\left(\left|\frac{X_{n}Y_{n}}{r_{n}s_{n}}\right|>\varepsilon^{2}\right)&\leq\pr\left(\left\{\left|\frac{X_{n}}{r_{n}}\right|>\varepsilon\right\}\cup\left\{\left|\frac{Y_{n}}{s_{n}}\right|>\varepsilon\right\}\right)\\
		&\leq\pr\left(\left\{\left|\frac{X_{n}}{r_{n}}\right|>\varepsilon\right\}\right)+\pr\left(\left\{\left|\frac{Y_{n}}{s_{n}}\right|>\varepsilon\right\}\right)\\
		&<2\delta\qquad\every n>N_{\varepsilon}.
	\end{align*}
	Set $q=\max\left\{r_{n},s_{n}\right\}$. For the same $\varepsilon,\delta>0$ and $N_{\varepsilon}$ we have 
	\begin{equation*}
		\begin{carray}
			\pr\left(\left\{\left|\frac{X_{n}}{r_{n}}\right|>\varepsilon\right\}\right)<\delta\qquad\pr\left(\left\{\left|\frac{Y_{n}}{s_{n}}\right|>\varepsilon\right\}\right)<\delta.
		\end{carray}
	\end{equation*}
	So
	\begin{align*}
		\pr\left(\left|\frac{X_{n}+Y_{n}}{q_{n}}\right|>2\varepsilon\right)&\leq\pr\left(\left|\frac{X_{n}}{q_{n}}\right|+\left|\frac{Y_{n}}{q_{n}}\right|>2\varepsilon\right)\\
		&\leq\pr\left(\left\{\left|\frac{X_{n}}{q_{n}}\right|>\varepsilon\right\}\cup\left\{\left|\frac{Y_{n}}{q_{n}}\right|>\varepsilon\right\}\right)\\
		&\leq\pr\left(\left\{\left|\frac{X_{n}}{q_{n}}\right|>\varepsilon\right\}\right)+\pr\left(\left\{\left|\frac{Y_{n}}{q_{n}}\right|>\varepsilon\right\}\right)\\
		&<2\delta\qquad\every n>N_{\varepsilon}.
	\end{align*}
\end{fancyproof}
\begin{theorem}
	Let ${(X_{n})}_{n}$ be a sequence of \rv s and let $\seqn{r}$ be a sequence of (positive) reals such that
	\begin{equation*}
		r_{n}(X_{n}-c)\convw Z\qquad n\to\infty
	\end{equation*}
	where $c$ is a constant and $Z$ is a non degenerate \rv. Then
	\begin{equation*}
		X_{n}=c+\mathscr{O}_{\pr}\left(r_{n}\right).
	\end{equation*}
\end{theorem}
\begin{fancyproof}
	Denote by $F(\cdot)$ the c.d.f. of $Z$. Let $t$ be a point of continuity of this function $F(\cdot)$. The there exist a sequence of positive reals $\seqn{s}$ such that $s_{n}\to0$ and
	\begin{align*}
		G(t)-s_{n}<\pr\left(\left|r_{n}(X_{n}-c)\right|>t\right)<G(t)+\varepsilon
	\end{align*}
	where $G(t)=\pr(|Z|>t)$ and $\varepsilon>0$. Remember that we are dealing with a sequence of distributions, which is basically a sequence of numbers (not \rv s!) Since $Z$ is non-degenerate then taking $\varepsilon>0$ we can find a value $t_{\varepsilon}$ such that 
	\begin{equation*}
		G(t_{\varepsilon})<\varepsilon.
	\end{equation*}
	Since $s_{n}\to0$ then there exists a value $n_{0}$ such that for all $n>n_{0}$ such that
	\begin{equation*}
		\ubracketthin{\pr\left(\left|
		r_{n}(X_{n}-c)\right|>t_{\varepsilon}\right)}_{\claptext{but this is the definition of $\mathscr{O}_{\pr}$}}<G(t_{
		\varepsilon})+s_{n}<\varepsilon
	\end{equation*}
	so we have that 
	\begin{equation*}
		X_{n}-c=\mathscr{O}_{\pr}\left(r_{n}\right).
	\end{equation*}
\end{fancyproof}
\begin{theorem}
	Let $\seqn{X}$ be a sequence of \rv s such that 
	\begin{equation*}
		X_{n}=c+\mathscr{O}_{\pr}(r_{n})
	\end{equation*}
	where $x$ is a constant and $\seqn{r}$ is a sequence of (positive) real numbers with $r_{n}\to0.$ If $g(\cdot)$ is a function with $k$ (continuous) derivatives then
	\begin{equation*}
		g(X_{n})=g(c)+g'(c)(X_{n}-c)+\ldots+\frac{g^{(k)}(c)}{k!}(X_{n}-c)^{k}+\mathcal{o}_{\pr}\left(r_{n}^{k}\right).
	\end{equation*}
\end{theorem}
So this is something similar to a Taylor expansion for functions of \rv s.
\begin{fancyproof}
	Take the Taylor expansion for $g(\cdot)$ around $c$ up to the order $k$ such that the remainder is
	\begin{equation*}
		\frac{\left(X_{n}-c\right)^{k}}{k!}\left(g^{(k)}\left(Z_{n}\right)-g^{(k)}(c)\right)
	\end{equation*}
	with $Z_{n}\in(c,X_{n})$; but this is equivalent to saying $Z_{n}\in(c,c+\mathscr{O}_{\pr}(r_{n}))$ by the assumption of the theorem. Since $r_{n}\to0$ then $Z_{n}$ converges in distribution, but \ul{since the limit it constant} I can also say it converges in probability to the constant (this is true \textit{only because $c$ is a constant}\footnote{This is true \textbf{just because} $c$ is a constant}):
	\begin{equation*}
		X_{n}\convw c\ubracketthin{\implies}_{\claptext{what makes this true is that $c$ is a \textsc{constant}}} Z_{n}\convpr c\implies Z_{n}-c=\mathcal{o}_{\pr}(1).
	\end{equation*}
	Then I use the fact that all derivatives are continuous functions to say that 
	\begin{equation*}
		\begin{carray}
			(X_{n}-c)^{k}=\mathscr{O}_{\pr}\left(r_{n}^{k}\right)\\\Downarrow\\\left(g^{(n)}(Z_{n})-g^{(n)}(c)\right)\left(X_{n}-c\right)^{k}=\mathcal{o}_{\pr}\left(r^{k}_{n}\right).
		\end{carray}
	\end{equation*}
\end{fancyproof}
\begin{theorem}
	\emph{Delta Method}.	Let $\seqn{X}$ be a sequence of \rv s such that 
	\begin{equation*}
		\sqrt{n}\left(X_{n}-c\right)\convw Z\qquad n\to\infty
	\end{equation*}
	where $c$ is a constant and $Z$ is a non-degenerate \rv. Take a function $g(\cdot)$ with continuous derivatives. Then
	\begin{equation*}
		\sqrt{n}(g(X_{n})-g(c))\convw Zg'(c)\qquad n\to\infty.
	\end{equation*}
\end{theorem}
The proof is just a replica of the proof of the previous theorem, where we have to take the Taylor expansion of $g(\cdot)$ at $c$.\par
We can now look at the Cramer theorem. But first remember the most important features of regularity:
\begin{itemize}
	\item $\exists$ derivatives up to order 3;
	\item $\exists$ the integrable functions $g(\cdot),h(\cdot),H(\cdot)$ such that 
	\begin{equation*}
		\begin{carray}
			\left|\deriv{\theta}f_{X}(x;\theta)\right|\leq g(x)\qquad\text{for all }x\\
			\left|\dderiv{\theta}f_{X}(x;\theta)\right|\leq h(x)\qquad\text{for all }x\\
			\left|\ddderiv{\theta}\log f_{X}(x;\theta)\right|\leq H(x).\\
		\end{carray}
	\end{equation*}
\end{itemize}
\begin{theorem}
	\emph{Gaussian limit / Cramer theorem}\footnotemark.
	Let $\rsampx$ be a sample from a regular model $X\sim f_{X}(x;\theta)$. We assume regularity for the model (all the conditions above apply) and moreover we assume
	\begin{equation*}
		\ev{H(x);\theta}<M_{0}<+\infty\qquad\every\theta\in\Theta.
	\end{equation*} If $\widehat{\theta}_{n}$ is the MLE for $\theta$  such that
	\begin{equation*}
		\widehat{\theta}_{n}\convpr\theta_{0}\qquad\theta_{0}\in\Theta
	\end{equation*}then as $n\to\infty$
	\begin{equation*}
		\sqrt{n}(\widehat{\theta}_{n}-\theta_{0})\convw R\distnorm{0,\frac{1}{I(\theta)}}
	\end{equation*}
	where $I(\theta)$ is the Fisher information of the model.
\end{theorem}
\footnotetext{If you are interested in the proof you can find it in \textit{A Course on Large Sample Theory}, Ferguson 1995.}
The proof uses what we have seen so far plus the Slutsky theorem.
\begin{fancyproof}
	Let 
	\begin{equation*}
		l(\theta)=\log\like(\theta;\ulx).
	\end{equation*}
	Take $\theta_{0}\in\Theta$ . We have
	\begin{equation*}
		l'(\theta)=l'(\theta_{0})+l''(\theta_{0})(\theta-\theta_{0})+\unmezz l'''\left(\widetilde{\theta}\right)(\theta-\theta_{0})^{2}
	\end{equation*}
	where $\widetilde{\theta}\in\left(\theta,\theta_{0}\right)$. Replace $\theta$ with the MLS $T_{n}:=\widehat{\theta}_{n}$.
	We have
	\begin{equation*}
		l'(\theta)=l'(\theta_{0})+l''(\theta_{0})\left(\widehat{\theta}_{n}-\theta_{0}\right)+\unmezz l'''\left(\widetilde{\theta}\right)\left(\widehat{\theta}_{n}-\theta_{0}\right)^{2}
	\end{equation*}
	where $\widetilde{\theta}\in\left(\widehat{\theta}_{n},\theta_{0}\right)$. Set 
	\begin{equation*}
		\begin{carray}
			0=l'(\theta_{0})+l''(\theta_{0})\left(\widehat{\theta}_{n}-\theta_{0}\right)+\unmezz l'''\left(\widetilde{\theta}\right)\left(\widehat{\theta}_{n}-\theta_{0}\right)^{2}\\
			\Downarrow\\
			\sqrt{n}\left(\widehat{\theta}_{n}-\theta_{0}\right)=\frac{-\frac{1}{\sqrt{n}}l'(
				\theta_{0})}{\frac{1}{n}l''(\theta_{0})+\frac{l'''\left(\widetilde{\theta}\right)}{2n}\left(\widehat{\theta}_{n}-\theta_{0}\right)}.
		\end{carray}
	\end{equation*}
	The rest of the proof revolves around doing the CLT on the l.h.s. of this last equation. \begin{itemize}
	\item 	Start with the numerator:
	\begin{align*}
		-\frac{1}{\sqrt{n}}l'(\theta_{0})&=-\frac{1}{\sqrt{n}}\deriv{\theta_{0}}\log\like(\theta_{0};\ulx)\\
		&=-\frac{1}{\sqrt{n}}\deriv{\theta_{0}}\sum_{i=1}^{n}\log f_{X_{i}}(X_{i};\theta)\\
		&=-\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\deriv{\theta_{0}}\log f_{X_{i}}(X_{i};\theta)\\
		&=-\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\ubracketthin{\frac{\deriv{\theta_{0}}f_{X_{i}}(X_{i};\theta_{0})}{f_{X_{i}}(X_{i};\theta_{0})}}_{W_{i}}\\
		&=-\frac{1}{\sqrt{n}}\sum_{i=1}^{n}W_{i}\\
		&=-\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^{n}W_{i}\right)
	\end{align*}
	but $W_{i}$ is just the score function so 
	\begin{equation*}
		\ev{W_{i}}=0\qquad\var(W_{i})=I(\theta_{0}).
	\end{equation*}
	But this is just a sum of i.i.d. \rv s so by CLT we have
	\begin{equation*}
		-\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^{n}W_{i}\right)\convw\mathsf{N}\left(0;I(\theta_{0})\right)\qquad n\to\infty
	\end{equation*}
	where the $-$ sign is not a problem (Gaussian distribution is symmetric around 0) and the variance comes from the fact that $W_{i}$ is the score function.
	\item Now analyze the denominator:
	\begin{align*}
		\frac{1}{n}l''(\theta_{0})&=\frac{1}{n}\deriv{\theta_{0}}\sum_{i=1}^{n}\frac{\deriv{\theta_{0}}f_{X_{i}}(x_{i};\theta_{0})}{f_{X_{i}}(x_{i};\theta_{0})}\\
		&=\frac{1}{n}\sum_{i=1}^{n}\left[\frac{\dderiv{\theta_{0}}f_{X_{i}}(x_{i};\theta_{0})f_{X_{i}}(x_{i};\theta_{0})}{\left(f_{X_{i}}(x_{i};\theta_{0})\right)^{2}}-\frac{\deriv{\theta_{0}}f_{X_{i}}(x_{i};\theta_{0})\deriv{\theta_{0}}f_{X_{i}}(x_{i};\theta_{0})}{\left(f_{X_{i}}(x_{i};\theta_{0})\right)^{2}}\right]\\
		&=\ubracketthin{\frac{1}{n}\sum_{i=1}^{n}\frac{\left(\dderiv{\theta_{0}}f_{X_{i}}(x_{i};\theta_{0})\right)}{f_{X_{i}}(x_{i};\theta_{0})}}_{\claptext{sum of i.i.d. \rv s}}-\ubracketthin{\frac{1}{n}\sum_{i=1}^{n}\left(\frac{\deriv{\theta_{o}}f_{X_{i}}(x_{i};\theta_{0})}{f_{X_{i}}(x_{i};\theta_{0})}\right)^{2}}_{\claptext{sum of i.i.d. \rv s}}.
	\end{align*}
	We analyze the two components applying the LLN:
	\begin{itemize}
		\item $\frac{1}{n}\sum_{i=1}^{n}\frac{\left(\dderiv{\theta_{0}}f_{X_{i}}(x_{i};\theta_{0})\right)}{f_{X_{i}}(x_{i};\theta_{0})}\to0$
		\item $\frac{1}{n}\sum_{i=1}^{n}\left(\frac{\deriv{\theta_{o}}f_{X_{i}}(x_{i};\theta_{0})}{f_{X_{i}}(x_{i};\theta_{0})}\right)^{2}\convas -I(\theta_{0})$ as $n\to\infty$.
	\end{itemize}
	so up to now we have 
	\begin{enumerate}
		\item $-\frac{1}{\sqrt{n}}l'(\theta_{0})\convw R\distnorm{0,I(\theta_{0})}$ as $n\to\infty$
		\item $\frac{1}{n}l''(\theta_{0})\convw-I(\theta)$ as $n\to\infty$.
	\end{enumerate}
	We still have to analyze one more term from the denominator. We know that 
	\begin{equation*}
		\left(\widehat{\theta_{n}}-\theta_{0}\right)\convpr 0\implies \widehat{\theta}_{n}-\theta_{0}=\mathcal{o}_{\pr}(1).
	\end{equation*}
	Using triangular inequality we get
	\begin{align*}
		\frac{1}{n}\left|l'''\left(\widetilde{\theta}\right)\right|&\leq\frac{1}{n}\sum_{i=1}^{n}\left|\ddderiv{\theta}\log f_{X_{i}}(x_{i};\theta)\right|\Bigg|_{\theta=\widetilde{\theta}}\\
		&<\frac{1}{n}\sum_{i=1}^{n}H(X_{i}).
	\end{align*}
	So as $n\to\infty$ for the LLN we get
	\begin{equation*}
		\frac{1}{n}\sum_{i=1}^{n}H(X_{i})\convas\ev{H(X)}<N_{0}<\infty
	\end{equation*}
	so this is a $\mathscr{O}_{\pr}(1)$. So this means that
	\begin{equation*}
	\begin{larray}
		\frac{l'''\left(\widehat{\theta}\right)}{2n}\left(\widehat{\theta}_{n}-\theta_{0}\right)\qquad\text{is}\;\mathcal{o}_{\pr}(1)\\
		\implies\frac{l'''\left(\widehat{\theta}\right)}{2n}\left(\widehat{\theta}_{n}-\theta_{0}\right)\convpr0\\
		\implies\frac{l'''\left(\widehat{\theta}\right)}{2n}\left(\widehat{\theta}_{n}-\theta_{0}\right)\convw0
	\end{larray}
	\end{equation*}
	and this ultimately means
	\begin{equation*}
		\frac{1}{n}l''\left(\theta_{0}\right)+\frac{l'''\left(\widehat{\theta}\right)}{2n}\left(\widehat{\theta}_{n}-\theta_{0}\right)\convw-I(\theta_{0}).
	\end{equation*}
	So we now have the complete convergence:
	\begin{equation*}
		\frac{\overbracket[0.6pt]{-\frac{1}{\sqrt{n}}l'(\theta_{0})}^{\claptext{convergence to a Gaussian $R\distnorm{0,I(\theta_{0})}$}}}{\ubracketthin{\frac{1}{n}l''\left(\theta_{0}\right)+\frac{l'''\left(\widehat{\theta}\right)}{2n}\left(\widehat{\theta}_{n}-\theta_{0}\right)\convw-I(\theta_{0})}_{\claptext{convergence to a constant (not 0)}}}\convw R^{\star}\distnorm{0,\frac{1}{I(\theta_{0})}}.
	\end{equation*}
	\end{itemize}
\end{fancyproof}
So we have shown that 
\begin{equation*}
	\sqrt{n}\left(\widehat{\theta}_{n}-\theta_{0}\right)\convw R\distnorm{0,\frac{1}{I(\theta_{0})}}.
\end{equation*}
So what we found is that MLE is both:
\begin{itemize}
	\item asymptotically unbiased (the mean goes to $\theta_{0}$);
	\item asymptotically efficient (the variace goes to the lower Cramer-Rao bound).
\end{itemize}
\subsection{How large is an asymptote?}
We have seen that 
\begin{equation*}
	\widehat{\theta}_{n}\approx \mathsf{N}\left(\theta_{0},\frac{n}{I(\theta_{0})}\right)\qquad\text{ for $n$ large}.
\end{equation*}
But what does it mean ``large enough''? This is a very recent field of research so probably in next years we will have more satisfactory results. We can still get an idea: we proved that
\begin{equation*}
	\sqrt{nI(\theta_{0})}\left(\widehat{\theta}_{n}-\theta_{0}\right)\convw Z\distnorm{0,1}.
\end{equation*}
We want to quantify a distance between the object and the target distributions which in our case are:
\begin{itemize}
	\item $F_{n}(x)=\pr\left(\sqrt{nI(\theta_{0})}\left(\widehat{\theta}_{n}-\theta_{0}\right)\leq x\right)$;
	\item $\Phi(x)=\pr\left(Z\leq x\right)$.
\end{itemize}
To quantify the distance between distribution we usually use the \emph{Kolmogorov distance}:
\begin{equation*}
	d_{k}(F,G)=\sup_{x}\left|F(x)-G(x)\right|.
\end{equation*}
If we try to bound this distance for our case we find that the distance between $F_{n}$ and $\Phi$ is bounded by a (horrible) constant which does not depend on $n$ divided by $\sqrt[4]{n}$:
\begin{equation*}
	d_{k}(F_{n},\Phi)\leq \frac{c}{n^{\frac{1}{4}}}.
\end{equation*}
This rate is very slow (we typically expect $n^{\frac{1}{2}}$) but the problem is that working with the Kolmogorov distance is hard so we usually work with the weaker distance and then try to bring it to the strong distance. This is why no one bothered doing this research on 80 years old math until recently. \par
Once we have results like this we can build convergence intervals and we apply the delta method to Cramer's theorem: indeed by Cramer we have
\begin{equation*}
	\sqrt{n}\left(\widehat{\theta}_{n}-\theta_{0}\right)\convw R\distnorm{0,\frac{1}{I(\theta_{0})}}
\end{equation*}
and we saw that if $g$ is continuous then
\begin{equation*}
	\sqrt{n}\left(g\left(\widehat{\theta}\right)-g(\theta_{0})\right)\convw g'(\theta_{0})R.
\end{equation*}
\section{Exercises from past exams}
\begin{exercise}
	Let $\rsampx$ be a random sample from $X\distunif{\theta,\theta+|\theta|}$ with $\theta\in\R$. Find the MLE of $\theta$ when:
	\begin{enumerate}
		\item $\theta\in(0,+\infty)$;
		\item $\theta\in(-\infty,0)$;
		\item $\theta\in\R\setminus\{0\}$.
	\end{enumerate}
\end{exercise}
\begin{exercise}
	Let $(X_{1},\ldots,X_{n})$ from $X\sim p_{X}(x;\theta)$ with $\theta>0$. Let
	\begin{equation*}
		p_{X}(x;\theta)=\frac{\theta^{x}e^{-\theta}}{x!\left(1-e^{-\theta}\right)}\int_{\N}(x)\qquad\text{Poisson without 0 in the support}.
	\end{equation*}
	Show that the likelihood equation has a unique root when $\Xbar_{n}>1$. Show whether this is root is MLE for $\theta$. This is not solvable in closed form. \faSmile[regular]
\end{exercise}
\begin{exercise}
	Let $\rsampx$ be a random sample from $X$ such that
	\begin{equation*}
		\log X\sim N(\theta,\theta)\qquad\theta>0.
	\end{equation*}
	Prove that MLE for $\theta$ exists and it is unique. Find the asymptotic distribution of the MLE (apply Cramer).
\end{exercise}
\begin{exercise}
	Let $\rsampx$ be a sample from $X\distnorm{\mu,1}$ with $\mu\in\R$. Let
	\begin{equation*}
		\theta=\pr\left(X_{1}<c\right)
	\end{equation*}
	where $c$ is some known constant. Find the MLE for $\theta$ and compare the efficiency of the MLE (asymptotically) with respect to the efficiency of the following estimators:
	\begin{enumerate}
		\item $\widetilde{\theta}_{n}=\Phi\left(\frac{c-\Xbar_{n}}{\sqrt{1-\frac{1}{n}}}\right)$;
		\item $\theta^{\star}_{n}=\frac{1}{n}\sum_{i=1}^{n}\indi_{(-\infty,c)}(x_{i})$,
	\end{enumerate}
\end{exercise}
\begin{exercise}
	Let $X_{ij}$ with $j=1,\ldots,r>1$ and $i=1,\ldots,n$ be independent and distributed as $\mathsf{N}\left(\mu_{i},\sigma^{2}\right)$. Find the MLE of 
	\begin{equation*}
		\theta=(\mu_{1},\ldots,\mu_{n},\sigma^{2}).
	\end{equation*}
	Show that the MLE of $\sigma^{2}$ is not consistent for $n\to\infty$.
\end{exercise}
\chapter{Testing}
\section{General approach}
\subsection{Introduction to testing}
This is a different approach to the same problem of statistical inference, that is learning about the parameters starting from the data. We still have our random sample $\rsampx$ from $X\sim F_{\theta}$ with $\theta\in\Theta$. So far we assigned values (in various ways) to our parameter $\theta$ and in particular we chose the MLE
\begin{equation*}
	\theta^{\star}=\argmax_{\Theta}\log\like(\theta;\ulx).
\end{equation*}
Differently from point estimation we can learn about $\theta$ by performing a test: this means that we want to test an \textit{hypotesis} like
\begin{equation*}
	\theta=\unmezz\quad\text{against}\quad\theta\neq\unmezz.
\end{equation*}
Testing comes from decision theory and statistical testing it's just an adaptation of decision theory tests with the addition of randomness. There are many ways to perform this test but we will use (again) the likelihood function. We need some terminology for this:
\begin{itemize}
	\item \emph{statistical hypothesis}: this is any sentence that specifies a value or a collection of values for the parameter. We can have:
	\begin{itemize}
		\item \textit{simple hypothesis}: $\theta=\theta^{\star}$;
		\item \textit{composite hypothesis}: $\theta\in\Theta^{\star}\subseteq\Theta$.
	\end{itemize}
	Composite hypothesis can also be of the form
	\begin{equation*}
		\begin{carray}
			\theta>\theta^{\star}\\
			\theta<\theta^{\star}\\
			\theta\neq\theta^{\star}.
		\end{carray}
	\end{equation*}
	The standard setting consists in the specification of two hypotheses:
	\begin{itemize}
		\item \textit{null hypothesis} $H_{0}$;
		\item \textit{alternative hypothesis} $H_{1}$.
	\end{itemize}
	How do we decide which hypothesis is the null and which is the alternative? We usually select as null hypothesis as the one that we believe it's actually true and that we don't \textit{really want} to reject. In other words, what we hope is the true one. This is actually important because the null hypothesis is the one that we are less likely to reject by error, that is to reject the hypothesis just because of noise in the data;
	\item \emph{random sample} $\rsampx$ from $X\sim F_{\theta}$;
	\item \emph{decision rule} (statistic or a function of the random sample).
\end{itemize}
A test is an object that verifies the interaction between hypotheses and decision rule. If we specify 
\begin{equation*}
	\begin{larray}
		H_{0}:\theta\in\Theta_{0}\subseteq\Theta\\
		H_{1}:\theta\notin\Theta_{0}
	\end{larray}
\end{equation*}
then we are specifying a partition of the parametric space $\Theta$ in two areas. 
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[line width=1.5pt]
		%The empty ellipse node with minimum width 6cm and minimum height 4cm
		\node [ellipse,draw,minimum width=6cm, minimum height=4cm] (ep) at (0,0) {};
		
		%Draw the BÃ©zier curve and the horizontal line to make segments inside the ellipse
		\draw ([shift={(0.5\pgflinewidth,0.5\pgflinewidth)}]ep.215)
		.. controls ++(2,-0.5) and ++ (0,0)
		.. node (mp) [pos=0.485] {}  ([yshift=0\pgflinewidth]ep.65);
		
		%Draw label omega outside the ellipse
		\node (lb) [draw,circle] at ([shift={(-0.5,0.5)}]ep.155) {$\Theta$};
		\draw (ep.155) -- (lb);
		
		%Draw the labels for each segment
		\node at ([shift={(-0.75cm,1cm)}]ep.center) {$H_{0}$};
		\node at ([shift={(0.9cm,-0.7cm)}]ep.center) {$H_{1}$};
	\end{tikzpicture}
\end{figure} If we specify the decision rule $T_{n}=T\rsampx$ then our sample space that lives in $\R_{n}$ will have a region $R_{0}\in\R^{n}$ for which if
\begin{equation*}
	\rsampx\in\R_{0}
\end{equation*}
then I take a decision and in particular I decide to reject $H_{0}$. Of course we can also find the complement $R_{0}^{c}$ for which I do not reject $H_{0}$. 
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[line width=1.5pt]
		%The empty ellipse node with minimum width 6cm and minimum height 4cm
		\node [ellipse,draw,minimum width=6cm, minimum height=4cm] (ep) at (0,0) {};
		
		%Draw the BÃ©zier curve and the horizontal line to make segments inside the ellipse
		\draw ([shift={(0.5\pgflinewidth,0.5\pgflinewidth)}]ep.-115)
		.. controls ++(-2,2.5) and ++ (0,0) 
		.. node (mp) [pos=0.485] {}  ([yshift=0\pgflinewidth]ep.32);
		
		%Draw label omega outside the ellipse
		\node (lb) [draw,circle] at ([shift={(0.5,0.5)}]ep.5) {$\R^{n}$};
		\draw (ep.5) -- (lb);
		
		%Draw the labels for each segment
		\node at ([shift={(-0.75cm,1cm)}]ep.center) {$R_{0}$};
		\node at ([shift={(0.9cm,-0.7cm)}]ep.center) {$R_{0}^{c}$};
	\end{tikzpicture}
\end{figure} 
So take $\rsampx$ from $X\sim f_{X}(\cdot;\theta)$ with $\theta\in\Theta$. We have our null and alternative hypothesis with our partitions at sample and parameter level. Let's see all the possible combinations. 
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[line width=1.5pt]
		%The empty ellipse node with minimum width 6cm and minimum height 4cm
		\node [ellipse,draw,minimum width=6cm, minimum height=4cm] (ep) at (5,0) {};
		
		%Draw the BÃ©zier curve and the horizontal line to make segments inside the ellipse
		\draw ([shift={(0.5\pgflinewidth,0.5\pgflinewidth)}]ep.215)
		.. controls ++(2,-0.5) and ++ (0,0) 
		.. node (mp) [pos=0.485] {}  ([yshift=0\pgflinewidth]ep.65);
		
		%Draw label omega outside the ellipse
		\node (lb) [draw,circle] at ([shift={(-0.5,0.5)}]ep.155) {$\Theta$};
		\draw (ep.155) -- (lb);
		
		%Draw the labels for each segment
		\node at ([shift={(-0.75cm,1cm)}]ep.center) {$\Theta_{0}$};
		\node at ([shift={(0.9cm,-0.7cm)}]ep.center) {$\Theta_{0}^{c}$};
		%The empty ellipse node with minimum width 6cm and minimum height 4cm
		\node [ellipse,draw,minimum width=6cm, minimum height=4cm] (ep) at (-3,2) {};
		
		%Draw the BÃ©zier curve and the horizontal line to make segments inside the ellipse
		\draw ([shift={(0.5\pgflinewidth,0.5\pgflinewidth)}]ep.-115)
		.. controls ++(-2,2.5) and ++ (0,0) 
		.. node (mp) [pos=0.485] {}  ([yshift=0\pgflinewidth]ep.32);
		
		%Draw label omega outside the ellipse
		\node (lb) [draw,circle] at ([shift={(0.5,0.5)}]ep.5) {$\R^{n}$};
		\draw (ep.5) -- (lb);
		
		%Draw the labels for each segment
		\node at ([shift={(-0.75cm,1cm)}]ep.center) {$\ulx\in R_{0}$};
		\node at ([shift={(0.9cm,-0.7cm)}]ep.center) {$\ulx\in R_{0}^{c}$};
		  \draw [SpringGreen3, thick] (6,-1) edge[Stealth-, bend left=30] (-5,1);
		  \draw [SpringGreen3, thick] (3,-0.5) edge[Stealth-, bend left=15] (-3,1);
		  \draw [red, thick] (5, 1.4) edge[Stealth-, bend right=30] (-4,3.5);
		  \draw [red, thick] (6, 0.3) edge[Stealth-, bend right=15] (-2,2);
	\end{tikzpicture}
\end{figure}  Just because the sample is in $R_{0}$ this is \textit{not} enough to say that $H_{0}$ is false: it is \textit{very likely} that it is false, but it's not a clear cut.
So whenever we take a decision we make a mistake. This is different than decision theory, where we only have one arrow for partition because in that context we do not have to take into account randomness. The problem here is to control the probability to make those mistakes. So we have different scenarios:
\begin{itemize}
	\item $C_{1}$: decision of \textcolor{SpringGreen3}{accepting} $H_{0}$ (based on $\ulX$) and $H_{0}$ is \textcolor{SpringGreen3}{true};
	\item $C_{2}$: decision of \textcolor{red}{rejecting} $H_{0}$ (based on $\ulX$) and $H_{0}$ is \textcolor{red}{false};
	\item $E_{1}$: decision of \textcolor{red}{rejecting} $H_{0}$ (based on $\ulX$) and $H_{0}$ is \textcolor{SpringGreen3}{true};
	\item $E_{2}$: decision of \textcolor{SpringGreen3}{accepting} $H_{0}$ (based on $\ulX$) and $H_{0}$ is \textcolor{red}{false}.
\end{itemize}
I now need to assign probability to these events, in particular to the errors. We have
\begin{equation*}
	\begin{larray}
		\alpha=\pr\left(\{E_{1}\}\right)=\pr\left(\ulX\in R_{0};H_{0}:\theta\in\Theta_{0}\right)\\
			\beta=\pr\left(\{E_{2}\}\right)=\pr\left(\ulX\in R_{0}^{c};H_{1}:\theta\notin\Theta_{0}\right)
	\end{larray}
\end{equation*}
and then I have the complements
\begin{equation*}
	\begin{larray}
		1-\alpha=\pr\left(\{C_{1}\}\right)=\pr\left(\ulX\in R_{0}^{c};H_{0}:\theta\in\Theta_{0}\right)\\
		1-\beta=\gamma=\pr\left(\{C_{2}\}\right)=\pr\left(\ulX\in R_{0};H_{1}:\theta\notin\Theta_{0}\right).
	\end{larray}
\end{equation*}
We call: \begin{itemize}
\item $\alpha$ the probability of type-I error or \emph{size of the test};
\item $\beta$ the probability of type-II error;
\item $\gamma$ the \emph{power of the test}.
\end{itemize}
\subsection{Defining a decision rule}
We have three possibilities:
\begin{itemize}
	\item fix $\alpha$ and look for a decision rule that minimizes $\beta$;
	\item fix $\beta$ and look for a decision rule that minimizes $\alpha$;
	\item find a function of $\alpha$ and $\beta$ (but that's very difficult and we won't use this).
\end{itemize}
We choose the first one because ``it's better not to put in jail an innocent rather''. $\alpha$ is more ``dangerous'' because how we specify $H_{0}$ as the hypothesis we really trust on. Therefore if we reject $H_{0}$ I want to be sure that it doesn't happen by mistake (for example because I took a bad sample).
\begin{exercise}
	Prove that if you reduce $\alpha$ then $\beta$ increases. 
\end{exercise}
To find a way to define rejection regions we need some terminology. Fix $\alpha\in(0,1)$ and find a decision rule that minimizes $\beta$ (or equivalently maximizes the power $\gamma$). This decision rule is called \emph{optimal decision rule of size $\alpha$}. 
\begin{definition}
	For $\alpha\in[0,1]$ a test with power function $\gamma(\theta)$ is a \emph{size $\alpha$} test if 
	\begin{equation*}
		\sup_{\theta\in\Theta_{0}}\gamma(\theta)=\alpha.
	\end{equation*}
\end{definition}
\begin{definition}
	For $\alpha\in[0,1]$ a test with power function $\gamma(\theta)$ is a \emph{level $\alpha$} test if
	\begin{equation*}
		\sup_{\theta\in\Theta_{0}}\gamma(\theta)\leq\alpha.
	\end{equation*}
\end{definition}
Of course the set of level $\alpha$ tests contains the set of size $\alpha$ tests. We usually use the levels because we cannot achieve the exact size $\alpha$.
\begin{remark}
	We said that 
	\begin{equation*}
		\begin{larray}
			\alpha=\pr\left(\ulX\in R_{0};H_{0}:\theta\in\Theta_{0}\right)\\
		\gamma=\pr\left(\ulX\in R_{0};H_{1}:\Theta\in\Theta^{c}_{0}\right).
		\end{larray}
	\end{equation*}
	Since these two probabilities are defined on the same event, the difference lies in the parametrization. When we write $\gamma(\theta)$ we mean that when $\theta\in\Theta$ we call it the \emph{power function}, otherwise it is the type II error.
\end{remark}
\begin{definition}
	Let $C$ be a class of tests for testing
	\begin{equation*}
		\begin{larray}
			H_{0}:\theta\in\Theta_{0}\\
			H_{1}:\theta\in\Theta_{0}^{c}.
		\end{larray}
	\end{equation*}
	A test in class $C$ with power function $\gamma(\theta)$ is \emph{uniformly most powerful} (UMP) class $C$ test if 
	\begin{equation*}
		\gamma(\theta)\geq\gamma'(\theta)
	\end{equation*}
	For every $\theta\in\Theta_{0}^{c}$ and every $\gamma'(\theta)$ that is the power function of a test in class $C$.
\end{definition}
For us $C$ will be the class of all level $\alpha$ tests. So we have the following goals:
\begin{itemize}
	\item find a UMP level $\alpha$ test (we will see that it is possible only if the two hypotheses are point-wise);
	\item find a UMP level $\alpha$ test with unidirectional hypotheses (and lose number of models on which I can use this test) with 
	\begin{equation*}
		H_{0}:\theta\geq\Theta_{0}
	\end{equation*}
	\item get even a more general approach (but lose optimality) with hypotheses of the kind 
		\begin{align*}
			H_{0}:\theta\in\Theta_{0}\\
			H_{1}:\theta\notin\Theta_{0}.
		\end{align*}
\end{itemize}
\begin{theorem}
	Consider the problem of $H_{0}:\theta=\theta_{0}$ versus $H_{1}:\theta=\theta_{1}$ where we denote by $f_{\ulX}(\ulx;\theta_{i})$ for $i=0,1$ the density function of the distribution of $\ulX$ under $H_{0}$ and $H_{1}$. Assume that
	\begin{equation*}
		\ulX\in R\quad\text{if }f_{\ulX}(\ulx;\theta_{1})>kf_{\ulX}(\ulx;\theta_{0})
	\end{equation*} 
	and
	\begin{equation*}
		\ulX\in R^{c}\quad\text{if }f_{\ulX}(\ulx;\theta_{1})<kf_{\ulX}(\ulx;\theta_{0})
	\end{equation*} 
	(\emph{shape condition of the rejection region}) for some $k\geq0$ and the constraint
	\begin{equation*}
		\alpha=\pr\left(\ulX\in R;\theta_{0}\right)
	\end{equation*}
	(\emph{size condition of the rejection region}). Then:
	\begin{enumerate}[\circnum]
		\item any test that satisfies shape and size conditions is UMP of level $\alpha$;
		\item if there exists a test that satisfies shape and size conditions with $k>0$ then every UMP level $\alpha$ test is a size $\alpha$ test and every UMP level $\alpha$ test satisfies the shape condition except on a set $A$ of probability 0.
	\end{enumerate}
\end{theorem}
\begin{fancyproof}
	Remember the difference between size and level $\alpha$ tests. We have that any test that satisfies the size condition is a size $\alpha$ test (by definition) and it is also a level $\alpha$ test because the hypotheses are point-wise (we take the supremum over $\Theta_{0}$ which is a single point):
	\begin{equation*}
		\sup_{\theta\in\Theta_{0}}\pr\left(x\in R;\theta\right)=\pr\left(X\in R;\theta_{0}\right)=\alpha.
	\end{equation*}
	Consider the test function (on the sample space) such that it takes value 1 if $\ulx\in R$ (that is, if the sample is in the rejection region) and value 0 if $\ulx\in R^{c}$. Denote this function by $\phi(\ulx)$. Now we assume that $\phi(\cdot)$ satisfies shape and size conditions and consider another test function $\phi'$ of any other test of level $\alpha$. Consider:
	\begin{itemize}
		\item $\gamma(\theta)$ the power function of $\phi(\cdot)$;
		\item $\gamma'(\theta)$ the power function of $\phi'(\cdot)$
	\end{itemize} 
	This functions $\phi$ and $\phi'$ can take values 0 or 1 then by the shape conditions if I consider the quantity
	\begin{equation*}
		\left[\phi(\ulx)-\phi'(\ulx)\right]\left(f_{\ulX}(\ulx;\theta_{1})-kf_{\ulX}(\ulx;\theta_{0})\right)\tag*{\faDumpsterFire}\label{dumpfire}
	\end{equation*}
	I notice that 
	\begin{equation*}
		\phi(\ulx)=1\implies f_{\ulX}(\ulx;\theta_{1})>kf_{\ulX}(\ulx;\theta_{0})
	\end{equation*}
	and
	\begin{equation*}
		\phi(\ulx)=0\implies\cdots
	\end{equation*}
	and this is important because we know that \ref{dumpfire} is a non negative quantity and therefore
	\begin{align*}
		0\leq&\int	\left[\phi(\ulx)-\phi'(\ulx)\right]\left(f_{\ulX}(\ulx;\theta_{1})-kf_{\ulX}(\ulx;\theta_{0})\right)\dif\ulx\\
		=&\int\phi(\ulx)f_{\ulX}(\ulx;\theta_{1})\dif\ulx-\int\phi'(\ulx)f_{\ulX}(\ulx,\theta_{1})\dif\ulx-\\
		&-k\left(\int\phi(\ulx)f_{\ulX}(\ulx;\theta_{0})\dif\ulx-\int\phi'(\ulx)f_{\ulX}(\ulx,\theta_{0})\dif\ulx\right)\\
		=&\gamma(\theta_{1})+\gamma'(\theta_{1})-l\left(\gamma(\theta_{1})-\gamma'(\theta_{0})\right)	\end{align*}
	Since $\phi'$ is a level $\alpha$ test then
	\begin{equation*}
		\gamma(\theta_{0})-\gamma'(\theta_{0})=\alpha-\gamma'(\theta_{0})\geq0
	\end{equation*}
	\begin{equation*}
		0\leq\gamma(\theta_{1})-\gamma'(\theta_{1})-k\left(\gamma(\theta_{0})-\gamma(\theta_{0})\right)
	\end{equation*}
	but we know that $\gamma'(\theta_{0})\leq\alpha$ and that $\gamma(\theta_{0})=\alpha$. Substituting we get
	\begin{equation*}
		0\leq\gamma(\theta_{1})-\gamma'(\theta_{1})-k(\alpha-\gamma'(\theta_{0}))
	\end{equation*}
	and since $\gamma'(\theta_{0})\leq\alpha$ we know that 
	\begin{equation*}
		\alpha-\gamma'(\theta_{0})\geq0.
	\end{equation*}
	Now let $\phi'$ be the thest function of an UMP level $\alpha$ test . $\phi$, a test that satisfies shape and size conditions is also UMP level $\alpha$. Let $\gamma$ and $\gamma'$ be the power functions associated to $\phi$ and $\phi'$. We have
	\begin{equation*}
		\gamma(\theta_{1})=\gamma'(\theta_{1}).
	\end{equation*}
	Recall the inequality
	\begin{equation*}
		0\leq\gamma(\theta_{1})-\gamma'(\theta_{1})-k\left(\gamma(\theta_{0})-\gamma'(\theta_{0})\right).
	\end{equation*}
	Since $k>0$
	\begin{equation*}
		\ubracketthin{\alpha-\gamma'(\theta_{0})=\gamma(\theta_{0})-\gamma'(\theta_{0})\leq 0}_{\alpha-\gamma'(\theta_{0})\leq0}.
	\end{equation*}
	Since $\phi'$ is of level $\alpha$ then $\gamma'(\theta_{0})\leq\alpha$. So
	\begin{equation*}
		\begin{carray}
			\alpha-\gamma'(\theta_{0})\leq0\\
			\Downarrow\\
			\gamma'(\theta_{0})\leq \alpha
		\end{carray}\implies\gamma'(\theta_{0})\alpha.
	\end{equation*}
	so $\phi'$ is a size $\alpha$ test.
\end{fancyproof}
\begin{lemma}
	\emph{Neyman-Pearson lemma}. The likelihood ratio provides an optimal test of level $\alpha$.
\end{lemma}
Let's try an example. I have $X_{1}\ldots X_{n}$ from $X\distnorm{\theta,\sigma^{2}}$ with $\theta$ unknown and $\sigma^{2}$ fixed. I have
\begin{equation*}
	\begin{larray}
		H_{0}:\theta=\theta_{0}\\
		H_{1}:\theta=\theta_{1}>\theta_{0}.
	\end{larray}
\end{equation*}
I must find a constant $c$ such that
\begin{equation*}
	\frac{\like\left(\theta_{1};\ulx\right)}{\like\left(\theta_{0};\ulx\right)}\geq c
\end{equation*}
to reject the hypothesis. We have that the ratio is 
\begin{align*}
	\begin{carray}
		\frac{\cancel{\left(\frac{1}{\sqrt{2\pi\sigma^{2}}}\right)^{n}}\expg{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\theta_{1})^{2}}}{\cancel{\left(\frac{1}{\sqrt{2\pi\sigma^{2}}}\right)^{n}}\expg{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\theta_{0})^{2}}}\geq0\\
		\Downarrow\\
		\expg{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\theta_{1})^{2}+\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\theta_{0})^{2}}\geq x\\
		\Downarrow\\
		\expg{-\frac{1}{2\sigma^{2}}\left[\sum_{i=1}^{n}x_{i}^{2}-2\theta_{1}\sum_{i=1}^{n}+n\theta_{i}^{2}-\sum_{i=1}^{n}x_{i}^{n}+2\theta_{0}\sum_{i=1}^{n}x_{i}-n\theta^{2}_{0}\right]}\\
		\Downarrow\\
		\expg{-\frac{1}{2\sigma^{2}}\left[n(\theta_{1}^{2}-\theta^{2}_{0})-2\sum_{i=1}^{n}x_{i}(\theta_{1}-\theta_{0})\right]}\geq c\\
		\Downarrow\\
		\expg{-\frac{1}{2\sigma^{2}}(\theta_{1}-\theta_{0})\left[(\theta_{0}+\theta_{1})n-2\sum_{i=1}^{n}x_{i}\right]}\geq c\\
		\Downarrow\\
		-\frac{1}{2\sigma^{2}}(\theta_{1}-\theta_{0})\left[(\theta_{0}+\theta_{1})n-2\sum_{i=1}^{n}x_{i}\right]\geq c\\
		\Downarrow\\
		\left(\theta_{0}+\theta_{1}\right)n-2\sum_{i=1}^{n}x_{i}\leq c\\
		\Downarrow\\
		-2\sum_{i=1}^{n}\leq0\implies\sum_{i=1}^{n}x_{i}\geq c\\
		\Downarrow\\
		\frac{1}{n}\sum_{i=1}^{n}x_{i}\geq c.
	\end{carray}
\end{align*}
So we reject if $\Xbar_{n}\geq x$. We know that
\begin{equation*}
	\Xbar_{n}\distnorm{\theta_{0};\frac{\sigma^{2}}{n}}
\end{equation*} 
and
\begin{align*}
	\alpha&=\pr\left(\Xbar_{n}\geq c\right)\\
	&=\pr\left(\frac{\Xbar_{n}-\theta_{0}}{\frac{\sigma}{\sqrt{n}}}\geq\frac{c-\theta_{0}}{\frac{\sigma}{\sqrt{n}}}\right).
	\end{align*}
	We know that
	\begin{equation*}
		\frac{\Xbar_{n}-\theta_{0}}{\frac{\sigma}{\sqrt{n}}}\distnorm{0,1}
	\end{equation*}
	so
	\begin{equation*}
		\alpha=\pr\left(Z\geq\frac{c-\theta_{0}}{\frac{\sigma}{\sqrt{n}}}\right).
	\end{equation*}
	But since $\alpha$ is fixed we have 
	\begin{equation*}
		\Phi\left(\frac{x-\theta_{0}}{\sfrac{\sigma}{\sqrt{n}}}\right)=1-\alpha.
	\end{equation*}
	This gives us 
	\begin{equation*}
		\frac{c-\theta_{0}}{\sfrac{\sigma}{\sqrt{n}}}=z_{\alpha}\implies c=\theta_{0}+z_{\alpha}\frac{\sigma}{\sqrt{n}}.
	\end{equation*}
	So our rejection region is
	\begin{equation*}
		\left\{\Xbar\geq\theta_{0}+z_{\alpha}\frac{\sigma}{\sqrt{n}}\right\}.
	\end{equation*}
\begin{remark}
	\begin{itemize}
		\item The rejection region is a function of the sufficient statistic (and we like this).
		\item This example was easy but look how we had to solve a quantile problem to find this constant $c$. This is not always easy (even with Gaussian we need the tables).
		\item We need to refer the problem to a distribution that is tabulated.
		\item By remark 3 we need asymptotic testing.
		\item Can we always solve the quantile problem perfectly? No, we only can do this in the continuous distribution (for each $\alpha$) where I can always find a point in the distribution for every value of $\alpha$.
	\end{itemize}
\end{remark}
Let's look to another example. Let $\rsampx$ from $X\distbernoulli{\theta}$. Our hypotheses are
\begin{equation*}
	\begin{larray}
		H_{0}:\theta=\theta_{0}\\
		H_{1}:\theta=\theta_{1}\geq\theta_{0}.
	\end{larray}
\end{equation*}
We have
\begin{align*}
	\frac{\like(\theta_{0};\ulx)}{\like(\theta_{1};\ulx)}\geq c&\iff\frac{\theta_{0}^{\sum_{i=1}^{n}x_{i}}(1-\theta_{0})^{n-\sum_{i=1}^{n}x_{i}}}{\theta_{1}^{\sum_{i=1}^{n}x_{i}}(1-\theta_{1})^{n-\sum_{i=1}^{n}x_{i}}}\leq c\\
	&\iff\left(\sum_{i=1}^{n}x_{i}\right)\log\frac{\theta_{0}}{\theta_{1}}+\left(n-\sum_{i=1}^{n}x_{i}\right)\log\frac{1-\theta_{0}}{1-\theta_{1}}\leq c\\
	&\iff\left(\sum_{i=1}^{n}x_{i}\right)\log\frac{\theta_{0}}{\theta_{1}}-\left(\sum_{i=1}^{n}x_{i}\right)\log\frac{1-\theta_{0}}{1-\theta_{1}}+m\log\frac{1-\theta_{0}}{1-\theta_{1}}\leq c\\
	&=\iff\left(\sum_{i=1}^{n}x_{i}\right)\ubracketthin{\bigg[\ubracketthin{\log\frac{\theta_{0}}{\theta_{1}}}_{<0}-\ubracketthin{\log\frac{1-\theta_{0}}{1-\theta_{1}}}_{>0}\bigg]}_{<0}+n\log\frac{1-\theta_{0}}{1-\theta_{1}}\leq c
\end{align*}
and so we get the shape condition
\begin{equation*}
	\sum_{i=1}^{n}x_{n}\geq\widetilde{c}.
\end{equation*}
We are now interested in finding $c$ such that
\begin{equation*}
	\alpha=\pr\left(\left\{\sum_{i=1}^{n}x_{i}\geq c;\theta_{0}\right\}\right).
\end{equation*}
Define the statistic under the null hypothesis
\begin{equation*}
	T:=\sum_{i=1}^{n}X_{i}\sim\mathsf{Binom}(n,\theta_{0}).
\end{equation*}
In general $\not\exists c\in\left\{0,1,\ldots,n\right\}$ such that $\pr\left(T\geq c;\theta_{0}\right)=\alpha$. We need a lucky combination for that, so we usually find
\begin{equation*}
	\min\left\{c>0:\pr\left(T\geq c;\theta_{0}\right)\leq\alpha\right\}.
\end{equation*}
So we have just seen that as a consequence of the Neyman-Pearson lemma the Gaussian and Bernoulli distributions have a test that is a function of the sufficient statistic for $\theta$. We must remember that the Gaussian and the Bernoulli model are part of the exponential family whose density function is in the form
\begin{equation*}
	p_{X}(x;\theta)=\expg{A(x)Q(\theta)+c(x)-k(\theta)}.
\end{equation*}
We compute the likelihood ratio under 
\begin{equation*}
	\frac{\like\left(\theta_{1};\ulx\right)}{\like\left(\theta_{0};\ulx\right)}\geq c
\end{equation*}
and we get
\begin{align*}
	\frac{\like(\theta_{0};\ulx)}{\like(\theta_{1};\ulx)}&=\frac{\prod_{i=1}^{n}\expg{A(x)Q(\theta_{0})+c(x)-k(\theta_{0})}}{\prod_{i=1}^{n}\expg{A(x)Q(\theta_{1})+c(x)-k(\theta_{1})}}\\
	&=\expg{Q(\theta_{0})\sum_{i=1}^{n}A(x_{i})-nk(\theta_{0}-Q(\theta_{1})\sum_{i=1}^{n}A(x_{i})+nk(\theta_{1})}\leq k\\
	&\iff\expg{\sum_{i=1}^{n}A(x_{i})\left[Q(\theta_{0})-Q(\theta_{1})\right]-n\left[k(\theta_{0})-k(\theta_{i})\right]}\leq k\\
	&\iff\left[Q(\theta_{0})-Q(\theta_{1})\right]\sum_{i=1}^{n}A(x_{i})-n\left[k(\theta_{0})-k(\theta_{1})\right]\leq k\\
	&\iff\sum_{i=1}^{n}A(x_{i})\leq c.
\end{align*}
So from this we see that the shape of the rejection region is a function of $\sum_{i=1}^{n}A(x_{i})$ which is a sufficient statistic. This is a more general result that allows us to extend the Neyman-Pearson lemma.
\begin{corollary}
	Consider the same setting of the Neyman-Pearson lemma. Suppose that $T(X_{1},\ldots,X_{n})$ is a sufficient statistic for the parameter $\theta$. Denote by $g(t;\theta_{i})$ as the density/mass function of the distribution of the sufficient statistic $T$ under $H_{i}$ for $i=0,1$. Then any test based on $T$ with rejection region $S$ is UMP level $\alpha$ if it satisfies
	\begin{equation*}
		t\in S\quad\text{if}\quad g(t;\theta_{1})>kg(t;\theta_{0})
	\end{equation*}
	and \begin{equation*}
		t\in S^{c}\quad\text{if}\quad g(t;\theta_{1})<kg(t;\theta_{0})
	\end{equation*}
	for some $k>0$, where
	\begin{equation*}
		\alpha=\pr\left(T\in S;\theta_{0}\right).
	\end{equation*}
\end{corollary}
So when we have a situation like 
\begin{equation*}
	H_{1}:\theta=\theta_{0}\qquad\text{vs}\qquad H_{1}:\theta=\theta_{1}
\end{equation*}
I can use this result, but something different... it's not so easy.
\subsection{Rubin Test}
In the Neyman-Pearson lemma there were no assumptions. In the Rubin test we find a way to find level $\alpha$ UMP test even for non point-wise hypotheses, but the price we will pay is that we cannot use it with all kinds of models.
\begin{definition}
	A family of density functions or mass function $\left\{g(t,\theta);\theta\in\Theta\right\}$ for a univariate random variable $T$ has \emph{monotone likelihood ratio} if for every $\theta_{2}>\theta_{1}$ we have
	\begin{equation*}
		\frac{g(t;\theta_{2})}{g(t;\theta_{1})}
	\end{equation*}
	is a monotone (non-increasing or non-decreasing) function of $t$ on
	\begin{equation*}
		\begin{larray}
			\left\{t:g(t;\theta_{1})>0\right\}\\
			\left\{t:g(t;\theta_{0})>0\right\}.
		\end{larray}
	\end{equation*}
\end{definition}
\begin{theorem}
	Consider the problem of testing
	\begin{equation*}
		H_{0}:\theta:\theta_{0}\qquad\text{vs}\qquad H_{1}:\theta>\theta_{0}.
	\end{equation*}
	Assume that $T$ is a sufficient statistic for $\theta$ and the density (or mass) function of $T$ has the MLR property. Then the test that reject $H_{0}$ if and only if $T>t_{0}$ (for any $t_{0}$) is UMP level $\alpha$ where
	\begin{equation*}
		\alpha=\pr\left(T>t_{0};\theta_{0}\right).
	\end{equation*}
\end{theorem}
The proof is just applying over and over the NP lemma. We are not interested in getting the general result.
\begin{proposition}
	\emph{Likelihood ratio test}. Let $\rsampx$ be a random sample from $X\sim F_{\theta}$ with $\theta\in\Theta$. Consider the hypotheses
	\begin{equation*}
		H_{0}:\theta\in\Theta_{0}\subseteq\Theta\qquad\text{vs}\qquad H_{1}:\theta\in\Theta^{c}_{0}.
	\end{equation*}
	We define the LRT statistic as 
	\begin{equation*}
		\lambda(X_{1},\ldots,X_{n})=\frac{\sup_{\theta\in\Theta_{0}}\like(\theta;\ulx)}{\sup_{\theta\in\Theta}\like(\theta;\ulx)}.
	\end{equation*}
	The LRT is any test that has a rejection region of the form 
	\begin{equation*}
		\{\rsampx:\lambda\rsampx\leq c\}\every c\in[0,1].
	\end{equation*}
	for any $c\in[0,1]$. We determine so point as a quantile problem:
	\begin{equation*}
		\pr\left(\lambda\rsampx\leq c\right)\leq\alpha
	\end{equation*}
\end{proposition}
The problem, as anticipated, is that we lose optimality? Why? Take $\rsampx$ form $x\distnorm{(0
	\theta,\sigma^{2})}$ where $(\theta,\sigma^{2})$ is unknown.
	\begin{equation*}
		H_{0}:\theta=\theta_{0}\qquad\text{vs}\qquad H_{1}:\theta\neq 1.
	\end{equation*}
	The numerator of the LRT is
	\begin{align*}
		\sup_{\theta,\sigma^{2}\in\Theta}\like\left(\theta,\sigma^{2},\ulx\right)=\like\left(\ulX_{n},\tilde{s}^{2}_{n}\right)&=\left(\frac{1}{\sqrt{2\pi\widetilde{s}^{2}_{n}}}\right)^{n}\expg{-\frac{n}{2\cancel{\widetilde{s}^{2}_{n}}}\cancel{\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\theta_{i})^{2}}}\\
		&=\left(\frac{1}{\sqrt{2\pi\widetilde{s}^{2}_{n}}}\right)^{n}\expg{-\frac{n}{2}}\propto\left(\widetilde{s}^{2}_{n}\right)^{-\frac{n}{2}}\\
		&\propto\left(\theta_{0},\widetilde{s}^{2}_{0}\right)
	\end{align*}
	We set
	\begin{equation*}
		\widehat{\theta}_{n}=\theta_{0}\qquad \tilde{s}^{2}_{0}=\ubracketthin{\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\theta_{0})^{2}}_{\claptext{MLE of $\sigma^{2}$ in $\Theta_{0}$}}.
	\end{equation*}
	Again, repeating the computation for the denominator we get
	\begin{equation*}
		\like\left(\theta_{0},\tilde{s}^{2}_{0}\right)\propto\left(\tilde{s}^{2}_{0}\right)^{-\frac{n}{2}}
	\end{equation*}
	so our problem is 
	\begin{equation*}
		\lambda\left(X_{1},\ldots,X_{n}\right)=\left(\frac{\widetilde{s}^{2}_{n}}{\widetilde{s}^{2}_{0}}\right)^{\frac{n}{2}}\leq c
	\end{equation*}
	and we want to find the distribution of $\lambda\rsampx$. Write
	\begin{align*}
		\tilde{s}^{2}_{0}&=\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\theta_{0}\right)^{2}\\
		&=\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\Xbar_{n}+\Xbar_{n}-\theta_{0}\right)^{2}\\
		&=\tilde{s}^{2}_{n}+\left(\Xbar_{n}-\theta_{0}\right)^{2}.
	\end{align*}
	The ratio becomes
	\begin{align*}
		\lambda\rsampx&=\left(\frac{\tilde{s}^{2}_{n}}{\tilde{s}^{2}_{n}+\left(\Xbar_{n}-\theta^{0}\right)^{2}}\right)^{\frac{n}{2}}\leq c\\
		&\implies \left(\frac{\tilde{s}^{2}_{n}+\left(\Xbar_{n}-\theta^{0}\right)^{2}}{\tilde{s}^{2}_{n}}\right)^{-\frac{n}{2}}\leq c\\
		&\implies \frac{\left(\Xbar_{n}-\theta_{0}\right)^{2}}{den}\\
		&\implies\frac{\left(\Xbar_{n}-\theta_{0}\right)^{2}}{\frac{n-1}{n}s^{2}_{n}}\geq c\\
		&\implies\frac{\sqrt{n}\left|\Xbar_{n}-\theta_{0}\right|}{s_{n}}\geq c\\
		&\implies\left|T_{n}\right|>c
	\end{align*}
	with $T_{n}=\frac{\sqrt{n}(\Xbar_{n}-\theta_{0})}{s_{n}}$ so
	\begin{equation*}
		T_{n}=\frac{\frac{\Xbar_{n}-\theta_{0}}{\frac{\sigma}{\sqrt{n}}}}{\sqrt{\frac{s^{2}_{n}}{\sigma^{2}}}}=\frac{Z}{\sqrt{\frac{Y}{n-1}}}
	\end{equation*}
	but this is the definition of the \emph{T-student distribution}:
	\begin{equation*}
		T_{n}\sim t_{n-1}
	\end{equation*}
	and we have
	\begin{equation*}
		\alpha=\pr\left(\left|T_{n}\right|>c;\theta_{0}\right)
	\end{equation*}
	and
	\begin{equation*}
		(1-\alpha)=\pr\left(-t_{\sfrac{\alpha}{2}}\leq t_{n-1}\leq t_{\sfrac{\alpha}{2}}\right).
	\end{equation*}
	Testing the mean of a gaussian model when the variance is unknown implies using the t-test. Take $\rsampx$ from $X\distnorm{\theta_{x};\sigma^{2}}$ and $\rsampy$ from $Y\distnorm{\theta_{y}l\sigma^{2}}$. I want to test whether the two groups are the same. Typically this is called homogeneity test and this gives
	\begin{equation*}
		H_{0}:\theta_{x}-\theta_{y}=0\\
		H_{1}:\theta_{x}-\theta_{y}\neq 0.
	\end{equation*}
	In this case $\sigma^{2}$ is unknown and $(X_{1},\ldots,X_{n})$ is independent of $(Y_{1},\ldots,Y_{m})$. We write
	\begin{align*}
		\lambda(X_{1},\ldots,X_{n},Y_{1},\ldots,Y_{m})&=\frac{\sup_{\theta_{x}=\theta_{y},\sigma^{2}}\like\left(\overline{\underline{X}\underline{Y}}\right)}{\sup_{\theta_{x},\theta_{y},\sigma^{2}}\like(\overline{\underline{X},\underline{Y}})}\sim t_{n-1\cdot m-1}.
	\end{align*}
	One more example:
Take $\rsampx$ from $X\distnorm{\theta_{x},\sigma^{2}_{x}}$ and $\rsampy$ from $Y\distnorm{\theta_{y},\sigma^{2}_{y}}$ with $\rsampx\indep\rsampy$. Here we want to test the equality of the variances:
\begin{equation*}
	H_{0}:\sigma^{2}_{x}=\sigma^{2}_{y}\qquad H_{1}:\sigma^{2}_{x}\neq\sigma^{2}_{y}.
\end{equation*}
Again, we do the likelihood ratio and get
\begin{equation*}
	\lambda\left(X_{1},\ldots,X_{n},Y_{1},\ldots,Y_{m}\right)\sim \mathsf{F}_{n-1,m-1}.
\end{equation*}
So, to recap: the LRT works by taking
\begin{equation*}
	\lambda\left(\ulx\right)=\frac{\sup_{\theta\in\Theta_{0}}\like(\theta;\ulx)}{\sup_{\theta\in\Theta}\like(\theta;\ulx)}<c
\end{equation*}
which defines a rejection region
\begin{equation*}
	\left\{\ulX:\lambda(\ulx)\leq0\right\}.
\end{equation*}
\begin{remark}
	If I assume gaussianity ($X\distnorm{\mu,\sigma^{2}}$) then we can find the distribution of $\lambda\left(x\right)$ which is the $T$-student distribution. The same distribution applies for the equality of means while for the equality of variances we need to use the $F$-test distribution.\par
	This is the main advantage of staying in the Gaussian framework.
\end{remark}
\subsection{Asymptotic testing}
This is very important because we technically need the distribution of $\lambda$ to get the quantile $c:\pr\left(\lambda\left(\ulx\right)\leq c\right)\leq\alpha$. Not only that, we need the distribution of $\lambda$ to be tabulated (for example, the Gamma distribution is not) or use numerical methods. For all the other cases, we need to go asymptotic.
\begin{theorem}
	\emph{Wilks' theorem (Asymptotic for LRT)}. Let $\ulX=\rsampx$ from a regular model (since this theorem is based on the Cramer theorem) with $X\sim F_{\theta}$ with $\theta\in\Theta$. Consider testing 
	\begin{equation*}
		H_{0}: \theta=\theta_{0}\qquad\text{vs}\qquad H_{1}:\theta\neq\theta_{0}.
	\end{equation*}
	Und $H_{0}$, as $n\to\infty$ we have that 
	\begin{equation*}
		2\log\lambda\left(\ulX\right)\convw C\qquad C\sim\chi^{2}_{n}.
	\end{equation*}
\end{theorem}
This is good because the $\chi^{2}$ distribution is tabulated!
\begin{fancyproof}
	We have 
	\begin{equation*}
		\ell(\theta;\ulx)=\log\like(\theta;\ulx)
	\end{equation*}
	and $\widehat{\theta}_{n}$ is the MLE for $\theta$. We expand the log likelihood around $\widehat{\theta}_{n}$:
	\begin{align*}
		\ell(\theta;\ulx)=&\ell\left(\widehat{\theta}_{n};\ulx\right)+\ell'\left(\widehat{\theta}_{n};\ulx\right)\left(\theta-\widehat{\theta}_{n}\right)+\ell''\left(\widehat{\theta}_{n};\ulx\right)\left(\theta-\widehat{\theta}_{n}\right)^{2}\unmezz+\ldots\\
		-2\log\lambda\left(\ulx\right)=&-2\ell\left(\theta_{0};\ulx\right)+2\ell\left(\widehat{\theta}_{n};\ulx\right)\\
		=&-\cancel{2}\left[\cancel{\ell\left(\widehat{\theta}_{n};\ulx\right)}+\ell'\left(\widehat{\theta}_{n};\ulx\right)\left(\theta_{0}-\widehat{\theta}_{n}\right)+\ell''\left(\widehat{\theta}_{n};\ulx\right)\left(\theta_{0}-\widehat{\theta}_{n}\right)^{2}\unmezz\right]+\\
		&+2\left[\cancel{\ell\left(\widehat{\theta}_{n};\ulx\right)}+\cancel{\ell'\left(\widehat{\theta}_{n};\ulx\right)\left(\theta-\widehat{\theta}_{n}\right)}+\cancel{\ell''\left(\widehat{\theta}_{n};\ulx\right)\left(\theta-\widehat{\theta}_{n}\right)^{2}}\unmezz\right]\\
		=&-\ell''\left(\widehat{\theta}_{n},\ulx\right)\left(\theta_{0}-\widehat{\theta}_{n}\right)^{2}\\
		=&-\frac{1}{n}\ell''\left(\widehat{\theta}_{n};\ulx\right)\left(\sqrt{n}\left(\theta_{0}-\widehat{\theta}_{n}\right)\right)^{2}
	\end{align*}
	but by LLN we know that
	\begin{equation*}
		-\frac{1}{n}\ell''(\theta_{0},\ulx)\convpr I(\theta_{0})\qquad n\to\infty
	\end{equation*}
	and by Cramer's theorem we know that
	\begin{equation*}
		\sqrt{n}\left(\theta_{0}-\widehat{\theta}_{n}\right)\convw H\distnorm{0,\frac{1}{I(\theta_{0})}}
	\end{equation*}
	so combining those result we get
	\begin{equation*}
		-2\log\lambda\left(\ulx\right)\convw\left(\sqrt{I(\theta_{0})}H\right)^{2}\sim\chi^{2}_{1}\qquad\text{as }n\to\infty
	\end{equation*}
	since we get the square of a $\mathsf{N}(0,1)$ \rv.
\end{fancyproof}
For example, let's try to see what happens with the Poisson distribution. Take $\rsampx$ from $X\sim\mathsf{Pois}(\lambda)$ with $\lambda>0$. We want to test
\begin{equation*}
	H_{0}: \lambda=\lambda_{0}\qquad\text{vs}\qquad H_{1}:\lambda\neq\lambda_{0}.
\end{equation*}
We know that the MLE is $\widehat{\lambda}_{n}=n^{-1}\sum_{i=1}^{n}X_{i}$. We get
\begin{align*}
	-2\log\lambda\left(\ulx\right)&=-2\log\frac{e^{-2\lambda_{0}}\lambda_{0}^{\sum_{i=1}^{n}x_{i}}\cancel{\prod_{i=1}^{n}\frac{1}{x_{i}!}}}{e^{-n\widehat{\lambda}_{n}}\widehat{\lambda}_{n}^{\sum_{i=1}^{n}x_{i}}\cancel{\prod_{i=1}^{n}\frac{1}{x_{i}!}}}\\
	&=2n\left[\left(\lambda_{0}-\widehat{\lambda}_{n}\right)-\widehat{\lambda}_{n}\log\left(\frac{\lambda_{0}}{\widehat{\lambda}_{n}}\right)\right].
\end{align*}
This is a strange distribution. Even if I found it analytically for sure there is no tabulation for this. And this was not some obscure Russian model, it was just a Poisson model. But we can use Wilks' theorem:
\begin{equation*}
	-2\log\lambda\left(\ulx\right)\convw C\sim\chi^{2}_{1}\qquad\text{as }n\to\infty.
\end{equation*}
So our rejection reason is
\begin{equation*}
	R=\left\{-2\log\lambda\left(\ulx\right)>\chi^{2}_{1,\alpha}\right\}
\end{equation*}
which is the quantile of order $\alpha$ of $\chi^{2}_{1}$. This means that
\begin{equation*}
	\lim_{n\to\infty} \pr\left(\ulx\in\R,H_{0}\right)\to\alpha.
\end{equation*}
The need for regularity, however, is the price to pay: look what happens when we force Wilks' theorem on a non regular model. Take a random sample $\rsampx$ from $X\sim f_{X}(x;\theta)=\expg{-(x-\theta)}\indi_{\left(\theta,+\infty\right)}(x)$: this is like taking $Y\sim\mathsf{NegExp}(1)$ and $X=Y+\theta$. We have $T_{n}=\min\left\{X_{1}\right\}$ as the MLE. We want to test
\begin{equation*}
	H_{0}: \theta=\theta_{0}\qquad\text{vs}\qquad H_{1}:\theta\neq\theta_{0}.
\end{equation*}  We have eerie questions\footnote{Someone rightfully asked where $\indi_{(0,+\infty)}$ went. Favaro replied that we don't need it anyway because we are part the process of optimization.} but we get
\begin{align*}
	\lambda\rsampx&=\frac{\expg{-\sum_{i=1}^{1}(X_{i}-\theta_{0})}}{\expg{-\sum_{i=1}^{1}\left(X_{i}-T_{n}\right)}}\\
	&=\expg{-n\left(T_{n}-\theta_{0}\right)}.
\end{align*}
At this point we want to both compute the distribution and apply Wilks' theorem. Start with
\begin{equation*}
	-2\log\lambda\rsampx=2n(T_{n}-\theta_{0}).
\end{equation*}
We know that $T_{n}=X_{(1)}$ and in particular that $T^{\star}_{n}=\min\left\{\left(X_{1}-\theta_{0}\right),\ldots,\left(X_{n}-\theta_{0}\right)\right\}$ and we want to compute the distribution of the minimum. Typically we must use the anti-cdf:
\begin{align*}
	\pr\left(X_{(i)}>x\right)&=\pr\left(\left\{X_{1}>x\right\}\cap\ldots\cap\left\{X_{n}>x\right\}\right)\\
	&=\pr\left(\left\{X_{1}-\theta_{0}>x\right\}\cap\ldots\cap\left\{X_{n}-\theta_{0}>x\right\}\right)\\
	&=\prod_{i=1}^{n}\expg{-x}\\
	&=\expg{-nx}
\end{align*}
and this tells us that 
\begin{equation*}
	T^{\star}_{n}\sim\mathsf{NegExp}(n).
\end{equation*}
Now that we have the distribution of $T^{\star}_{n}$ we need to compute the distribution of $H=2nT^{\star}_{n}$. We get
\begin{align*}
	\pr\left(H\leq h\right)&=\pr\left(2nT^{\star}_{n}\leq h\right)\\
	&=\pr\left(T^{\star}_{n}\leq\frac{h}{2n}\right)\\
	&=1-\expg{-\frac{h}{2}}.
\end{align*}
So  $H\sim\mathsf{NegExp}\left(\unmezz\right)$. But remember that negative exponential distributions and $\chi^{2}$ distributions are actually just Gamma distributions, so we can rewrite the distribution of $H$ as 
\begin{equation*}
	H=-2\log\lambda\rsampx\sim\chi^{2}_{2}.
\end{equation*}
But if we used Wilks' we would have gotten
\begin{equation*}
	-2\log\lambda\rsampx\sim\chi^{2}_{\mathcolor{Magenta3}{1}}
\end{equation*}
so the two results would have been in contradiction! This is why we cannot use Wilks' with non regular models.
\subsection{Asymptotic testing but we are uncomfortable}
\begin{theorem}
	\emph{Wilks' theorem (multidimensional)}. Take $\rsampx$ from a regular model $F_{\theta}$ with $\theta\in\Theta$. If $\theta\in\Theta_{0}$ then the statistic
	\begin{equation*}
		-2\log\lambda\left(\ulX\right)\convw C\sim\chi^{2}_{g}\qquad\text{as }n\to\infty.
	\end{equation*}
	The parameter $g$ (degrees of freedom) is the difference between the number of parameters specified by $\Theta_{0}$ and the number of parameters specified in $\Theta$.
\end{theorem}
Take for example the roll of a dice. We have $\theta=\left(p_{1},p_2,p_3,p_4,p_5\right)$. We have in general $p_{j}=\pr\left(X_{i}=j\right)$ for $j=1,\ldots,5$. Take the number of successes
\begin{equation*}
	Y_{j}=\sum_{i=1}^{n}\indi\left(X_{i}=j\right)\qquad j=1,\ldots,5.
\end{equation*}
This is simply the multinomial model where
\begin{equation*}
	\pr\left(\left\{Y_{1}=y_{1}\right\}\cap\ldots\cap\left\{Y_{5}=y_{5}\right\}\right)=\prod_{i=1}^{5}p_{j}^{y_{i}}\frac{n!}{y_{1}!\ldots y_{5}!}
\end{equation*}and the likelihood is
\begin{align*}
	\pr\left((Y_{1},\ldots Y_{5})=(y_{1},\ldots,y_{5})\right)&=p_{1}^{y_{1}}\cdot\ldots \cdot p_{5}^{y_{5}}\frac{n!}{y_{1}!\ldots y_{5}!}
\end{align*}
where
\begin{equation*}
	\sum_{i=1}^{5}p_{j}=1\wedge\sum_{i=1}^{5}y_{j}=n.
\end{equation*}
Consider the problem
\begin{equation*}
	H_{0}: p_{1}=p_{2}=p_{3}\wedge p_{4}=p_{5}\qquad\text{vs}\qquad H_{1}:\text{$H_{0}$ is false}.
\end{equation*}
The dimension of $\Theta$ is 4 (because $p_{j}$ must sum up to 1 so one of them is "forced" and I can write it in terms of the other 4) and the dimension of $\Theta_{0}$ is 1 (see below). 
So we have to maximize
\begin{align*}
	\lambda\left(\uly\right)=\frac{\sup_{\Theta_{0}}\like(\theta;\uly)}{\sup_{\Theta}\like(\theta;\uly)}\qquad \theta=\left(p_{1},\ldots, p_{5}\right).
\end{align*}
It can be helpful to write the likelihood with the constraint baked in:
\begin{align*}
	\like\left(\theta,\uly\right)&\propto\left(\prod_{i=1}^{4}p_{j}^{y_{j}}\right)\left(1-\sum_{i=1}^{1}p_{j}\right)^{n-\sum_{i=1}^{4}y_{j}}\\
	\log\like\left(\theta,\uly\right)&=\sum_{i=1}^{4}y_{j}\log p_{j}+\left(n-\sum_{i=j}^{4}y_{j}\right)\log\left(1-\sum_{i=j}^{4}p_{j}\right)
\end{align*}
and taking the derivative yields
\begin{align*}
	\deriv{\theta}\log\like\left(\theta;\uly\right)=0\iff \ubracketthin{\widehat{p}_{j}}_{\claptext{MLE}}=\frac{y_{j}}{n}.
\end{align*}
But if I fix $p_{1}$ and then by consequence also $p_{2}$ and $p_{3}$. But then $p_{5}=p_{4}$ must be equal to $\frac{1-3p_{1}}{2}$, so I actually only have \ul{one} free parameter. So I can write all the other parameters in function of $p_{1}$:
\begin{align*}
	\like\left(\theta;\uly\right)&\propto p_{1}^{y_{1}+y_{2}+y_{3}}\left(\frac{1-3p_{1}}{2}\right)^{y_{4}+y_{5}}
\end{align*} 
so now just take the derivative with respect to $p_{1}$:
\begin{align*}
	\deriv{p_{1}}\log\like\left(\theta;\uly\right)&\implies \widehat{p}_{1,0}=\frac{y_{1}+y_{2}+y_{3}}{2n}=\widehat{p}_{2,0}=\widehat{p}_{3,0}\\
	&\implies\widehat{p}_{4,0}=\widehat{p}_{5,0}=\frac{1-3\widehat{p}_{1,0}}{2}.
\end{align*}
Now I can write the LRT as
\begin{align*}
	\lambda\left(\uly\right)&=\prod_{j=1}^{3}\left(\frac{y_{1}+y_{2}+y_{3}}{3y_{j}}\right)^{y_{j}}\prod_{l=1}^{5}\left(\frac{y_{4}+y_{5}}{2y_{l}}\right)^{y_{l}}.
\end{align*}
Finding the distribution of this shit is pretty much impossible. We know the single distribution of the single $y$ but finding the joint and then compute the quantile? Absolutely a horrible way to spend an afternoon. So we turn to asymptotic testing and apply Wilks' theorem:
\begin{equation*}
	-2\log\lambda\left(\ulx\right)\convw\chi^{2}_{3}\qquad n\to\infty.
\end{equation*}
But what about non regularity? The idea is that if I want to prove something like
\begin{equation*}
	\frac{T_{n}-\theta}{\sigma_{n}}\convw Z\distnorm{0,1}
\end{equation*}
so that we can compare the test $	\frac{T_{n}-\theta}{\sigma_{n}}$ with $Z$.
\begin{theorem}
	\emph{Wald test}. Let $\rsampx$ be a random sample from $F_{\theta}$, $\theta\in\Theta$. Let $T_{n}=T(X_{1},\ldots,X_{n})$ be an estimator for $\theta$ (for instance, the MLE). Denote
	\begin{equation*}
		\sigma^{2}=\var(T_{n}).
	\end{equation*}
	Le $s_{n}$ be an estimator of $\sigma_{n}$ such that
	\begin{equation*}
		\frac{s_{n}}{\sigma_{n}}\convpr 1\qquad\text{as }n\to\infty.
	\end{equation*}
	Then 
	\begin{equation*}
	\ubracketthin{	\frac{T_{n}-\theta}{s_{n}}}_{\claptext{\emph{Wald's statistic}}}\convw Z\distnorm{0,1}\qquad\text{as }n\to\infty.
	\end{equation*}
\end{theorem}
We can now define the shape of the rejection region and the power of the test. We want to test
\begin{equation*}
	H_{0}:\theta=\theta_{0}\qquad\text{vs}\qquad H_{1}:\theta\neq\theta_{0}.
\end{equation*}
We define \begin{equation*}
	Z_{n}=\frac{T_{n}-\theta_{0}}{s_{n}}.
\end{equation*}
We reject $H_{0}$ if 
\begin{equation*}
	Z_{n}<-z_{\sfrac{\alpha}{2}}\vee Z_{n}>z_{\sfrac{\alpha}{2}}.
\end{equation*}
So using the convergence we get
\begin{equation*}
	\pr\left(Z_{n}<-z_{\frac{\alpha}{2}}\vee Z_{n}>z_{\frac{\alpha}{2}}\right)\to\pr\left(Z<-z_{\frac{\alpha}{2}}\vee Z>z_{\frac{\alpha}{2}}\right)=\alpha.
\end{equation*}
For the power of the test (which, recall, is the probability of correctly staying in the rejection region) we need to take $\theta\neq\theta_{0}$ and see where it brings us:
\begin{align*}
	Z_{n}&=\frac{T_{n}-\theta_{0}}{s_{n}}\\
	&=\ubracketthin{\frac{T_{n}-\theta}{s_{n}}}_{\to\mathsf{N}(0,1)}+\frac{\theta-\theta_{0}}{s_{n}}=\begin{cases}
		\convpr+\infty\\
		\convpr-\infty
	\end{cases}
\end{align*}
Under $H_{1}$ (that is under $\theta\neq\theta_{0}$) then 
\begin{equation*}
	Z_{n}\convpr\pm\infty\qquad n\to\infty.
\end{equation*}
Then 
\begin{equation*}
	\pr\left(Z_{n}<-z_{\frac{\alpha}{2}}\vee Z_{n}>z_{\frac{\alpha}{2}};H_{1}\right)\to 1
\end{equation*}
which tells us that Wald test has asymptotic power = 1. \par
As an example, let's look at $\rsampx\distbernoulli{\theta}$. We already have an UMP test (the Rubin test) for this, but let's try Wald. I need an estimator for $\theta$ and I take the empirical mean:
\begin{equation*}
	\widehat{\theta}_{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}\qquad (\text{the MLE})
\end{equation*}
and I take the variance 
\begin{equation*}
	\var(T_{n})=\frac{\theta(1-\theta)}{n}.
\end{equation*}
The MLE for variance is 
\begin{equation*}
	\widehat{s}_{n}=\sqrt{n^{-1}\widehat{\theta}_{n}\left(1-\widehat{\theta}_{n}\right)}.
\end{equation*}
The \textit{actual} variance is 
\begin{equation*}
	\sigma_{n}=\sqrt{\var(T_{n})}.
\end{equation*}
To apply Wald's test we need to show that 
\begin{equation*}
	\frac{\sigma_{n}}{s_{n}}=\frac{\left[\theta(1-\theta)\right]^{\unmezz}}{\left[\widehat{\theta}_{n}\left(1-\widehat{\theta}_{n}\right)\right]^{\unmezz}}\convpr1\qquad\text{as }n\to\infty.
\end{equation*}
This is like checking
\begin{equation*}
	\widehat{\theta}_{n}\left(1-\widehat{\theta}_{n}\right)\convpr\theta(1-\theta).
\end{equation*}
This means that $\every\varepsilon>0$ we must have
\begin{align*}
	\pr\left(\left|\sqrt{\widehat{s}_{n}}-\sqrt{\theta(1-\theta)}\right|>\varepsilon\right)
	&=\pr\left(\left|\widehat{s}^{2}_{n}-\theta(1-\theta)\right|>\varepsilon\left(\widehat{s}_{n}+\sqrt{\theta(1-\theta)}\right)\right)\\
	&\leq\pr\left(\left|\widehat{s}^{2}_{n}-\theta(1-\theta)\right|>\varepsilon\sqrt{\theta(1-\theta)}\right)\\
	&=\pr\left(\left|\frac{\widehat{s}^{2}_{n}}{\theta(1-\theta)}-1\right|>\varepsilon\right)\\
	&=\pr\left(\frac{\theta(1-\theta)}{1+\varepsilon}\leq\widehat{s}^{2}_{n}\leq\frac{\theta(1-\theta)}{1-\varepsilon}\right)\\
	&=\pr\left(\theta(1-\theta)-\frac{2\theta(1-\theta)}{1+\varepsilon}\leq\widehat{s}^{2}_{n}\leq\theta(1-\theta)+\frac{\theta(1-\theta)}{1-\varepsilon}\right)
\end{align*}
But we know that
\begin{equation*}
	\theta(1-\theta)+\frac{\varepsilon(1-\theta)\theta}{1+\varepsilon}<\theta(1-\theta)-\frac{\theta(1-\theta)\varepsilon}{1-\varepsilon}
\end{equation*}
so 
\begin{align*}
	&\leq\pr\left(\theta(1-\theta)-\frac{\varepsilon\left(1-\theta\right)\theta}{1+\varepsilon}\leq\widehat{s}^{2}_{n}\leq\theta(1-\theta)+\frac{\theta(1-\theta)\varepsilon}{1-\varepsilon}\right)\\
	&=\pr\left(\left|\widetilde{s}^{2}_{n}-\theta(1-\theta)\right|\leq\frac{\theta(1-\theta)\varepsilon}{1+\varepsilon}\right)
\end{align*}
So 
\begin{equation*}
	Z_{n}=\frac{\widehat{\theta}_{n}-\theta_{0}}{\widehat{s}_{n}}\convw Z\sim\mathsf{N}(0,1)\qquad n\to\infty.
\end{equation*}
We didn't actually need all this: since for Bernoulli we know that the MLE $\convpr$ to the actual parameter and for the delta method then any plug-in estimator of a MLE also converges to its actual parameter.

\section{Some exercises}
So the total probability of correct classification is
\begin{equation*}
	C(R_{1},\ldots,R_{n})=\sum_{i=1}^{n}p_{i}\int_{R}f_{i}(x)\dx.
\end{equation*}
The goal is finding the classification rule $(R_{1},\ldots,R_{n})$ in such a way to maximize $C(R_{1},\ldots,R_{k})$. For $k=2$ this is just the normal Neyman-Pearson testing. We can define the regions as
\begin{equation*}
	R_{1}=\left\{x:\frac{f_{1}(x)}{f_{2}(x)}>\frac{p_{2}}{p_{1}}\right\}\qquad R_{2}=\left\{x:\frac{f_{1}(x)}{f_{2}(x)}<\frac{p_{1}}{p_{2}}\right\}.
\end{equation*}
The conditions are
\begin{equation*}
	\begin{larray}
		p_{1}+p_{2}=1\\
		\indi_{R_{1}}+\indi_{R_{2}}=1.
	\end{larray}
\end{equation*}
So we have 
\begin{align*}
	C(R_{1},R_{2})&=\int p_{1}f_{1}(x)\indi_{R_{1}}(x)\dx+\int p_{2}f_{2}(x)\indi_{R_{2}}(x)\dx\qquad\left(\pm\int p_{2}f_{2}(x)\indi_{R_{2}}(x)\dx\right)\\
	&=\int\left[\left(p_{1}f_{1}(x)-p_{2}f_{2}(x)\right)\indi_{R_{1}}(x)+p_{2}f_{2}(x)\right]\tag*{\faGasPump}\label{gasp}.\\
\end{align*}
Now take two other classification rules $R^{\star}_{1}$ and $R^{\star}_{2}$ being two disjoint sets of $\R$ with union equal to $\R$. For $k=\frac{p_{1}}{p_{2}}$:
\begin{equation*}
	\left(\indi_{R_{1}}(x)-\indi_{R^{\star}_{1}}(x)\right)\left(f_{1}(x)-f_{2}(x)k\right)\geq0 \every k.
\end{equation*}
Therefore
\begin{align*}
	&\int	\left(\indi_{R_{1}}(x)-\indi_{R^{\star}_{1}}(x)\right)\left(f_{1}(x)-f_{2}(x)k\right)\dx\geq0\\
	\iff&\int	\left(\indi_{R_{1}}(x)-\indi_{R^{\star}_{1}}(x)\right)\left(p_{1}f_{1}(x)-f_{2}(x)p_{1}\right)\dx\geq0\\
	\iff&\int\indi_{R_{1}}(x)\left[p_{1}f_{1}(x)-p_{2}f_{2}(x)\right]\dx\geq\int\indi_{R^{\star}_{1}}\left[p_{1}f_{1}(x)-p_{2}f_{2}(x)\right]\dx\\
	\iff&\int\left[\left(p_{1}f_{1}(x)-p_{2}f_{2}(x)\right)\indi_{R_{1}}(x)+p_{2}f_{(x)}\right]\geq\int\left[\left(p_{1}f_{1}(x)-p_{2}f_{2}(x)\right)\indi_{R^{\star}_{1}}(x)+p_{2}f_{(x)}\right]
\end{align*}
so 
we get that
\begin{equation*}
	C(R_{1},R_{2})\geq C\left(R^{\star}_{1},R^{\star}_{2}\right).
\end{equation*}
For example: solve the classification problem for gaussian population. For example, we have two populations and we want to classify a new individual as belonging to one or the other populations. We have in this case
\begin{equation*}
	f_{i}(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\expg{-\left(x-\mu_{i}\right)^{2}\frac{1}{2\sigma^{2}}}\qquad \begin{larray}
		i=1,2\\
		\mu_{1}\neq\mu_{2}.
	\end{larray}
\end{equation*}
Now consider $X_{i1},\ldots,X_{in_{i}}$ for $i=1,2$ being two independent random samples from $\mathsf{Unif}(0,\theta_{i})$ and $i=1,2$. Here $\theta_{i}>0$ is unknown. What can we do?
\begin{enumerate}
	\item The LRT test $H_{0}:\theta_{1}=\theta_{2}$ vs $H_{1}=\theta_{1}\neq\theta_{2}$. With the gaussian model we saw that this test would be distributed as a chi squared \rv.
	\item The asymptotic for the LRT $-2\log \lambda$, when $\frac{n_{1}}{n_{2}}\to k$ and $n_{1}\to\infty$ and $n_{2}\to\infty$ and $k>0$.
\end{enumerate}
In the general space the MLE for $\theta_{1}$ is:
\begin{equation*}\begin{carray}
	Y_{1}=\max\left\{X_{11},\ldots,X_{1n_{1}}\right\}\\
	Y_{2}=\max\left\{X_{21},\ldots,X_{2n_{2}}\right\}.
	\end{carray}
\end{equation*}
We have the likelihood funtion
\begin{equation*}
	\like\left(\theta_{1},\theta_{1};\ulx\right)=\theta_{1}^{-n_{1}}\theta_{2}^{-n_{2}}\indi_{(0,\theta_{1})}(Y)\indi_{(0,\theta_{2})}(Y_{2}).
\end{equation*}
in particolar $Y_{1}$ is the MLE of $\theta_{1}$ and $Y_{2}$ is the MLE of $\theta_{2}$. But under $H_{0}:\theta_{1}=\theta_{2}$ then the MLE will be
\begin{equation*}
	Y=\max\{Y_{1},Y_{2}\}.
\end{equation*}
Our likelihood then becomes
\begin{equation*}
	\like\left(\theta_{1},\ulx\right)=\theta_{1}^{-n_{1}-n_{2}}.
\end{equation*}
The LRT will be
\begin{align*}
	\lambda\left(\ulx\right)=\frac{Y_{1}^{n_{1}}Y_{2}^{n_{2}}}{Y^{n_{1}+n_{2}}}.
\end{align*}
The denominator is not nice, since it introduces some nasty dependence between the terms.
Of course Wilks does not apply; we could use Wald or, as we will do now, try to compute it exactly. Assume $\theta_{1}=\theta_{2}$. For some $t\in(0,1)$
\begin{align*}
	\pr\left(\lambda\left(\ulx\right)\leq t\right)&=\pr\left[\lambda(\ulx)\leq t, Y_{1}\leq Y_{2}\right]+\pr\left(\lambda\left(\ulx\right)\leq t, Y_{1}<Y_{2}\right)\\
	&=\pr\left(Y_{2}<t^{\frac{1}{n_{2}}}Y_{1},Y_{1}\geq Y_{2}\right)+\pr\left(Y_{1}<t^{\frac{1}{n_{1}}}, Y_{1}<Y_{2}\right)\\
	&=\pr\left(Y_{2}<t^{\frac{1}{n_{2}}}Y_{1}\right)+\pr\left(Y_{1}<t^{\frac{1}{n_{1
	}}}Y_{2}\right).
\end{align*}
If $U_{1},\ldots, U_{n}$ are i.i.id from $\mathsf{U}(0,\theta)$ then
\begin{equation*}
	\pr\left(U_{(n)}\leq n\right)=u^{n}
\end{equation*}
and its density function is 
\begin{equation*}
	f_{U_{(n)}}(u)=nu^{n-1}.
\end{equation*}
At this point we we do
\begin{align*}
	\pr\left(\lambda\left(\ulx\right)\leq t\right)=&n_{1}n_{2}\int_{0}^{1}\int_{0}^{t^{\frac{1}{n_{2}}}y_{2}}y_{2}^{n_{2}-1}y_{1}^{n_{1}-1}\dif y_{2}\dif y_{1}+\\
	&+n_{1}n_{2}\int_{0}^{1}\int_{0}^{t^{\frac{1}{n_{1}}y_{2}}}y_{1}^{n_{2}-1}y_{2}^{n_{1}-1}\dif y_{2}\dif y_{1}\\
	=&n_{1}t\int_{0}^{1}y_{1}^{n_{1}+n_{2}-1}\dif y_{1}+n_{2}t\int_{0}^{1}y_{2}^{n_{1}+n_{2}-1}\dif y_{2}\\
	=&(n_{1}+n_{2})t\int_{0}^{1}y^{n_{1}+n_{2}-1}\dy\\
	=&\frac{\left(n_{1}+n_{2}\right)t}{\left(n_{1}+n_{2}\right)}=t.
\end{align*}
So we have 
\begin{equation*}
	\alpha=\pr\left(\lambda\left(\ulx\right)<t\right)=\alpha\qquad t=\alpha.
\end{equation*}
This, however, is a rare case. We found that $\lambda\left(\ulx\right)$ is independent of $n_{1}$ and $n_{2}$ under $H_{1}.$ Consider $\theta_{1}<\theta_{2}$ (under $H_{1}$). If we want to use Wald's test we need to study the distribution of 
\begin{equation*}
	-2\log\lambda\left(\ulX\right)=-2\log\frac{Y_{1}^{n_{1}}}Y_{2}^{n_{2}}{Y^{n_{1}+n_{2}}}.
\end{equation*}
We have the same problem as before, we write
\begin{align*}
	\pr\left(Y_{1}>Y_{2}\right)=\pr\left(Y_{2}-Y_{1}-(\theta_{2}-\theta_{1})<-\left(\theta_{2}-\theta_{1}\right)\right).
\end{align*}
We know that
\begin{equation*}\begin{carray}
		Y_{1}=\max\left\{X_{11},\ldots,X_{1n_{1}}\right\}\\
		Y_{2}=\max\left\{X_{21},\ldots,X_{2n_{2}}\right\}
	\end{carray}
\end{equation*}
so
\begin{align*}
	\pr\left(Y_{1}\leq x\right)&=\left(\frac{x}{\theta_{1}}\right)^{n_{1}}\qquad\pr\left(Y_{2}\leq x\right)=\left(\frac{x}{\theta_{2}}\right)^{n_{2}}.
\end{align*}
The expected value is
\begin{equation*}
	\ev{Y_{i}}=\frac{n_{i}}{n_{i}+1}\theta_{i}
\end{equation*}
and the variance is
\begin{equation*}
	\var(Y_{i})=\theta_{1}^{2}\frac{n_{i}}{(n_{i}+2)(n_{i}+1)^{2}}
\end{equation*}
and as $n_{i}\to\infty$ we have 
\begin{equation*}
	\pr\left(Y_{1}>Y_{2}\right)\to0.
\end{equation*}
Now consider $Y_{1}\leq Y_{2}$:
\begin{equation*}
	-2\log\lambda\left(\ulX\right)=2n_{1}\left(\log Y_{2}-\log Y_{1}\right)\to \chi^{2}_{2}.
\end{equation*}
Now consider a population with 3 types:
\begin{equation*}
	\begin{rcases}
		\pr\left(\text{Type I}\right)&=\theta^{2}\\
		\pr\left(\text{Type II}\right)&=2\theta(1-\theta)\\
		\pr\left(\text{Type III}\right)&=(1-\theta)^{2}\\
	\end{rcases}\theta\in(0,1).
\end{equation*}
Take a sample of size $n$ from the population. Define as $N_{i}$ the number of individuals in the sample that belong to population $i$. The likelihood is
\begin{equation*}
	\like\left(\theta;n_{1},n_{2},n_{3}\right)=\frac{n!}{n_{1}!n_{2}!n_{3}!}\theta^{2n_{1}}\left(2\theta(1-\theta)\right)^{n_{2}}\left(1-\theta\right)^{2n_{3}}.
\end{equation*}
Now take the derivative
\begin{align*}
	\deriv{\theta}\log\like\left(\theta;n_{1},n_{2},n_{3}\right)&=\frac{2n_{1}+n_{2}}{\theta}-\frac{2n_{3}+n_{2}}{1-\theta}=0.
\end{align*}
The MLE is 
\begin{equation*}
	\widehat{\theta}=\frac{2n_{1}+n_{2}}{2n}
\end{equation*}
and this also gives us the sufficient statistic which is
\begin{equation*}
	2N_{i}+N_{2}.
\end{equation*}
Why are we saying this? We said that the sufficient statistic with monotonicity allows us to use Rubin's test for
\begin{equation*}
	H_{1}:\theta\geq\theta_{0}\qquad\text{vs}\qquad H_{1}:\theta<\theta_{0}.
\end{equation*}
We have the sufficient statistic and we just need to prove monotonicity for the distribution of $T=2N_{1}+N_{2}$. For $\theta_{2}>\theta_{1}$ we have
\begin{align*}
	V_{\theta_{1},\theta_{2}}&=\frac{\theta_{2}^{2n_{1}}\left(2\theta_{2}(1-\theta_{3})\right)^{n_{2}}\left(2-\theta_{2}\right)^{2n_{3}}}{\theta_{1}^{2n_{1}}\left(2\theta_{1}\left(1-\theta_{1}\right)\right)^{n_{1}}\left(1-\theta_{1}\right)^{2n_{1}}}\\
	&=\left(\frac{\theta_{2}}{\theta_{1}}\right)^{2n_{1}+n_{2}}\left(2^{n_{2}-n_{1}}\right)\left(\frac{1-\theta_{2}}{1-\theta_{1}}\right)^{2n_{3}+n_{2}}\\
	&=\left(\frac{\theta_{2}}{\theta}\right)^{2n_{1}+n_{2}}\left(\frac{1-\theta_{2}}{1-\theta_{1}}\right)^{2n-\left(2n_{1}+n_{2}\right)}\\
	&=\ldots\\
\end{align*}
So we get
\begin{equation*}
\ubracketthin{	\left(\frac{\theta_{2}}{\theta_{1}}\right)^{t}}_{\text{monotone increasing}}\qquad\ubracketthin{	\left(\frac{1-\theta_{2}}{1-\theta_{1}}\right)^{2n-t}}_{\text{monotone increasing}}.
\end{equation*}
So by Ruin test $\{T=2N_{1}+N_{2}\geq c\}$ is UMP level $\alpha$.
\chapter{Linear Models}
\section{Introduction to linear models}
We call $\varepsilon$ the error component (noise).
We call $y$ a \emph{regression model}:
\begin{equation*}
	y=r\left(x_{1},\ldots,x_{p}\right)+\varepsilon.
\end{equation*}
The idea is that $y$ is the measurement that we observe (like $x_{1},\ldots,x_{p}$) while $\varepsilon$ is not observed and we want to use it to learn about the mapping part. We assume a special case of the function $r(\cdot)$. In particular, we assume $r(\cdot)$ to be a \emph{linear function} depending on a finite number of unknown parameters
\begin{equation*}
	\left\{\beta_{1},\ldots,\beta_{p}\right\}.
\end{equation*}
So the complete regression model is
\begin{equation*}
	y=\beta_{1}x_{1}+\ldots+\beta_{p}x_{p}+\varepsilon.
\end{equation*}
Typically the goal is making assumptions about the noise to learn something about the parameters. The modeling assumption is on the noise $\varepsilon$ and we have two main classes of assumptions:
\begin{itemize}
	\item \emph{second order assumptions};
	\item \emph{Gaussian assumption}.
\end{itemize}
The first assumption is something akin to the method of moments: imagine we have $\rsampx$ from $X\sim F_{\theta}$ and we know that $\ev{X}=\theta$. Using the method of moments we can say that $\widehat{\theta}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$. This of course has the advantage that we just need to know the moment (which is a much weaker assumption to using MLE, which requires knowing the distribution function). Second order assumptions, similarly, do not need any assumption about the distribution of the noise.
\subsection{Second order assumptions}
I assume that $\varepsilon$ is random and that $\left(y_{1},\ldots,y_{n}\right)$ are observations (responses) that are sampled from the random variables $\rsampy$ Such that
\begin{equation*}
	Y_{i}=\beta_{1}x_{i1}+\ldots+\beta_{p}+x_{ip}+\varepsilon\qquad i=1,\ldots,n
\end{equation*} 
where the first index identifies which response and the second identifies which feature. In matrix notation:
\begin{equation*}
	\mathbf{Y}=\mathbf{X}\beta+\varepsilon.
\end{equation*}
\begin{itemize}
	\item $Y=\rsampy^{\trsp}$ is the (random) vector with the $n$ components of the response and $\mathbf{X}=\left(x_{ij}\right)$ is the \textit{map matrix} (non random) called the \emph{design matrix} or \emph{regression matrix} containing all the values of $p$ explanatory variables (with $n\geq p$).
	\item $\varepsilon=\left(\varepsilon_{1},\ldots,\varepsilon_{n}\right)^{\trsp}$ is the (random) vector of the erratic components.
	\item $\beta=\left(\beta_{1},\ldots,\beta_{n}\right)^{\trsp}$ is the vector (non random) of explanatory variables.
\end{itemize}
Assume that:
\begin{equation*}
	\begin{carray}
		\ev{\varepsilon}=0\\
		\var(\varepsilon)=\sigma^{2}\mathbf{I}_{n}\qquad\text{with $\sigma^{2}$ unknown and positive}\\
		\mathbf{X}\qquad\text{matrix of non-random elements with full rank }p. 
	\end{carray}
\end{equation*}
We suppose that the \rv s are uncorrelated (which means $Y_{i}\indep Y_{j}$) but they do \textit{not} have the same distribution. Our goal is to learn the $p+1$ parameters
\begin{equation*}
	\beta_{1},\ldots,\beta_{p},\sigma^{2}.
\end{equation*}
Based on the second order assumptions
\begin{equation*}
	\mathbf{Y}=\mathbf{X}\beta+\varepsilon\tag*{\faModx}\label{modx}
\end{equation*}
we know that:
\begin{itemize}
	\item $\ev{\varepsilon}=\ev{X\beta+\varepsilon}=X\beta$;
	\item $\var\left(\mathbf{Y}\right)=\var\left(X\beta+\varepsilon\right)=\sigma^{2}\mathbf{I}_{n}$.
\end{itemize}
Under model \ref{modx} then $y=\left(y_{1},\ldots,y_{n}\right)^{\trsp}$ are the observations. A reasonable way is to think of minimizing the distance
\begin{equation*}
	\norm{y-\mu}.
\end{equation*}
Equivalently, we can minimize the square of the euclidean distance:
\begin{align*}
	Q(\beta)&=\norm{y-\mu}^{2}\\
	&=\left(y-\mu\right)^{\trsp}\left(y-\mu\right)\\
	&=\left(y-\mathbf{X}\beta\right)^{\trsp}\left(y-\mathbf{X}\beta\right).
\end{align*}
This is called the \emph{Least square approach}. Consider the vector $y$ with the elements $x_{1},\ldots,x_{p}\in\R^{n}$. When $\beta$ varies in $\R^{p}$ the quantity $\mathbf{X}\beta$ provides a parametric equation of the linear subspace of $\R^{n}$ generated by the columns of $\mathbf{X}$. We denote this subspace as $C(\mathbf{X})$ (of dimension $p$). To identify $\widehat{\beta}$ we minimize the function
\begin{equation*}
	Q(\beta)=\left(y-\mathbf{X}\beta\right)^{\trsp}\left(y-\mathbf{X}\beta\right).
\end{equation*}
We need to differentiate with respect to the column vector:
\begin{revise}
	Recall that if $x$ is a column vector of a matrix $\mathbf{A}$ then
	\begin{equation*}
		\deriv{x}\mathbf{A}x=\mathbf{A}^{\trsp}
	\end{equation*}
	and if $\mathbf{B}$ is $p\times p$ symmetric matrix
	\begin{equation*}
		\deriv{x}x^{\trsp}\mathbf{B}x=2\mathbf{B}x.
	\end{equation*}
\end{revise}
So in our case we have
\begin{align*}
	\deriv{\beta}Q(\beta)&=\deriv{\beta}\left(y^{\trsp}y-2y^{\trsp}\mathbf{X}\beta+\beta^{\trsp}\mathbf{X}^{\trsp}\mathbf{X}\beta\right)\\
	&=\deriv{\beta}\left(-2y^{\trsp}\mathbf{X}\beta\right)+\deriv{\beta}\left(\beta^{\trsp}\mathbf{X}^{\trsp}\mathbf{X}\beta\right)\\
	&=-2\left(y^{\trsp}\mathbf{X}\right)^{\trsp}+2\mathbf{X}^{\trsp}\mathbf{X}\beta\\
	&=-2\mathbf{X}^{\trsp}y+2\mathbf{X}^{\trsp}\mathbf{X}\beta\\
	&=2\left(\mathbf{X}^{\trsp}\mathbf{X}\beta-\mathbf{X}^{\trsp}-y\right)
\end{align*}
which implies
\begin{equation*}
	\mathbf{X}^{\trsp}\mathbf{X}\beta=\mathbf{X}^{\trsp}y
\end{equation*}
which is called the \emph{normal equation} whose solution is
\begin{equation*}
	\widehat{\beta}=\left(\mathbf{X}^{\trsp}\mathbf{X}\right)^{-1}\mathbf{X}^{\trsp}y.
\end{equation*}
\subsection{Estimates of the regression coefficients}
So we have
\begin{align*}
	\dderivm{\beta}Q(\beta)=2\mathbf{X}^{\trsp}\mathbf{X}
\end{align*}
which is positive definite, so I can estimate
\begin{equation*}
	\mu=\mathbf{X}\beta\implies\widehat{\mu}=\mathbf{X}\widehat{\beta}\qquad\text{as the \emph{fitted value}}
\end{equation*}
and
\begin{align*}
	\widehat{\mu}=&\mathbf{X}\widehat{\beta}\\&=\mathbf{X}\left(\mathbf{X}^{\trsp}\mathbf{X}\right)^{-1}\mathbf{X}y\\
	&=\mathbf{P}{y}
\end{align*}
where 
\begin{equation*}
	\mathbf{P}=\mathbf{X}\left(\mathbf{X}^{\trsp}\mathbf{X}\right)^{-1}\mathbf{X}^{\trsp}
\end{equation*}
is the \emph{projection matrix on the space $C(x)$}.
\input{tutorials}
\end{document}
%THIS IS TH\feqE DARK AGE OF LOVE  
