\chapter{Partial Differential Equations (PDE) [Professor Badiale]}
\section{Lecture 7}
%Badiale 4 weeks, then Toaldo again
\subsection{WHAT IS A PDE?}

A first answer is given by the following definition. 
% slide  1
\begin{DefBox}
    \begin{Def}
        A PDE is an equation involving the partial derivatives of an unknown function \( u: \Omega \to \mathbb{R} \), where \( \Omega \) is an open subset of \( \mathbb{R}^n, n \geq 2 \).
    \end{Def}
\end{DefBox}

This definition is correct but too abstract.\\
% slide  2
In the theory of PDEs, one does not study any arbitrary equation, but concentrates instead on those equations that naturally occur in various applications (physics and other natural sciences, engineering, economy), or in other mathematical theories.

More general equations are studied as generalizations of basic equations.

\textbf{Notations for Partial Derivatives}\\
\underline{Partial Derivatives of Order 1:}
\[
\frac{\partial u}{\partial x_i} = D_{i}u = \partial_{i}u = \partial_{x_i} u = u_{x_i} \quad \text{partial derivatives of order $1$} \quad (i= 1, \ldots, n)
\]

% slide  3

\underline{Partial Derivatives of Order 2: } 
\[
\frac{\partial^2 u}{\partial x_i \partial x_j} = D_{ij}u = D^2_{ij}u = \partial_{ij}u = u_{x_ix_j} \quad i,j = 1, \ldots, n
\]
Here $x = (x_1, \ldots, x_n) \in \mathbb{R}^n$
%\[
%S_{55} = 1 \quad (M)
%\]

%Here, \(* = (*, \to, *a)\) represents the general form.
\begin{DefBox}
    \begin{Def}
        The \textbf{Laplacian} is defined as 
        \[
\Delta u = \sum_{i = 1}^n \frac{\partial^2 u}{\partial x_i^2} =  \frac{\partial^2 u}{\partial x_1^2} + \frac{\partial^2 u}{\partial x_2^2} + \dots + \frac{\partial^2 u}{\partial x_n^2}
\]
where 
\begin{equation*}
    \frac{\partial^2 u}{\partial x_i^2} = \frac{\partial^2 u}{\partial x_i \partial x_i} = D_i^2
\end{equation*}
    \end{Def}
\end{DefBox}


% slide  4

\subsection{Examples of PDEs}
\begin{itemize}
    \item \textbf{Laplace Equation:} 
\[
\Delta u = 0
\]
    \item \textbf{Poisson Equation:} 
\[
\Delta u = f
\]
where \( f: \Omega \to \mathbb{R} \) is a given function.\\
The Laplace equation models many equilibrium states in physics, and the Poisson equation arises, for example, in electrostatics.

% slide 5
\item \textbf{The Heat Equation.}

Here, one coordinate \( t \) is distinguished as the "time" coordinate, while the remaining coordinates \( x_1, \dots, x_n \) represent "spatial" variables. 

Thus, the unknown function \( u = u(x, t) \) is a function of $n+1$ variables (both space and time).

We consider:
\begin{equation*}
    u: \Omega \times \mathbb{R}^+ \rightarrow \mathbb{R}
\end{equation*}
where \( \Omega \) is an open subset of \( \mathbb{R}^n \), and %the equation is defined on \( \Omega \times (0, T) \).
\begin{equation*}
    \mathbb{R}^+ = (0,+\infty) = \{t \in \mathbb{R}: t> 0\}
\end{equation*}
and the equation 
\[
\frac{\partial u}{\partial t} - \Delta u = 0
\]
where $\Delta u= \sum_{i = 1} ^ n\frac{\partial^2 u}{\partial x_i^2}$

% slide 6
The Heat Equation models heat diffusion (or temperature) over time and other diffusion processes.

In this course, we will deal with the Heat Equation and Laplace's Equation.
\end{itemize}
\begin{itemize}
    \item \textbf{Wave Equation}: 
    \begin{equation*}
        \partial^2_t u - \Delta u = 0
    \end{equation*}
    \item \textbf{Maxwell's Equations of Classical Electromagnetism}
    \item \textbf{Navier-Stokes Equations} for the motion of an incompressible fluid
    \item \textbf{Einstein's Field Equations of General Relativity}
    \item \textbf{Schrödinger Equation of Quantum Mechanics}
\end{itemize}

% slide 7

\subsection{Classification of PDEs}

\textbf{Linear/Nonlinear}

A PDE can be written as:
\[
L u = 0
\]
or
\[
L u = v
\]
where \( L: X \to Y \), and \( X, Y \) are suitable spaces of functions, $v \in Y, u \in X$.

\begin{DefBox}
    \begin{Def}
        The equation is \textbf{linear} if \( L \) is linear, that is:
\[
L(\alpha u + \beta w) = \alpha L u + \beta L w \quad \forall u, w \in X, \, \forall \alpha, \beta \in \mathbb{R}
\]
    \end{Def}
\end{DefBox}
This means that if $L$ is linear, than any linear combination of solutions is still a solution of the equation. \\
% slide 8
This result implies that the set of solutions of \( L u = 0 \) is a vector space.

\begin{DefBox}
    \begin{Def}
        The equation is \textbf{nonlinear} if \( L \) is not linear.
    \end{Def}
\end{DefBox}

\textbf{Examples of Linear Equations}
\begin{itemize}
    \item \( L u = \Delta u \) (Laplace Equation)
    \item \( L u = \frac{\partial u}{\partial t} - \Delta u \) (Heat Equation)
    \item \( L u = \frac{\partial^2 u}{\partial t^2} - \Delta u \) (Wave Equation)
\end{itemize}
% slide 9

\textbf{Examples of Nonlinear Equations}
\begin{itemize}
    \item \( L u = u_t - \text{div}(u^\gamma \nabla u) \) \hfill (Porous Medium Equation), \quad \(\gamma > 1 \)
    \item \( u_{xx} u_{yy} - (u_{xy})^2\) \hfill (Monge-Ampère Equation)
    \item \( L u = \text{div}(|\nabla u|^{p-2})\nabla u\) \hfill (p-Laplace Equation)
\end{itemize}

% slide 10
\begin{DefBox}
    \begin{Def}
        \textbf{Order of a PDE:} The order of a partial differential equation (PDE) is defined as the highest order of the derivatives appearing in the equation.
    \end{Def}
\end{DefBox}

The Laplace, Heat, and Wave equations are all linear second-order PDEs.
% slide 11
\subsection{What Results Do We Want to Achieve?}

Let us recall what happens in the study of systems of ordinary differential equations:
\[
\begin{cases}
    x'(t) &= f(t, x(t))\\
    x(t_0) &= x_0
\end{cases}
\]
which is a Cauchy problem.\\ 
To simplify, \( f: \mathbb{R} \times \mathbb{R}^n \to \mathbb{R}^n, x: I \rightarrow \mathbb{R}^n \quad  (I \subseteq \mathbb{R}) \).


% slide 12
Assuming suitable hypotheses on \( f \), it is possible to get:
\begin{itemize}
    \item \textbf{Existence of Solutions}
    \item \textbf{Uniqueness of Solutions}
    \item Some kind of "\textbf{Stability}" with respect to initial data \( x_0, t_0 \)
\end{itemize}


% slide 13


When studying a PDE, we want to obtain (if possible) similar results:

\begin{itemize}
    \item \textbf{Existence of Solutions}
    \item \textbf{Uniqueness of Solution}
    \item "\textbf{Stability}" with respect to data.
\end{itemize}

When we deal with the last two aspects of the list in the ODE context, we study a problem in which data appear in the initial conditions. So, when we talk about "uniqueness" and "stability", we mean uniqueness of the problem with respect to initial values and stability with respect to perturbation of the initial status. \begin{remark}
    
\begin{itemize}
    \item In nonlinear problems, multiple solutions easily appear.
    \item What are the data of the problem? This depends on the equation.
\end{itemize}
\end{remark}

% slide 14
\textbf{Notations:}

If $x = (x_1, \ldots, x_n) \in \mathbb{R}^n, |x| = (x_1^2 + \ldots + x_n^2)^\frac{1}{2}$. 

If \( A \subseteq \mathbb{R}^n\) is a measurable set, then \( |A|\) is its measure (Lebesgue measure).

\begin{DefBox}
    \begin{Def}
        If \( x_0 \in \mathbb{R}^n, r > 0 \), let
\[
B(x_0, r) = \{ y \in \mathbb{R}^n \mid |x_0 - y\| < r \} \text{ for some } x_0 \in X
\]
denote the \textbf{ball of radius \( r \) centered at \( x_0 \)}, excluding the border.
\begin{equation*}
    \Bar{B}(x_0, r) = \{y \in \mR^n \mid |x_0 - y| \leq r\}
\end{equation*}
will denote its \textbf{closure}, including the border. 
\begin{equation*}
    \delta B (x_0,r)= S(x_0,r) = \{y \in \mR^n \mid |x_0-y| = r\}
\end{equation*}
is the \textbf{border} of the ball. 
    \end{Def}
\end{DefBox}


Let \( B = B(0, 1) \) and \( |B| = w_n \). Then $\forall x_0 \in \mR^n, \forall r > 0$
\begin{equation*}
    |B(x_0,r)| = w_n r^n
\end{equation*}
and 
\begin{equation*}
    |\partial B(x_0,r)| = n w_n r^{n-1}
\end{equation*}

\begin{DefBox}
    \begin{Def}
        \begin{equation*}
    \begin{split}
        \mathcal{C}^0(\Omega) &= \{\text{continuous functions } u: \Omega \rightarrow \mR\}\\
        \mathcal{C}^k(\Omega) &= \{\text{functions } u: \Omega \rightarrow \mR \text{ with all derivatives up to order $k$, and all continuous}\} \quad (k \in \mathbb{N}, k \geq 1)\\
        \mathcal{C}_0^k(\Omega) &= \{ u \in \mathcal{C}^k(\Omega) \mid supp(u) \text{ is a compact set and } supp(u) \subseteq \Omega\}\\
        \mathcal{C}^\infty(\Omega) &= \{\text{functions } u: \Omega \rightarrow \mR \text{ with derivatives of any order, and all continuous}\}\\
        \mathcal{C}^\infty_0(\Omega) &= \{u \in \mathcal{C}^\infty(\Omega) \mid supp(u) \text{ is a compact set and } supp(u) \subseteq \Omega\}\\
    \end{split}
\end{equation*}
    \end{Def}
\end{DefBox}
% slide 15


% slide 16
\begin{ThBox}
    \begin{Th}
        \textbf{Integration by Parts (or "Divergence Theorem")}

Let \( \Omega \subseteq \mR^n \) be a bounded domain (open and connected) and $F:\Bar{\Omega} \rightarrow \mR^n$ a function, $F \in \mathcal{C}^1(\Bar{\Omega})$. \\
Then, the Divergence Formula holds:
\begin{equation}
\label{DF}
    \int_{\Omega} \nabla \cdot \mathbf{F} \, dx = \int_{\delta \Omega} \mathbf{F} \cdot \nu \, d\sigma_x
\end{equation}
where \( \mathbf{F} \) is a vector field, \( \nabla \cdot \mathbf{F} \) is the divergence of \( \mathbf{F} \), \( dV \) is the volume element in \( \Omega \), \( \delta\Omega \) is the boundary of \( \Omega \), and \( \mathbf{n} \) is the outward normal vector on the boundary.
    \end{Th}
\end{ThBox}
% slide 17


\textbf{Remarks:}

1. \( DIV F = \sum_{j= 1}^n D_j F_j \quad (F_1, F_2, \ldots, F_n) \)

2. \( F \in C^1(\Bar{\Omega}) \) means that \( F \in \mathcal{C}^0(\Bar{\Omega}) \) and all its derivatives \( D_j F_i \) are defined in \( \Omega \), continuous in $\Omega$, and can be extended to a continuous function in $\Bar{\Omega}$.

3. \( \int_\Omega (\ldots) dx \) is the usual Lebesgue integral.

4. \( \nu(x) \) is the outward normal to $\Omega$ in $x \in \delta \Omega$ (the boundary). 

5. $F \cdot \nu = \sum_{j=1}^n F_j \nu_j$

6. $\int_{\delta \Omega}  (\ldots) d \sigma_x$ is a surface integral. 

7. The precise definition of "$\Omega$ is $\mathcal{C}^1$" can be found on page 12 of Salsa's Book

We will apply the theorem mainly in the case in which $\Omega = B(x_0, r)$ or $\Omega = B(x_0,r) \setminus B(y_0,s)$ with $\Bar{B}(y_0,r) \subseteq B(x_0,r)$ and these sets are \( \mathcal{C}^1 \).


\begin{remark}
    Recall the definition of partial derivative, where we consider the incremental ratio
    \begin{equation*}
        \frac{f(x+te_i) - f(x_i)}{t}
    \end{equation*}
    and take the limit as $t$ goes to $0$. Nevertheless this is not clear when we are on the border, so we need to work in the closure to include it. 
\end{remark}
% slide 18



% slide 19
Other useful formulas can be derived from (\ref{DF}).

Assume we have \( u,v \in \mC^2(\Bar{\Omega})\) then 
\begin{equation*}
    Div(\underbrace{v}_{\text{number}} \underbrace{ \nabla u}_{\text{vector}}) = \nabla v \underbrace{\cdot}_{\text{scalar product}} \nabla u + v \nabla u 
\end{equation*}
Notice that the equality on the right-hand side is a surface integral. \\
Hence
\begin{equation*}
    \int_\Omega \nabla v \cdot \nabla u dx+ \int_\Omega v \Delta u dx = \int_{\delta \Omega }v \nabla u \nu d \sigma_x
\end{equation*}
When \( V = 1 \), this gives
\[
\int_{\Omega} \Delta u \, dx = \int_{\delta \Omega } \nabla u \nu d \sigma_x
\]
Interchanging \( u \) and \( v \) and subtracting, we also have:
\begin{equation*}
    \int_{\Omega} (v\Delta u-u \Delta v) \, dx = \int_{\delta \Omega } (v \nabla u \cdot \nu -u \nabla v \cdot \nu ) d \sigma_x
\end{equation*}
%Per me ci sono troppi EHMMMM, UHMMMMM come già in triennale mamma mia 

\begin{remark}
    The integral on border of $\Omega$ is 0 since the support of v is compact and included strictly in $\Omega$, so v is $0$ outside of it and in particular on the border of $\Omega$.
\end{remark}
% slide 20

If \( v \in \mC_0^2(\Omega) \), then
\begin{equation*}
    \int_{\Omega} v\Delta u \, dx = - \int_{\Omega} \nabla v \cdot \nabla u \, dx + \int_{\delta \Omega } v \nabla u \cdot \nu d \sigma_x =-  \int_{\Omega }   \nabla v \cdot \nabla u  dx
\end{equation*}
because \( v = 0 \) on $\delta \Omega  $. On the other hand:
\begin{equation*}
    \int_{\Omega} u\Delta v \, dx = - \int_{\Omega} \nabla v \cdot \nabla u \, dx + \int_{\delta \Omega } v \nabla u \cdot \nu d \sigma_x = - \int_{\Omega }   \nabla v \cdot \nabla u  dx
\end{equation*}
because \( \nabla v =0  \) on \( \delta \Omega \). Hence
\begin{equation*}
    \int_\Omega  v \Delta u dx = \int_\Omega u \Delta v dx 
\end{equation*}
%slide 21,22
\paragraph{An Integration Formula}
Let \( \Omega \subseteq \mathbb{R}^n \) be open, \( f: \Omega \to \mathbb{R} \) continuous, and let \( B(x_0, r) \subseteq \Omega \) be a ball centered at \( x_0 \) with radius \( r \). We have:
\[
\int_{B(x_0, r)} f(z) \, dz = \int_0^r\Bigg( \int_{\delta B(x_0, \rho)} f(z) \, d\sigma_z \Bigg) \, d\rho
\]
As a consequence 
\begin{equation*}
    \frac{d}{dr}\int_{B(x_0, r)} f(z) \, dz =  \int_{\delta B(x_0, r)} f(z) \, d\sigma_z 
\end{equation*}
\begin{ThBox}
    \begin{Th}[Dominated Convergence Theorem]
    Let $\Omega \subseteq \mathbb{R}^n$ be an open set and let \( \{ f_n \}_{k \in \mathbb{N}} \) be a sequence of functions  \( f_k \in  L^1(\Omega) \) such that:
    \begin{itemize}
        \item  \( f_k (x) \ra f(x)\) a.e in $\Omega$
        \item There exists an integrable function \( g \in L^1(\Omega) \) such that \( |f_k(x)| \leq g(x) \) \( \forall k \in \mathbb{N} \) and almost every \( x  \in \Omega\).
    \end{itemize}
Then:
\[
 \int_\Omega f_k(x) \, dx \ra \int_\Omega f(x) \, dx
\]
\end{Th}
\end{ThBox}

\paragraph{Heat (Diffusion) equation}
\begin{equation*}
\begin{cases}
    \delta_t u - \Delta u = 0 \\
    \text{initial/boundary conditions }
\end{cases}    
\end{equation*}
with $u=u(x,t)$, $x=(x_1, \dots, x_n) \in \mR^n$ and $t>0$. 
\subsection{Derivation of the Heat Equation (Not Rigorous!)} 
In many applications, the heat equation describes the evolution in time of the density of some quantity, such as heat or chemical concentration.

If we are studying the quantity in a space region \( U \subseteq \mathbb{R}^3 \), and \( V \subseteq U  \) is a subregion (both open), and \( u(x,t) \) is the density at  \( x  \in V \) and at time \( t \geq 0\), then the total quantity in \( V \) is
\[
 \int_V u(x,t) \, dx
\]
and its rate of change is 
\begin{equation*}
    \frac{d}{dt} \int_V u(x,t) \, dx
\end{equation*}
It is also possible to get 
\begin{equation*}
    \frac{d}{dt} \int_V u(x,t) \, dx = \int_V \frac{\partial u}{\partial t }(x,t) \, dx
\end{equation*}
but the rate of change must be equal to the flux through the boundary, that  is 
\begin{equation*}
    \frac{d}{dt} \int_V u(x,t) \, dx = - \int_{\delta V} q \cdot \nu \, d\sigma
\end{equation*}
which becomes 
\begin{equation*}
   \int_V  \frac{\partial u}{\partial t}  (x,t) \, dx = - \int_{\delta V} (Div \, q) \, dx
\end{equation*}
On the other hand, we assume there is no source of the quantity, so the change in the total amount is due only to the flux through the boundary $\delta V$. If $q$ is the vector of the flux, and $\nu$ is the outer unit normal to $\delta V$, then the total inner flux through $\delta V$ is given by 
\begin{equation*}
    -\int_{\delta V} q \cdot \nu \, d\sigma 
\end{equation*}
Under reasonable hypotheses, it is possible to apply divengence theorem, so that 
\begin{equation*}
    -\int_{\delta V} q \cdot \nu \, d\sigma = - \int_{ V} (Div \, q) \, dx
\end{equation*}
Hence 
\begin{equation*}
    \int_V \big( \frac{\partial u}{\partial t} + div \, q \big)dx =0
\end{equation*}
as this holds \emph{for any} $V\subseteq U$, it is possible to deduce 
\begin{equation*}
    \frac{\partial u}{\partial t} (x,t) + div \, q (x,t)=0
\end{equation*}
In many physical problems it is reasonable to assume 
\begin{equation*}
    q = -k \nabla u 
\end{equation*}
where $k> 0$ is a constant, so that $div \, q= -k \Delta u$. And we get 
\begin{equation*}
    \frac{\partial u}{\partial t}-k \Delta u =0
\end{equation*}
For the sake of simplicity, in what follows we assume \( k = 1 \).

\section{Lecture 8}
\subsection{Fundamental solution of the heat equation}
We look for specific solutions of the heat equation, which will be useful for many different problems. 
\begin{equation*}
    n= 1 \quad \text{the equation is} \quad u_t - u_{xx}=0 \quad (k=1)
\end{equation*}
with $u: \mathbb{R} \times (0,+\infty) \rightarrow \mathbb{R}$. \\
We start by noticing that if $u(x,t)$ is a solution, and $a > 0$, then
\begin{equation*}
    v(x,t) = u(ax,a^2t)
\end{equation*}
also is a solution. \\
(This is due to the fact there in the equation there is one time derivative and two spatial ones). \\
Indeed,
\begin{equation*}
    v_t = a^2 u_t(ax,a^2t) = a^2 u_{xx}(ax,a^2t) = v_{xx}
\end{equation*}
So, it is reasonable to look for functions for which this kind of invariance is "obvious." \\
For example, let 
\begin{equation*}
     u(x,t) = U(\frac{x}{\sqrt{t}})
\end{equation*} 
with $t>0$, where \( U: \mathbb{R} \to \mathbb{R} \). In this case
\begin{equation*}
    u(ax, a^2 t)= U (\frac{ax}{\sqrt{a^2t}})= U(\frac{x}{\sqrt{t}})= u(x,t)
\end{equation*}
We also notice that the following holds:\\
In many problems, the solution \( u(x,t) \) represents the density of "something." If there are no sources, then the total amount of this "something" must remain constant, that is:
\[
    \int_{\mR} u(x,t) \, dx = c= \text{constant}
\]
But if $u(x,t)=U(\frac{x}{\sqrt{t}})$, then, by chance $\frac{x}{\sqrt{t}}=y$.
\[
\int_\mR u(x,t) \, dx = \int_\mR U(\frac{x}{\sqrt{t}}) \, dx = \int_\mR U(y)\sqrt{t} \, dy = \sqrt{t} \int_\mR U(y) \, dy
\]
and, of course, this is not constant! Hence, a reasonable guess for a solution is:
\[
u(x,t) = \frac{1}{\sqrt{t}}U(\frac{x}{\sqrt{t}}) \quad (t > 0)
\]

Before computing, we notice another feature of the heat equation
\begin{equation*}
    u_t = u_{xx}
\end{equation*}
that is, if $u(x,t)$ is a solution then also 
\begin{equation*}
    v(x,t) = u(-x,t) 
\end{equation*}
is a solution. \\
The simplest way to have this feature, in our guess
\begin{equation*}
    u(x,t) = \frac{1}{\sqrt{t}} U(\frac{x}{\sqrt{t}})
\end{equation*}
is to assume $U$ \emph{even}, that is 
\begin{equation*}
    U(-x) = U(x)
\end{equation*}
which implies 
\begin{equation*}
    u(-x,t) = u(x,t)
\end{equation*}
Notice that $U$ even implies that $U'(0) = 0$. \\
So, our problem is as follows:

Find \( u \in C^2(\mathbb{R}) \), $U$ even, such that the function \( u(x,t) = \frac{1}{\sqrt{t}} U( \frac{x}{\sqrt{t}})\) , $t>0$, solves the heat equation. \\
We will skip the details of the computations, which lead to the equation:
\[
U'(y) + \frac{1}{2}yU(y) = 0
\]
which is a linear, first-order ODE.\\
By standard techniques, we obtain the set of all solutions:
\begin{equation*}
    U(y) = c_0 e^{-\frac{y^2}{4}} \quad (c_0 \in \mathbb{R})
\end{equation*}
Hence, these solutions give us solutions of the heat equation:
\begin{equation*}
    u(x,t)= c_0 \frac{1}{\sqrt{t}} e^{- \frac{x^2}{4t}}
\end{equation*}
Now we fix $c_0$ such that 
\begin{equation*}
    \int_\mR u(x,t) \, dx= 1
\end{equation*}
For this, we recall the Gaussian integral
\begin{equation*}
    \int_{-\infty}^{+\infty} e^{-x^2} \, dx  = \sqrt{\pi}
\end{equation*}
Hence 
\begin{equation*}
    \int_\mR u(x,t) dx = \frac{c_0}{\sqrt{t}} \int_\mR e^{\frac{x^2}{4t}} dx = [y  =\frac{x}{2 \sqrt{t}}] = \frac{c_0}{\sqrt{t}} \int_\mR e^{-y^2}2 \sqrt{t} dy = 2 c_0 \int_\mR e^{-y^2} dy = 2 c_0 \sqrt{\pi} 
\end{equation*}



Hence, we choose \( c_0= \frac{1}{2 \sqrt{\pi}}= \frac{1}{\sqrt{4 \pi }} \), and we obtain the fundamental solution of the heat equation:
\[
\Gamma(x,t) = \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}} \quad (x \in \mathbb{R}, \, t > 0).
\]
In the case \( n>1 \), we argue in a similar way:
\begin{itemize}
    \item We set \( u(x,t) = \frac{c_0}{t^{\frac{n}{2}}} U(\frac{|x|}{\sqrt{t}}) \), $c_0 \in \mR$.
    \item We look for an equation for the function \( U \in C^2(\mathbb{R}) \).
    \item After some computations,  more complicated than for \( n = 1 \), we end up with
\[
U'(y) + \frac{1}{2} y U(y) = 0
\]
So, as before,
\[
U(t) = c_0 e^{-\frac{y^2}{4}}
\]
and 
\[
u(x,t) = \frac{c_0}{t^\frac{n}{2}} e^{-\frac{|x|^2}{4t}} 
\]
\item To choose $c_0$, we set as before 
\begin{equation*}
    \int_{\mR^n} u(x,t) dx = 1
\end{equation*}
We have 
\begin{align*}
    \int_{\mR^n} e^{-\frac{|x|^2}{4t}} dx &\stackrel{y = \frac{x}{\sqrt{4t}}}= \int_{\mR^n} e^{-|y|^2}(4t) ^ \frac{n}{2}dy \\
    &= (4t)^\frac{n}{2} \int_{\mR^n}e^{- y_1^2} e^{-y_2^2} \ldots e^{-y_n^2} dy_1 \ldots dy_n \\
    &= (4t)^\frac{n}{2} (\int_{\mR^n} e^{-y^2}dy)^n \\
    &= (4t)^\frac{n}{2} \pi^\frac{n}{2}
\end{align*}
\end{itemize}
Hence, 
\[
 \int_{\mR^n} u(x,t) dx= \frac{c_0}{t^{n/2}} \int_{\mR^n} e^{-\frac{|x|^2}{4t}} dx=  \frac{c_0}{t^{n/2}}(4t)^{\frac{n}{2}}(\pi)^{\frac{n}{2}} = c_0 (4\pi)^{\frac{n}{2}}
\]
So we choose the constant \( c_0 = \frac{1}{(4\pi )^{n/2}} \), and we obtain the fundamental solution for the heat equation:
\[
\Gamma(x,t) = \frac{1}{(4\pi t)^{n/2}} e^{-\frac{|x|^2}{4t}} \quad (t > 0)
\]
Of course, when $x \rightarrow +\infty$ the solution goes to $0$, that is the function is very small in value when we fix a certain value of $t$. \\
We are now going to study the behaviour of the function when $t \rightarrow + \infty$. \\

\subsection{Some Remarks on the Asymptotic Behavior of $\Gamma (x,t)$ }\\
\begin{itemize}
    \item It is 
  \[
  \lim_{t \to\ +\infty} \Gamma (x,t) = 0 \quad \forall \ x \in \mR^n, 
  \]
  and also 
  \begin{equation*}
      \lim_{t \to +\infty}\sup_{x \in \mR^n}\Gamma (x,t) = 0 
  \end{equation*}
  As for $t\ra 0^+$, we notice what follows:    
  \item for any $\delta>0$, we set $A_\delta= \{x \in \mR^n| |x| \geq \delta \}$, and we have 
  \begin{equation*}
      0 < \Gamma(x,t)\leq \frac{1}{(4 \pi t )^{n/2}} e ^{-\frac{\delta^2}{4t}} \quad \forall t >0, \, \forall x \in A_\delta
  \end{equation*}
\end{itemize}
\begin{remark}
    Notice that the limit $0$ as $t \rightarrow 0^+$ is due to the greater strength of the exponential with respect to the power function, thus solving the indeterminate form $+\infty \cdot 0$ giving result $0$. \\
\end{remark}
Hence 
\begin{equation*}
    0 < \sup_{x \in A_\delta} \Gamma(x,t) \leq \frac{1}{(4\pi t)^\frac{n}{2}} e^{-\frac{\delta^2}{4t}} \rightarrow 0 \quad t \rightarrow 0^+
\end{equation*}
It holds 
\begin{equation*}
    \lim_{t \rightarrow 0^+} \int_{A_\delta} \Gamma(x,t) dx = 0
\end{equation*}
Indeed we have 
\begin{align*}
    0 < \int_{A_\delta} \Gamma(x,t) dx &= \int_{A_\delta} \frac{1}{(4 \pi t)^\frac{n}{2}} e^{- \frac{|x|^2}{4t}} dx \\
    &\stackrel{y = \frac{x}{2 \sqrt{t}}}= \frac{1}{(4 \pi t)^\frac{n}{2}} \int_{\{|y| \geq \frac{\delta}{2 \sqrt{t}}\}} 2^n t^\frac{n}{2}e^{-|y|^2} dy\\
    &= \frac{1}{\pi ^\frac{n}{2}}\int_{\{|y| \geq \frac{\delta}{2 \sqrt{t}}\}} e^{-|y|^2} dy 
\end{align*}
as $e^{-|y|^2} \in L^1(\mathbb{R}^n)$ and $\frac{\delta}{2 \sqrt{t}} \rightarrow +\infty, \quad t \rightarrow 0^+$.\\
Standard integration theory gives 
\begin{equation*}
    \lim_{t \ra 0^+}\int _{\{|y| \geq \frac{\delta}{2 \sqrt{t}}\}} e ^{- |y|^2} dy =0
\end{equation*}
Hence 
\begin{equation*}
    \lim_{t \ra 0^+} \int_{A_\delta}\Gamma(x,t)dx=0
\end{equation*}
We also get 
\begin{equation*}
    1 = \int_{\mR^n} u(x,t)dx = \int_{|x|< \delta} u(x,t)dx+\int_{|x|\geq \delta} u(x,t)dx
\end{equation*}
hence 
\begin{equation*}
    \lim_{t \ra 0^+}\int_{|x|< \delta}u(x,t)dx=1 \qquad \forall \delta >0
\end{equation*}
Roughly: as \( t \ra 0^+ \), the total amount of \( u(t,x) \) "concentrates near the origin." 

When \((x,t) \ra (0,0^+)\), the function \( \Gamma(x,t) \) has no limit. For simplicity, let us see this in the case \( n = 1 \).\\
In this case
\[
\Gamma(t,x) =  \frac{1}{\sqrt{4\pi t}} e^{-\frac{x^2}{4t}}
\]
along any curve $x^2= ct$, $c>0$ constant, we have $\Gamma(t,x) =  \frac{1}{\sqrt{4\pi t}} e^{-\frac{c}{4}}$, so 
\begin{equation*}
    \lim_{t \rightarrow 0^+, \,x^2 = ct}\Gamma(x,t)= +\infty
\end{equation*}

On the other hand, along any curve $x^4= ct$, $c>0$ constant, we have \[\Gamma(t,x) =  \frac{1}{\sqrt{4\pi t}} e^{-\frac{\sqrt{ct}}{4t}} = \frac{1}{\sqrt{4 \pi t}}e^{-\frac{\sqrt{c}}{4}\frac{1}{\sqrt{t}}}\]so that 
\begin{equation*}
    \lim_{t \ra 0^+, x^4 = ct } \Gamma(x,t)= 0
\end{equation*}
So $ \lim_{(x,t)\ra(0,0)}\Gamma(x,t)$ does not exist.
%% Baddy ha balzato tante slide 
\begin{remark}
    As $\Gamma \in \mathcal{C}^{\infty}(\mR^n \times (0,+\infty))$, it is obvious that all the derivatives of $\Gamma$ are solutions of the diffusion equation. \\
    For example 
    \begin{align*}
        (\partial_t - \Delta)(\partial_{x_j}\Gamma) &= \partial_t \partial_{x_j} \Gamma - \sum_{i=1}^n \partial_{x_ix_i} \partial_{x_j} \Gamma \\
        &= \partial_{x_j} \partial_t \Gamma - \partial x_j \sum_{i=1}^n \partial_{x_i x_j} \Gamma \\
        &= \partial_{x_i}(\partial_t \Gamma - \Delta \Gamma) \\
        &= \partial_{x_i} 0 = 0
    \end{align*}
\end{remark}

\textbf{Derivatives of} $\Gamma$\\
Standard computations give that 
\begin{align*}
    \frac{\partial \Gamma}{\partial t}(x,t) &= -\frac{n}{2} \frac{1}{(4 \pi)^\frac{n}{2}} \frac{1}{t^\frac{n+2}{2}} e^{- \frac{|x|^2}{4t}} + \frac{1}{(4 \pi)^\frac{n}{2}} \frac{1}{4} \frac{|x|^2}{t^ \frac{n+4}{2}} e^{- \frac{|x|^2}{4t}} \\
    &= - \frac{n}{2} \frac{1}{(4 \pi)^\frac{n}{2}} \frac{1}{t^\frac{n+2}{2}}e^{-\frac{|x|^2}{4t}} + \sum_{j=1}^n \frac{1}{4 (4 \pi)^\frac{n}{2}} \frac{x_j^2}{t^\frac{n+4}{2}} e^{-\frac{|x|^2}{4t}}
\end{align*}
\begin{equation*}
    \frac{\partial \Gamma}{\partial x_i}(x,t) = -\frac{1}{2} \frac{1}{(4 \pi)^\frac{n}{2}} \frac{x_i}{t^\frac{n+2}{2}} e^{- \frac{|x|^2}{4t}}
\end{equation*}
\begin{PropBox}
    \begin{Proposition}
        Any derivative of $\Gamma$ is a sum of terms of the following form:
        \begin{equation*}
            a \frac{x_1^{k_1}\ldots x_n^{k_n}}{t^\frac{m}{2}} e^{-\frac{|x|^2}{4t}}
        \end{equation*}
        where $a \in \mR, k_i \in \mathbb{N}, m \in \mathbb{N} \setminus \{0\}$.
    \end{Proposition}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
    The thesis is true for $\partial_t \Gamma$ and $\partial_{x_i} \Gamma$. It is enough to prove that the derivatives of 
    \begin{equation*}
            a \frac{x_1^{k_1}\ldots x_n^{k_n}}{t^\frac{m}{2}} e^{-\frac{|x|^2}{4t}}
        \end{equation*}
        are finite sums of terms of the same form, and this is obtained by standard computations. 
\end{proof}
\end{ProofBox}
\begin{PropBox}
    \begin{Lemma}
        Let 
        \begin{equation*}
            g(x,t) = a \frac{x_1^{k_1}\ldots x_n^{k_n}}{t^\frac{m}{2}} e^{-\frac{|x|^2}{4t}}
        \end{equation*}
        as above. \\
        Then
        \begin{itemize}
            \item [i)] $\int_{\mR^n} |g(x,t)| dx < +\infty$\\
            \item [ii)] $\lim_{(x,t) \rightarrow (x_0, 0)} g(x,t) = 0 \quad (x_0 \neq 0, t> 0)$
            \item [iii)] $\lim_{t \rightarrow +\infty} g(x,t) = 0 = \lim_{|x| \rightarrow +\infty} g(x,t)$
        \end{itemize}
    \end{Lemma}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
    \begin{itemize}
        \item (i) Notice that 
        \begin{equation*}
            |g(x,t)| \leq \frac{|a|}{t^\frac{m}{2}} |x|^k e^{-\frac{m^2}{4t}}
        \end{equation*}
        where $k = k_1 +\ldots + k_n$. Hence 
        \begin{align*}
            \int_{\mR^n}|g(x,t)|dx &\leq \frac{|a|}{t^\frac{m}{2}}\int_{\mR^n} |x|^k e^{-\frac{|x|^2}{4t}}dx \\
        &\stackrel{y=\frac{x}{2\sqrt{t}}}= \frac{|a|}{t^\frac{m}{2}} \int_{\mR^n} 2^k t^\frac{k}{2} |y|^k e^{-|y|^2} 2^n t^\frac{n}{2} dy \\
        &= c(t) \int_{\mR^n} |y|^k e^{-|y|^2} dy < +\infty
        \end{align*}
        where $c(t)$ is a positive constant depending on $t$.
        \item (ii) and (iii) are easy to check. 
    \end{itemize}
\end{proof}
\end{ProofBox}
As any derivative of $\Gamma$ is a finite sum of functions like $g(x,t)$ in the previous lemma, we easily obtain the following result. 

\begin{PropBox}
    \begin{Proposition}
        Let $D(x,t)$ be any derivative of $\Gamma(x,t)$. We have
        \begin{itemize}
            \item (i) $\int_{\mR^n} |D(x,t)| dx < +\infty$
            \item (ii) $\lim_{(x,t) \rightarrow (x_0,0)}D(x,t) = 0 \quad (x_0 \neq 0, t> 0)$
            \item (iii) $\lim_{t \rightarrow +\infty} D(x,t) = \lim_{|x| \rightarrow +\infty} D(x,t) = 0$ 
        \end{itemize}
    \end{Proposition}
\end{PropBox}

\begin{Cor}
    Let us extend the definition of $\Gamma$ to non-positive t's, setting 
    \begin{equation*}
        \Gamma(x,t)= 0 \quad \forall (x,t) \in \mathbb{R}^n \times (-\infty,0] \setminus \{(0,0)\}
    \end{equation*}
    Hence 
    \begin{itemize}
        \item (i) $\Gamma \in \mathcal{C}^\infty (\mR^n \times \mR \setminus \{(0,0)\})$
        \item (ii) $(\partial_t - \Delta) \Gamma = 0$ in $\mR^n \times \mR \setminus \{(0,0)\}$
    \end{itemize}
\end{Cor}

The fundamental solution $\Gamma(x,t)$ gives us a way to get solutions of the following initial value problem
\begin{equation*} (IVP)
    \begin{cases}
        u_t - \Delta u = 0 & \text{ on  } \mR^n \times (0,+\infty) \\
        u=g & \text{ on  } \mR^n \times \{t=0\}
    \end{cases}
\end{equation*}
where $g:\mR^n \rightarrow \mR$ is a function, and we will assume suitable hypotheses. \\
We will prove that a solution to (IVP) is given by the convolution between $\Gamma$ and $g$.    
\begin{ThBox}
    \begin{Th}
        Assume $g \in L^\infty(\mR^n)$. Define 
        \begin{equation*}
            u(x,t) = \int_{\mR^n}\Gamma(x-y,t) g(y) dy = \frac{1}{(4 \pi t)^\frac{n}{2}}\int_{\mR^n} e^{-\frac{|x-y|^2}{4t}} g(y) dy 
        \end{equation*}
        Then, we have
        \begin{itemize}
            \item $u \in \mathcal{C}^\infty(\mR^n \times (0,+\infty))$
            \item $u_t - \Delta u = 0 \quad$ in $\mR^n \times (0,+\infty)$
            \item If $g$ is continuous at $x_0 \in \mR^n$, then
            \begin{equation*}
                \lim_{(x,t) \rightarrow (x_0, 0^+)} u(x,t) = g(x_0)
            \end{equation*}
        \end{itemize}
    \end{Th}
\end{ThBox}
We will prove the theorem through several lemmas. \\
The first ones are devoted to prove (i). For this, the main tool will be Lebesgue's Dominated Convergence Theorem. We will prove only the existence of first derivatives, the general argument is similar (but more complicated). 

\begin{PropBox}
    \begin{Lemma}[Lemma 1]
    \label{Lemma 1}
        $u$ is continuous in $\mR^n \times (0,+\infty)$.
    \end{Lemma}
\end{PropBox}

\newpage
\section{Lecture 9}
\begin{ProofBox}
    \begin{proof}
    Assume that $\{(x_k, t_k)\}_{k \in \mathbb{N}}$ is a sequence in $\mR^n \times (0,+\infty)$ converging to $(\Bar{x}, \Bar{t}) \in \mR^n \times (0,+\infty)$. We have to prove that 
    \begin{equation*}
        u(x_k,t_k) \rightarrow u(\Bar{x}, \Bar{t})
    \end{equation*}
    We want to apply the Dominated Convergence Theorem, therefore we need convergence (pointwise) and dominance.\\
    We have that 
    \begin{equation*}
        \Gamma(x_k - y, t_k) g(y) \rightarrow \Gamma(\Bar{x} - y, \Bar{t}) g(y)
    \end{equation*}
    as $k \rightarrow +\infty$.\\
    We then have to prove that the functions $\Gamma(x_k - y, t_k) g(y)$ are uniformly bounded in $L^1(\mR^n)$ as $t_k \rightarrow \Bar{t} > 0$. We can assume $0 < c_1 < t_k < c_2$ and also $|x_k - \Bar{x}| < \frac{1}{2}$. We have 
    \begin{equation*}
        |\Gamma(x_k - y, t_k) g(y)| \leq M \frac{1}{(4 \pi c_1) ^ \frac{n}{2}} e^\frac{|x_k - y|^2}{4 c_2} = c_3 e^{-\frac{|x_k - y|^2}{4c_2}}
    \end{equation*}
    To apply Dominated Convergence Theorem, we need the bound to be independent of $k$, so we are not done yet.\\
    Let us define the sets 
    \begin{equation*}
        \begin{split}
            A_1 &= \{y \in \mR^n \mid |\Bar{x}-y| < 2\} = B_2(\Bar{x}) \\
            A_2 &= \mR^n \setminus A_1 = \{y \in \mR^n \mid |\Bar{x}-y| \geq 2\}
        \end{split}
    \end{equation*}
    We need to construct a new $L^1$ bound since $c_3$ on $\mR^n$ is not an $L^1$ function: it is such instead on $A_1$ since the latter is bounded. \\
    If $y \in A_2$, then
    \begin{equation*}
        |x_k - y| = |x_k -\Bar{x} + \Bar{x} - y| \geq |\Bar{x}-y| - |x_k - \Bar{x}| \geq 2 -\frac{1}{2} = \frac{3}{2} > 1
    \end{equation*}
    Hence
    \begin{equation*}
        |x_k - y|^2 > |x_k - y| \geq |\Bar{x} - y| - |x_k - \Bar{x}| > |\Bar{x} - y| - \frac{1}{2}
    \end{equation*}
    so that 
    \begin{equation*}
        e^{-\frac{|x_k - y|^2}{4c_2}} \leq e^{-\frac{|\Bar{x}-y|}{4 c_2} + \frac{1}{8 c_2}} = c_4 e^{-\frac{|\Bar{x}-y|}{4 c_2}}
    \end{equation*}
    This is a bound which does not depend on $k$ anymore. \\
    Obviously, we have 
    \begin{equation*}
        \int_{A_2} e^{-\frac{|\Bar{x}-y|}{4 c_2}} dy < +\infty
    \end{equation*}
    that is, this function is $L^1$ on $A_2$.
    Hence, if we define $\phi(y)$ as follows:
    \begin{equation*}
    \phi(y)=
        \begin{cases}
        c_3 & \text{ if } y \in A_1\\
        c_3 c_4 e^{-\frac{|\Bar{x}-y|}{4 c_2}} & \text{ if } y \in A_2
        \end{cases}
    \end{equation*}
    We obtain $\phi \in L^1(\mR^n)$. By construction, it is 
    \begin{equation*}
        |\Gamma(x_k-y,t_k)g(y)| \leq \phi(y) \quad \forall y \in \mR^n
    \end{equation*}
    We can then apply Dominated Convergence Theorem, and we get 
    \begin{equation*}
        \int_{\mR^n} \Gamma(x_k - y, t_k) g(y) dy \rightarrow \int_{\mR^n} \Gamma(\Bar{x}-y, \Bar{t}) g(y) dy 
    \end{equation*}
    that is 
    \begin{equation*}
        \lim_{(x_k,t_k)\rightarrow (\Bar{x}, \Bar{y})} u(x_k,t_k) = u(\Bar{x}, \Bar{t})
    \end{equation*}
\end{proof}
\end{ProofBox}
\begin{PropBox}
    \begin{Lemma}[Lemma 2]
    \label{Lemma 2}
        The partial derivative $\partial_t u(x,t)$ exists for all $(x,t) \in \mR^n \times (0,+\infty)$, and it holds 
        \begin{equation*}
            \partial_tu(x,t) = \int_{\mR^n} \partial_t \Gamma(x-y,t) g(y)dy
        \end{equation*}
    \end{Lemma}
\end{PropBox}

\begin{ProofBox}
    \begin{proof}
        Let us fix $(x,t) \in \mR^n \times (0,+\infty)$ and write, for small $h$'s,
        \begin{equation*}
            \frac{u(x,t+h) - u(x,t)}{h} = \int_{\mR^n} \frac{\Gamma(x-y,t+h) - \Gamma(x-y,t)}{h} g(y) dy \stackrel{(*)}= \int_{\mR^n} \partial_t \Gamma(x-y,t+s) g(y) dy
        \end{equation*}
        where $|s| \leq |h|$. We used Lagrange Theorem to write the third equality $(*)$. Hence we have
        \begin{align*}
            \Bigg|\frac{u(x,t+h) - u(x,t)}{h}-\int_{\mR^n} \partial_t \Gamma(x-y,t) g(y)dy\Bigg|
            &= \Bigg|\int_{\mR^n} (\partial_t \Gamma(x-y,t+s) - \partial_t \Gamma(x-y,t)) g(y) dy\Bigg|\\
            &\leq M \int_{\mR^n} |\partial^2_{tt} \Gamma(x-y,t+s_1)| |s| dy
        \end{align*}
        where $s_1 = s_2(h)$ and $|s_1| \leq |s| \leq |h|$. \\
        Since, when $h \rightarrow 0, s \rightarrow 0$, we just need to show that the partial derivative in modulus is bounded. Then, the whole absolute value at the beginning is going to $0$, proving the Lemma.\\
        As $t > 0$ and $h \rightarrow 0$, we can assume that there are $c_1, c_2 > 0$ such that $c_1 < t+s_1 < c_2$ for all $h$. \\
        
        We know that $\partial^2_{tt}\Gamma$ is a sum of terms as 
        \begin{equation*}
            g(x,t) = a \frac{x_1^{k_1}\ldots x_n^{k_n}}{t^\frac{m}{2}} e^{-\frac{|x|^2}{4t}} = \psi(x,t)
        \end{equation*}
        and we have
        \begin{equation*}
            |\psi(x-y,t+s_1)| \leq |a| \frac{1}{c_1^\frac{m}{2}}|x-y|^k e^{-\frac{|x-y|^2}{4c_2}} \quad (k=k_1 + \ldots + k_n)
        \end{equation*}
        Hence
        \begin{align*}
            \int_{\mR^n} |\psi(x-y,t+s_1)| dy &\leq \frac{|a|}{c_1^\frac{m}{2}} \int_{\mR^n} |x-y|^k e^{-\frac{|x-y|^2}{4c_2}}dy \\
            &= \frac{|a|}{c_1^\frac{m}{2}} \int_{\mR^n} |y|^k e^{-\frac{|y|^2}{4c_2}} dy \\
            &= c
        \end{align*}
        where $c$ is a constant independent of $h$. Hence also
        \begin{equation*}
            \int_{\mR^n} |\partial_{tt} \Gamma(x-y,t+s_1)| dy \leq c'
        \end{equation*}
        where $c'$ is a constant independent of $h$. So,
        \begin{equation*}
            \lim_{h \rightarrow 0} |\frac{u(x,t+h) - u(x,t)}{h}-\int_{\mR^n} \partial_t \Gamma(x-y,t) g(y)dy| \leq \lim_{h \rightarrow 0} M c'|s| = 0
        \end{equation*}
        which means that 
        \begin{equation*}
            \lim_{h \rightarrow 0} \frac{u(x,t+h) - u(x,t)}{h} = \int_{\mR^n} \partial_t \Gamma(x-y,t) g(y)dy
        \end{equation*}
        that is 
        \begin{equation*}
            \partial_t u(x,t) = \int_{\mR^n} \partial_t \Gamma(x-y,t) g(y) dy
        \end{equation*}
    \end{proof}
\end{ProofBox}
\begin{PropBox}
    \begin{Lemma}[Lemma 3]
    \label{Lemma 3}
        The partial derivative $\partial_{x_i} u(x,t)$ exists for all $i = 1, \ldots, n$ and all $(x,t) \in \mR^n \times (0,+\infty)$ and the following holds:
        \begin{equation*}
            \partial_{x_i} u(x,t) = \int_{\mR^n} \partial_{x_i} \Gamma(x-y,t) g(y) dy
        \end{equation*}
    \end{Lemma}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
        Let us fix $(x,t) \in \mR^n \times (0,+\infty)$ and write, for small $h$'s,
        \begin{equation*}
            \frac{u(x+he_i,t) - u(x,t)}{h} = \int_{\mR^n} \frac{\Gamma(x+he_i-y,t) - \Gamma(x-y,t)}{h} g(y) dy = \int_{\mR^n} \partial_{x_i} \Gamma(x+se_i - y, t) g(y) dy
        \end{equation*}
        where $s = s(h)$ and $|s| \leq |h|$. Hence we have
        \begin{align*}
            |\frac{u(x+he_i,t) - u(x,t)}{h} - \int_{\mR^n} \partial_{x_i} \Gamma(x+se_i - y, t) g(y) dy| &= |\int_{\mR^n}  \Bigg[ \partial_{x_i} \Gamma(x+se_i - y, t) - \partial_{x_i} \Gamma(x - y, t)\Bigg] g(y) dy| \\
            &\leq M \int_{\mR^n} |\partial_{x_ix_i} \Gamma(x+s_1 e_i - y,t)| |s| dy \\
            & \leq |h| M \int_{\mR^n} |\partial_{x_ix_i} \Gamma(x+s_1 e_i - y,t)| dy
        \end{align*}
        where $s_1 = s_1 (h)$ and $|s_1| \leq |s| \leq |h|$. \\
        Arguing as in Lemma \ref{Lemma_2}, it is possible to prove that 
        \begin{equation*}
            \int_{\mR^n} |\partial_{x_ix_i} \Gamma(x+s_1 e_i - y,t)| dy \leq c
        \end{equation*}
        where $c$ is a constant independent from $h$, at least for small $h$'s. \\
        As $h \rightarrow 0$, we get 
        \begin{equation*}
            |h| M \int_{\mR^n} |\partial_{x_ix_i} \Gamma(x+s_1 e_i - y,t)| dy \rightarrow 0
        \end{equation*}
        Hence
        \begin{equation*}
            \lim_{h \rightarrow 0} |\frac{u(x+he_i,t) - u(x,t)}{h} - \int_{\mR^n} \partial_{x_i} \Gamma(x-y,t){h} g(y)dy | = 0
        \end{equation*}
        which means 
        \begin{equation*}
            \lim_{h \rightarrow 0} \frac{u(x+he_i,t) - u(x,t)}{h} = \int_{\mR^n} \partial_{x_i} \Gamma(x-y,t){h} g(y)dy
        \end{equation*}
        that is 
        \begin{equation*}
            \partial_{x_i} u(x,t) = \int_{\mR^n} \Gamma(x-y,t) g(y) dy
        \end{equation*}
    \end{proof}
\end{ProofBox}

Repeating the argument of Lemmas \ref{Lemma 1}, \ref{Lemma 2} and \ref{Lemma 3}, it is possible to prove that all derivatives of $u(x,t)$ exist in $\mR^n \times (0,+\infty)$, and are continuous. This means 
\begin{equation*}
    u \in \mathcal{C}^\infty(\mR^n \times (0,+\infty))
\end{equation*}
Also, we obtain
 \begin{equation*}
        \partial_{x_ix_i} u(x,t) = \int_{\mR^n} \partial_{x_ix_i} \Gamma(x-y,t) g(y) dy
\end{equation*}
Hence
\begin{equation*}
    \Delta u(x,t) = \int_{\mR^n} \Delta \Gamma(x-y,t) g(y) dy
\end{equation*}
It is then obvious to get 
\begin{align*}
    (\partial_t - \Delta) u &= \int_{\mR^n} \partial_t \Gamma(x-y,t) g(y) dy - \int_{\mR^n} \Delta \Gamma(x-y,t) g(y) dy \\
    &= \int_{\mR^n} (\partial_t  - \Delta) \Gamma(x-y,t) = 0
\end{align*}
in all $\mR^n \times (0,+\infty)$. \\
In this way we have proved (i) and (ii). Let us prove (iii):\\

We now fix $x_0 \in \mR^n$ and we prove 
\begin{equation*}
        \lim_{(x,t) \rightarrow (x_0, 0^+)} u(x,t) = g(x_0)
\end{equation*}
We have
\begin{align*}
    u(x,t) &= \frac{1}{(4 \pi t)^\frac{n}{2}}\int_{\mR^n} e^{-\frac{|x-y|^2}{4t}} g(y) dy \\
    & \stackrel{z = \frac{x-y}{2 \sqrt{t}}} = \frac{1}{(4 \pi t)^\frac{n}{2}} \int_{\mR^n} e^{-|z|^2} g(x-2z\sqrt{t}) (4t)^\frac{n}{2}dz\\
    &= \frac{1}{\pi^\frac{n}{2}}\int_{\mR^n} e^{-|z|^2} g(x-2\sqrt{t} z) dz 
\end{align*}
As $x \rightarrow x_0$ and $t \rightarrow 0^+$ we have 
\begin{equation*}
    g(x-2\sqrt{t}z) \rightarrow g(x_0)
\end{equation*}
because $g$ is continuous in $x_0$. Hence
\begin{equation*}
    e^{-|z|^2}g(x-2\sqrt{t}z) \rightarrow e^{-|z|^2} g(x_0) \quad \forall z \in \mR^n
\end{equation*}
Also
\begin{equation*}
    |e^{-|z|^2}g(x-2\sqrt{t}z)| \leq M e^{-|z|^2} \in L^1(\mR^n)
\end{equation*}
because $g \in L^\infty(\mR^n)$. Hence
\begin{equation*}
    \frac{1}{\pi^\frac{n}{2}} \int_{\mR^n} e^{-|z|^2}g(x-2\sqrt{t}z) dz\rightarrow \frac{1}{\pi^\frac{n}{2}} \int_{\mR^n} e^{-|z|^2} g(x_0) dz = g(x_0)
\end{equation*}
as $x \rightarrow x_0$ and $t \rightarrow 0^+$. 
\begin{remark}
    \begin{itemize}
        \item Setting $D = \mR^n \times (0,+\infty) \subseteq \mR^{n+1}$, we have that $D$ is an open set in $\mR^{n+1}$, and $\Bar{D} = \mR^n \times [0,+\infty)$.\\
        Setting 
        \begin{equation*}
            u(x,t) = \int_{\mR^n} \Gamma(x-y,t) g(y) dy
        \end{equation*}
        when $t > 0$
        \begin{equation*}
            u(x,0) = g(x) \in L^\infty(\mR^n) \cap \mathcal{C}^0(\mR^n)
        \end{equation*}
        We have $u \in \mathcal{C}^\infty(D) \cap \mathcal{C}^0(\Bar{D})$ and $u$ solves the initial value problem
        \begin{equation*}
            \begin{cases}
                u_t - \Delta u = 0 & \text{ in } D \\
                u(x,0) = g(x) & \text{ in } \mR^n \times \{0\}
            \end{cases}
        \end{equation*}
        \item $g$ can be non regular in $\mR^n$, while $u(x,t) \in \mathcal{C}^\infty (D)$: the convolution operation that gives $u(x,t)$ has a smoother effect.\\
        \begin{tikzpicture}
    \begin{axis}[
        axis lines=middle,
        xlabel=$x$, ylabel=$g(x)$,
        domain=-3:3, % Definisce il dominio del grafico
        samples=100, % Aumenta la qualità del grafico
        ymin=-1, ymax=5, % Limiti dell'asse y
        xmin=-3, xmax=3, % Limiti dell'asse x
        ]
        % Funzione che è 0 fuori dall'intervallo [-2, 2]
        \addplot [
            domain=-3:-1,
            samples=2,
            thick,
            color=blue
        ] {0};

        \addplot [
            domain=1:3,
            samples=2,
            thick,
            color=blue
        ] {0};
        
        % Funzione parabola con concavità verso il basso dentro [-2, 2]
        \addplot [
            domain=-1:1,
            thick,
            color=red
        ] {-x^2 + 1};
    \end{axis}
\end{tikzpicture}\\
        \item Assume $g \geq 0, g \not\equiv 0$, and support $g$ "very small": that is, $g$ is strictly positive only in a small ball, for example. By the formula for $u(x,t)$, we deduce $u(x,t) > 0$ for all $x \in \mR^n$ and all $t> 0$. \\
        This means that there is "infinite speed" of diffusion, a feature not very coherent with physics. However, for large $x$'s the value $u(x,t)$ is very small.
    \end{itemize}
\end{remark}

\textbf{Homework}\\
\begin{itemize}
    \item Verify that indeed
    \begin{equation*}
        \frac{\partial}{\partial t} \Gamma - \Delta \Gamma = 0 \quad \forall (x,t) \in \mR^n \times (0,+\infty)
    \end{equation*}
    \item Fix $\tau > 0$ and define 
    \begin{equation*}
        u(x,t) = \frac{1}{(4 \pi (\tau -t))^\frac{n}{2}} e^{-\frac{|x|^2}{4(\tau - t)}} \quad (x,t) \in \mR^n \times (-\infty, \tau)
    \end{equation*}
    Verify that 
    \begin{equation*}
        \frac{\partial u}{\partial t} - \Delta u = 0
    \end{equation*}
    in $\mR^n \times (-\infty, \tau)$.
\end{itemize}
%%
%%
%% Nuovo pacchetto slide 
%%
%%

\subsection{Nonhomogeneous Problems}

We now give, without proof, Duhamel's formula for the solution of the nonhomogeneous problem (NHP):
\begin{equation}\label{NHP}
(NHP):
    \begin{cases}
        u_t - \Delta u = f \quad \text{in} &\quad \mathbb{R}^n \times (0,+ \infty)\\
        u=0 \quad  \text{in} & \mathbb{R}^n \times \{0\}
    \end{cases}
\end{equation}
with suitable hypotheses on \( f =f(x,t). \)

Duhamel's formula for the solution of (NHP) is as follows:
\begin{equation}\label{DF}
    \begin{split}
        u(x,t) &= \int_0^t \int_{\mR^n} \Gamma(x-y,t-s) f(y,s) dyds \\
        &= \int_0^t \int_{\mR^n} \frac{1}{(4 \pi (t-s))^{\frac{n}{2}}} e^{-\frac{|x-y|^2}{4(t-s)}} f(y,s) dyds 
    \end{split}
\end{equation}
Under suitable hypotheses on \( f \), it is possible to prove that \eqref{DF} gives a classical solution to (NHP). Here, a "classical solution" means that in \( \mathbb{R}^n \times (0, +\infty) \) there exist all the derivatives:
\begin{equation*}
    \frac{\partial u}{\partial t}, \, \frac{\partial u}{\partial x_i},\, \frac{\partial^2 u}{\partial x_i \partial x_j} \qquad i,j= 1, \dots, n
\end{equation*}
that they are all continuous in \( \mathbb{R}^n \times (0, +\infty) \).\\ 
Furthermore, if we set \( u(x,0) = 0 \), we get the following result:
\begin{equation*}
    u \in \mathcal{C}^0 (\mR^n \times [0, +\infty))
\end{equation*}
The result we get is the following:
\begin{ThBox}
    \begin{Th}
    Assume that the function has in $\mR^n \times [0, + \infty)$ the derivatives:
    \begin{equation*}
    \frac{\partial f}{\partial t}, \, \frac{\partial f}{\partial x_i},\, \frac{\partial^2 f}{\partial x_i \partial x_j} \qquad i,j= 1, \dots, n
\end{equation*}
that they are continuous, together with $f$, in $\mR^n \times [0, +\infty)$, and they are all bounded in $\mR^n \times [0, T]$, for all $T>0$. Hence, formula \eqref{DF} gives a classical solution to \eqref{NHP}
\end{Th}
\end{ThBox}
\begin{remark}
    \begin{itemize}
        \item \ref{DF} derives from a general principle that is applied to different kinds of PDE.
        \item Thanks to the linearity of the equation, the solution of the general problem:
        \begin{equation*}
            \begin{cases}
                u_t - \Delta u = f & \text{ in } \mR^n \times (0,+\infty) \\
                u = g & \text{ on } \mR^n \times \{0\}
            \end{cases}
        \end{equation*}
        can be obtained by the sum of the solutions of the two problems
        \begin{equation*}
            \begin{cases}
                u_t - \Delta u = f & \text{ in } \mR^n \times (0,+\infty) \\
                u = 0 & \text{ on } \mR^n \times \{0\}
            \end{cases}
        \end{equation*}
        and 
        \begin{equation*}
            \begin{cases}
                u_t - \Delta u = f & \text{ in } \mR^n \times (0,+\infty) \\
                u = g & \text{ on } \mR^n \times \{0\}
            \end{cases}
        \end{equation*}
        Of course, we need suitable hypotheses on $f$ and $g$ to apply our previous results. 
        \item Looking at physical problems, the nonhomogeneous term $f(x,t)$ corresponds to a source (or a sink). 
    \end{itemize}
\end{remark}
\subsection{Fundamental Solutions of Heat Equation}
We now prove that the function $\Gamma(x,t)$ is the fundamental solution of the heat equation, in the sense of distribution theory. \\
Here, we extend $\Gamma$ to all $\mR^n \times \mR$, setting $\Gamma(x,t) = 0 \quad \forall t \leq 0$.\\
As we know
\begin{equation*}
    \lim_{t \rightarrow 0^+} \underbrace{\Gamma(x,t)}_{x \neq 0} = 0
\end{equation*}
and the same holds for all derivativews of $\Gamma$. \\
We get that the extended $\Gamma$ satisfies $\Gamma \in \mathcal{C}^\infty(\mR^n \times \mR \setminus \{(0,0)\})$.\\
We need to prove $\Gamma \in L^1_{\text{loc}}(\mR^n \times \mR)$.\\
We have to check what happens on open bounded sets containing the origin. For $M > 0$ we define 
\begin{equation*}
    A_M = B_M \times (-M,M) = \{(x,t) \in \mR^n \times \mR \mid |x| < M, |t| < M\}
\end{equation*}
The fact that $\Gamma \in L^1_{\text{loc}}(\mR^n \times \mR)$ is an obvious consequence of the following 
\begin{PropBox}
    \begin{Lemma}
        It is 
        \begin{equation*}
            \int_{A_M} \Gamma(x,t) dx dt < +\infty \quad \forall M > 0
        \end{equation*}
    \end{Lemma}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
    Recall that $\Gamma(x,t) = 0, t \leq 0$. 
        It is 
        \begin{align*}
            \int_{A_M} \Gamma(x,t) dx dt  &= c \int_0^M \Bigg[\int_{|x| < M} \frac{1}{t^\frac{n}{2}}e^{-\frac{|x|^2}{4t}}dx\Bigg]dt \\
            &\stackrel{y = \frac{x}{2 \sqrt{t}}}= c \int_0^M \Bigg[\int_{|y| < \frac{M}{2 \sqrt{t}}} \frac{1}{t^\frac{n}{2}}e^{- |y|^2} 2^n t ^\frac{n}{2}dy\Bigg]dt \\
            &\stackrel{2^n\text{ goes into }c}= c \int_0^M \Bigg[\int_{|y| < \frac{M}{2 \sqrt{t}}} \frac{1}{t^\frac{n}{2}}e^{- |y|^2}dy\Bigg]dt \\
            &\leq c \int_0^M \Bigg(\int_{\mR^n}e^{- |y|^2}dy\Bigg)dt = c_1 
        \end{align*}
    \end{proof}
\end{ProofBox}
We want now to prove that $\Gamma$ is the fundamental solution of the heat operator $\partial_t -  \Delta_x$, that is 
\begin{equation*}
    (\partial_t - \Delta_x) \Gamma = \delta 
\end{equation*}
in the sense of distribution theory. What does it mean?\\
This means that for every $\phi \in \mathcal{C}^\infty_0(\mR^n \times \mR)$ it holds 
\begin{equation*}
    < (\partial_t  - \Delta_x) \Gamma, \phi > = < \delta, \phi> = \phi(\underbrace{0}_{\text{vector in the space} \mR^n \times \mR}) 
\end{equation*}
from the definition of derivative of a distribution, we know that 
\begin{equation*}
    < (\partial_t  - \Delta_x) \Gamma, \phi > = < \Gamma, (-\partial_t - \Delta_x) \phi> = - \int_{\mR^n} \int_\mR \Gamma(x,t) (\partial_t + \Delta_x) \phi(x,t) dx dt 
\end{equation*}
So, what we have to prove is the following theorem.
\begin{ThBox}
    \begin{Th}
        For any $\phi \in \mathcal{C}^\infty_0(\mR^n \times \mR)$ it holds 
        \begin{equation*}
            \int_{\mR^n} \int_\mR \Gamma(x,t) (\partial_t + \Delta_x) \phi(x,t) dx dt = \phi(0,0)
        \end{equation*}
        where in $(0,0)$, the first entry belongs to $\mR^n$ while the second one is in $\mR$.
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        We fix $M > 0$ such that $supp(\phi) \subseteq A_M$, where 
        \begin{equation*}
            A_M = B_M \times (-M,M) = \{(x,t) \in \mR^n \times \mR \mid |x|< M, |t| < M \}.
        \end{equation*}
        Recalling that $\Gamma(x,t) = 0$ for $t \leq 0$, we obtain 
        \begin{align*}
            - \int_{\mR^n \times \mR} (\partial_t + \Delta_x) \phi(x,t) dx dt & \stackrel{(*)}= -\int_{A_M} \Gamma(x,t) (\partial_t + \Delta_x) \phi(x,t) dx dt \\
            &= - \int_{-M}^M \int_{|x| < M} \Gamma(x,t) (\partial_t + \Delta_x) \phi(x,t) dx dt \\
            &\stackrel{(**)}= - \int_0^M \int_{|x|< M} \Gamma(x,t) (\partial_t + \Delta_x) \phi(x,t) dxdt \\
            &\stackrel{(-)}= -\lim_{\epsilon \rightarrow 0^+} \int_\epsilon^M \int_{|x| < M} \Gamma(x,t) (\partial_t + \Delta_x) \phi(x,t) dxdt 
        \end{align*}
        In $(*)$ we use the fact that the function $\Gamma$ is $0$ outside $A_M$, while in $(-)$ we avoid the singularity in $0$. Finally, in $(**)$, we recall that $\Gamma(x,t) = 0$ when $t \leq 0$. 
        Let us study separately the different terms:
        \begin{equation*}
            \int_\epsilon^M \int_{|x|<M} \Gamma (x,t) \partial_t\phi(x,t) dx dt = \int_{|x| < M} (\int_\epsilon^M \Gamma(x,t) \partial_t \phi(x,t) dt) dx 
        \end{equation*}
        and, for any fixed $x \in \mR^n$, it is 
        \begin{align*}
            \int_\epsilon^M \Gamma(x,t) \partial_t \phi(x,t) dt &= \Gamma(x,t) \phi(x,t) |_{t=\epsilon}^{t = M} -\int_\epsilon^M \Bigg[\partial_t \Gamma(x,t)\Bigg] \phi(x,t) dt \\
            &= - \Gamma(x,\epsilon) \phi(x,\epsilon) - \int_\epsilon^M \partial_t \Gamma(x,t) \phi(x,t) dt 
        \end{align*}
        so that 
        \begin{equation*}
            - \int_\epsilon^M \int_{|x|<M} \Gamma (x,t) \partial_t\phi(x,t) dx dt = \int_{|x| < M} \Gamma(x,\epsilon) \phi(x,\epsilon) dx + \int_\epsilon^M \int_{|x| < M} \partial_t \Gamma(x,t) \phi(x,t) dx dt 
        \end{equation*}
        Here we have used the fact that $\phi(x,M) = 0 \quad \forall x$, because $supp(\phi) \subseteq A_M$.
        \begin{equation*}
            \int_\epsilon^M \int_{|x| < M} \Gamma(x,t) \Delta_x \phi(x,t) dx dt 
        \end{equation*}
        using the fact that $\phi$ and all its derivatives are zero on $|x|<M$, applying twice diverge theorem we get 
        \begin{equation*}
            \int_{|x| < M} \Gamma(x,t) \Delta_x \phi(x,t) dx = \int_{|x| < M} (\Delta_x \Gamma(x,t)) \phi(x,t) dx 
        \end{equation*}
        Hence 
        \begin{equation*}
            - \int_\epsilon^M \int_{|x|<M} \Gamma (x,t) \partial_t\phi(x,t) dx dt = -\int_\epsilon^M \int_{|x|<M} (\Delta_x \Gamma(x,t)) \phi(x,t) dx dt 
        \end{equation*}
        Putting all together we get 
        \begin{align*}
            -&\int_\epsilon^M \int_{|x| < M} \Gamma(x,t) (\partial_t + \Delta_x) \phi(x,t) dx dt \\
            &= \int_{|x| < M} \Gamma(x,\epsilon) \phi(x,\epsilon) dx + \int_\epsilon^M \int_{|x|< M} \partial_t \Gamma(x,t) \phi(x,t) dx dt - \int_\epsilon^M \int_{|x| < M} \Delta_x \Gamma(x,t) \phi(x,t) dx dt \\
            &= \int_{|x| < M} \Gamma(x,\epsilon) \phi(x,\epsilon) dx + \int_\epsilon^M \int_{|x| < M} (\partial_t - \Delta_x) \Gamma(x,t) dx dt \\
            &= \int_{|x| < M} \Gamma(x,\epsilon) \phi(x,\epsilon) dx \\
            &= \int_{|x| < M}  \frac{1}{(4 \pi \epsilon)^\frac{n}{2}} e^{- \frac{|x|^2}{4 \epsilon}} \phi(x,\epsilon)dx \\
            &= \int_{\mR^n} \frac{1}{(4 \pi \epsilon)^\frac{n}{2}} e^{- \frac{|x|^2}{4 \epsilon}} \phi(x,\epsilon) dx \\
            &= \int_{\mR^n} \frac{1}{(4 \pi \epsilon)^\frac{n}{2}} e^{-|y|^2} \phi(\sqrt{4 \epsilon}y,\epsilon) (4 \epsilon)^\frac{n}{2} dy \\
            &\stackrel{y=\frac{x}{\sqrt{4 \epsilon}}}= \frac{1}{\pi^\frac{n}{2}} \int_{\mR^n} e^{- |y|^2} \phi (2 \sqrt{\epsilon}y, \epsilon) dy 
        \end{align*}
        We have 
        \begin{equation*}
            \lim_{\epsilon \rightarrow 0^+}  e^{- |y|^2} \phi (2 \sqrt{\epsilon}y, \epsilon) =  e^{- |y|^2} \phi (0,0) \quad \forall y \in \mR^n
        \end{equation*}
        and 
        \begin{equation*}
            |e^{-|y|^2} \phi(2 \sqrt{\epsilon}y, \epsilon)| \leq e^{-|y|^2} \cdot C \quad \forall \epsilon > 0 \text{ and } \forall y \in \mR^n
        \end{equation*}
        Hence, by dominated convergence theorem,
        \begin{align*}
            -\lim_{\epsilon \rightarrow 0^+} \int_\epsilon^M \int_{|x| < M} \Gamma(x,t) (\partial_t + \Delta_x) \phi(x,t) dx dt \\
            &= \lim_{\epsilon \rightarrow 0^+} \frac{1}{\pi^\frac{n}{2}} \int_{\mR^n} e^{-|y|^2}\phi(2 \sqrt{\epsilon}y,\epsilon) dy \\
            &= \phi(0,0) \frac{1}{\pi^\frac{n}{2}}\int_{\mR^n} e^{-|y|^2} dy \\
            &= \phi(0,0) \frac{1}{\pi^\frac{n}{2}} \pi^\frac{n}{2} = \phi(0,0)
        \end{align*}
        and the theorem is proved. 
    \end{proof}
\end{ProofBox}
\section{Lecture 10}
\subsection{A problem on a bounded domain}
"Consider the evolution of the temperature $u$ inside a cylindrical bar, whose lateral surface is perfectly insulated and whose length is much larger than its cross-selectional area. Although the bar is three dimensional, we may assume that heat moves only down the length of the bar" [Salsa page 21]\\
We want to study the evolution of $u$, assuming that the initial value of $u$ is given, and some hypothesis on the behaviour of $u$ at the ends of the bar. \\
Several kinds of problems can be treated, we give here two examples. 
\begin{enumerate}
    \item \textbf{Cauchy-Dirichlet conditions}
    \begin{equation*}
    (CDP)
        \begin{cases}
            u_t(x,t) - u_{xx}(x,t)&= 0  \quad (x,t) \in (0,\pi) \times (0,+\infty)\\
            u(x,0) & = g(x) \quad x \in [0,\pi] \\
            u(0,t) &= u(\pi,t) = 0 \quad t > 0
        \end{cases}
    \end{equation*}
    We will prove existence and uniqueness of solution for this problem. We use the method of "separation of variables" (the second method we will use is the one concerning the Fourier series). \\
    This means that we look for solutions of a particular form: $u(x,t) = v(x) w (t)$. \\
    Also, for the moment we neglect the initial condition $u(x,0) = g(x)$, so we look for solutions of the problem
    \begin{equation*}
        \begin{cases}
            u_t - u_{xx} &= 0 \quad \forall (x,t)\\
            u(0,t) &= u(\pi,t) = 0 \quad \forall t
        \end{cases}
    \end{equation*}
    (The first step is to study the problem (CDP) without the second line, that is the initial conditions).\\
    We look for solutions assuming that $v,w$ are regular enough, so that the following computations make sense. \\
    If $u(x,t) = w(t) v(x)$ solves the equation, then
    \begin{equation*}
        0 = v(x) w'(t) - w(t) v''(x)
    \end{equation*}
    that is (holding for \emph{any} $x$ and \emph{any} $t$)
    \begin{equation*}
        \frac{w'(t)}{w(t)}=\frac{v''(x)}{v(x)} \quad (\text{when } w(t) \neq 0 \neq v(x))
    \end{equation*}
    This means that 
    \begin{equation*}
        \frac{w'(t)}{w(t)} \quad \text{ and } \quad \frac{v''(x)}{v(x)}
    \end{equation*}
    are constant , say
    \begin{equation*}
        \frac{w'(t)}{w(t)} = \frac{v''(x)}{v(x)} = \lambda
    \end{equation*}
    So we are let to the following problem for $v$:
    \begin{equation*}
    (BVP)
        \begin{cases}
            v''(x)-\lambda v(x) &= 0 \quad \\
            v(0) = v(\pi) &= 0
        \end{cases}
    \end{equation*}
    This equation can be solved, and we have different cases
    \begin{itemize}
        \item [i)] $\lambda > 0$. Setting $\mu = \sqrt{\lambda}$, the solutions are given by 
        \begin{equation*}
            v(x) = c_1 e^{\mu x} + c_2 e^{-\mu x}
        \end{equation*}
        for $c_1,c_2 \in \mR$. \\
        But it is easy to check that the boundary condition $v(0) = v(\pi) = 0 \implies c_1=c_2 = 0$, so in this case $v(x) = 0$ is the only solution. 
        \item [ii)] $\lambda = 0$. The solutions are given by 
        \begin{equation*}
            v(x) = c_1 + c_2 x \quad (c_1,c_2 \in \mR)
        \end{equation*}
        and again the condition $v(0) = v(\pi) = 0$ implies $c_1 = c_2 = 0$.
        \item [iii)] $\lambda < 0$. Setting $\mu = -\sqrt{\lambda}$, we have that the solutions of the equation are given by 
        \begin{equation*}
            v(x) = c_1 \cos \mu x + c_2 \sin \mu x \quad (c_1,c_2 \in \mR)
        \end{equation*}
        From $v(0)= 0$ we get $c_1=0$, hence
        \begin{equation*}
            v(x) = c_2 \sin \mu x 
        \end{equation*}
        From $v(\pi) = 0$ we get 
        \begin{equation*}
            c_2 \sin(\mu \pi) = 0
        \end{equation*}
        Hence, the only case in which the problem has non trivial solutions is when $\mu \in \mathbb{N}$. \\
        Set $\mu = k \in \mathbb{N}$, hence $\lambda = -k^2$ and we can choose 
        \begin{equation*}
            v(x) = v_k(x)=\sin(kx)
        \end{equation*}
        $\lambda_k=-k^2$ is an eigenvalue of problem (BVP) and $v_k(x) = \sin(kx)$ is an eigenfunction of $\lambda_k$. \\
        For any $c \in \mR, c v_k(x)$ is also a solution of (BVP). \\
        Now, from 
        \begin{equation*}
            \frac{w'(t)}{w(t)}=\lambda
        \end{equation*}
        we easily obtain
        \begin{equation*}
            w(t) = ce^{\lambda t} = c e^{-k^2 t}
        \end{equation*}
        Hence, a solution of the heat equation is given by
        \begin{equation*}
            u_k=e^{-k^2 t} \sin(kx) \quad \forall k \in \mathbb{N}
        \end{equation*}
        By linearity, any linear finite combination of these function is still a solution. That is, any function
        \begin{equation*}
            u(x,t) = \sum_{k=1}^m a_k u_k(x,t) = \sum_{k=1}^m a_k e^{-k^2 t} \sin(kx)
        \end{equation*}
        is a solution of the heat equation, satisfying 
        \begin{equation*}
            u(0,t) = u(\pi,t) = 0 \quad \forall t > 0
        \end{equation*}
        Of course, functions of this kind in general do \emph{not} satisfy $u(x,0)=g(x)$ (unless for very particular $g$). \\
        To treat general initial data, we have to use Fourier series. \\
        First, we assume 
        \begin{equation*}
            (H_1) g \in \mathcal{C}^1([0,\pi]), g(0) = g(\pi) = 0
        \end{equation*}
        The idea is to write the solution as a series 
        \begin{equation*}
            u(x,t) = \sum_{k=1}^\infty a_k e^{-k^2 t} \sin(kx)
        \end{equation*}
        Assume for the moment that such $u(x,t)$ is well defined. Hence
        \begin{equation*}
            u(x,0) = \sum_{k=1}^\infty a_k \sin(kx)
        \end{equation*}
        By Fourier series theory, we know that we can write
        \begin{equation*}
            g(x) = \sum_{k=1}^\infty g_k \sin(kx)
        \end{equation*}
        with
        \begin{equation*}
            g_k= \frac{2}{\pi} \int_0^\pi g(x) \sin(kx) dx
        \end{equation*}
        We get $u(x,0) = g(x)$ if and only if $a_k = g_k = \frac{2}{\pi} \int_0^\pi g(x) \sin(kx) dx$. By theory of Fourier series we also know that, assuming $(H_1)$, the function 
        \begin{equation*}
            u(x,t) = \sum_{k=1}^\infty g_k e^{-k^2 t} \sin(kt)
        \end{equation*}
        is a $\mathcal{C}^{2,1}$ function on $[0,\pi] \times (0,+\infty)$, $\mathcal{C}^0$ on $[0,\pi] \times [0,+\infty)$ and it is possible to compute its derivatives term-by-term, that is 
        \begin{equation*}
            (\partial_t-\partial_{xx})u(x,t) = \sum_{k=1}^\infty g_k (\partial_t - \partial_{xx}) e^{-k^2t} \sin(kt) = 0
        \end{equation*}
        hence, the function $u(x,t)$ is a solution of (CDP), that is $u \in \mathcal{C}^{2,1}([0,\pi] \times (0,+\infty)) \cap \mathcal{C}^0([0,\pi] \times [0,+\infty))$
        \begin{equation*}
            \begin{cases}
            u_t - u_{xx} &= 0 \quad \text{ in } [0,\pi] \times (0,+\infty)\\
            u(x,0) &= g(x) \quad \text{ in } [0,\pi] \\
            u(0,t) &= u(\pi,t) = 0 \quad \forall t \geq 0
            \end{cases}
        \end{equation*}
    \end{itemize}
    \item \textbf{Cauchy-Neumann Conditions}
        \begin{equation*}
        (CNP)
            \begin{cases}
                u_t(x,t) - u_{xx}(x,t) & = 0 \quad (x,t) \in (0,\pi) \times (0,+\infty) \\
                u(x,0) &= g(x) \quad x \in [0,\pi]\\
                u_x(0,t)&= u_x(\pi,t) = 0 
            \end{cases}
        \end{equation*}
        We assume 
        \begin{equation*}
            (H_2) g \in \mathcal{C}^1([0,\pi]), g'(0) = g'(\pi) = 0
        \end{equation*}
        We obtain existence of a solution by separation of variables, as before:\\
        we set $u(x,t) = v(x) w(t)$ and we first look for solution of the equation satisfying $u_x(0,t) = u_x(\pi,t) = 0$. \\
        We obtain that $v$ must satisfy the following conditions 
        \begin{equation*}
            \begin{cases}
                v''(x)-\lambda v(x) &= 0\\
                v'(0) = v'(\pi) &= 0 \quad \exists \lambda \in \mR
            \end{cases}
        \end{equation*}
        As before, if $\lambda \geq 0$ we obtain $v=0$. If $\lambda < 0$ and $\mu = \sqrt{- \lambda}$, we obtain 
        \begin{equation*}
            v(x)=c_1 \cos(\mu x) + c_2 \sin (\mu x) \quad (c_1,c_2 \in \mR)
        \end{equation*}
        so that 
        \begin{equation*}
            v'(x)=-c_1 \sin(\mu x) +c_2 \cos (\mu x)
        \end{equation*}
        hence $v'(x)=0 \iff c_2 \mu = 0 \iff c_2 = 0$, so that 
        \begin{equation*}
            v(x)=c_1 \cos (\mu x), \quad v'(x) = -c_1 \mu \sin (\mu x)
        \end{equation*}
        and $v'(x) = 0 \iff -c_1 \mu \sin(\mu \pi) = 0$. The only way to obtain solutions different from zero is to set $\mu = m \in \mathbb{N}$, so $\lambda = - m^2$.\\
        Any linear combination of such solution is still a solution.\\
        On the other hand, for $w(t)$ we obtain
        \begin{equation*}
            w'(t) = -m^2 w(t)
        \end{equation*}
        Hence
        \begin{equation*}
            w(t) = c e^{-m^2 t}
        \end{equation*}
        All the functions of the form
        \begin{equation*}
            u(x,t)=ce^{-m^2 t} \cos (m x)
        \end{equation*}
        are solutions, and all their finite linear combinations such as 
        \begin{equation*}
            u(x,t) = \sum_{k=1}^m a_k e^{-k^2 t} \cos (k x)
        \end{equation*}
        Arguing as before, we look for a soltuion
        \begin{equation*}
            u(x,t)=\sum_{k=1}^\infty a_k e^{-k^2 t} \cos (kx)
        \end{equation*}
        It must be $u(x,0)=g(x)$, hence
        \begin{equation*}
            \sum_{k=0}^\infty a_k \cos(kx) = g(x)
        \end{equation*}
        By Fourier theory we know that it holds
        \begin{equation*}
            g(x)=\frac{1}{2} g_0 + \sum_{k=1}^\infty g_k \cos(kx)
        \end{equation*}
        where
        \begin{equation*}
            g_k = \frac{2}{\pi} \int_0^\pi g(x) \cos(kx) dx
        \end{equation*}
        Hence we can write
        \begin{equation*}
            u(x,t)=\frac{1}{2}g_0 + \sum_{k=1}^\infty g_ke^{-k^2t} \cos (kx)
        \end{equation*}
        We then obtain, by general theory, that this function is $\mathcal{C}^{2,1}((0,+\infty) \times (0,+\infty)) \cap \mathcal{C}^0([0,\pi] \times [0,+\infty))$ and it is a solution of (CNP).
\end{enumerate}



\subsection{A one-dimensional diffusion problem}
There are several types of diffusion problems that can we treated with techniques similar to those used up to now. \\
As an example, we will study the following one-dimensional diffusion problem on a half-line:
\begin{equation}
\label{P}
(P)
    \begin{cases}
        \partial_t u - \partial_{xx} u = 0 &\quad \text{ on } \quad (0,+\infty)^2\\
        u(0,t) = g(t) & \quad (t > 0)\\
        u(x,0) = f(x) & \quad (x  > 0)
    \end{cases}
\end{equation}
assuming $f,g \in \mathcal{C}^0((0,+\infty)) \cap L^\infty((0,+\infty))$.\\
As first thing we split \ref{P} in the following two problems:
\begin{equation}
\label{P1}
(P_1)
    \begin{cases}
        \partial_t u - \partial_{xx} u = 0 &\quad \text{ on } \quad (0,+\infty)^2\\
        u(0,t) = 0 & \quad (t > 0)\\
        u(x,0) = f(x) & \quad (x  > 0)        
    \end{cases}
\end{equation}
and 
\begin{equation}
\label{P2}
(P_2)
    \begin{cases}
        \partial_t u - \partial_{xx} u = 0 &\quad \text{ on } \quad (0,+\infty)^2\\
        u(0,t) = g(t) & \quad (t > 0)\\
        u(x,0) = 0 & \quad (x  > 0)        
    \end{cases}
\end{equation}
It is obvious that, if $u_1$ solves \ref{P1} and $u_2$ solves \ref{P2}, then $u=u_1 + u_2$ solves \ref{P}. 
\begin{ProofBox}
    Let us start with (\ref{P1} - P1).\\
$(P_1)$.  Let us define the function $\hat{f}: \mR \rightarrow \mR$ as follows
\begin{equation*}
\hat{f}(x) = 
    \begin{cases}
        f(x) & x > 0\\
        0 & x = 0\\
        -f(-x) & x < 0
    \end{cases}
\end{equation*}
As  $f \in \mathcal{C}^0((0,+\infty)) \cap L^\infty((0,+\infty))$, it is obvious that $\hat{f} \in L^\infty(\mR) \cap \mathcal{C}^0(\mR \setminus \{0\})$. Let us define 
\begin{equation*}
    u_1 (x,t) = \int_\mR \Gamma(x-y, t) \hat{f}(y) dy = \frac{1}{2 \sqrt{\pi t}} \int_\mR e^{-\frac{|x-y|^2} {4t}} \hat{f}(y) dy 
\end{equation*}
Thanks to our previous general results, we know that $u_1$ satisfies the following properties:
\begin{itemize}
    \item (i) $u_1 \in \mathcal{C}^\infty(\mR \times (0,+\infty))$
    \item (ii) $(\partial_t - \partial_{xx}) u_1 = 0$ in $\mR \times (0,+\infty)$
    \item (iii) $\lim_{(x,t) \rightarrow (x_0,0^+)}u_1(x,t) = \hat{f}(x_0) \quad \forall x_0 \neq 0$
\end{itemize}
Hence, in particular,
\begin{equation*}
\lim_{(x,t) \rightarrow (x_0,0^+)}u_1(x,t) = f(x_0) \quad \forall x_0 > 0   
\end{equation*}
To see what happens when $x = 0$, we argue as follows:
\begin{align*}
    u_1(x,t) = \int_\mR \Gamma(x-y,t) \hat{f}(y) dy &= \int_{- \infty}^0 \Gamma(x-y, t) (-f(-y)) dy + \int_0^{+\infty} \Gamma(x-y, t) f(y) dy \\
    &= \int_{+\infty}^0 \Gamma(x+y, t) f(y) dy + \int_0^{+\infty} \Gamma(x-y, t) f(y) dy \\
    &= \int_0^{+\infty} \Bigg(\Gamma(x-y, t) - \Gamma(x+y, t) \Bigg) f(y) dy 
\end{align*}
Hence
\begin{equation*}
    u_1(0,t) = \int_0^{+\infty} \Bigg(\Gamma(-y,t) - \Gamma(y,t)) f(y) dy = 0
\end{equation*}
because $\Gamma(-y,t) = \Gamma(y,t) \quad \forall (y,t) \in \mR \times (0,+\infty)$. \\
Hence, if we define $u_1(x,0) = f(x) \quad \forall x > 0$, we obtain
\begin{itemize}
    \item $u_1 \in \mathcal{C}^\infty((0,+\infty)^2) \cap \mathcal{C}^0([0,+\infty) \cap (0,+\infty)) \cap \mathcal{C}^0((0,+\infty) \cap [0,+\infty))$
    \item $(\partial_t - \partial_{xx}) u_1 = 0$ in $(0,+\infty)^2$
    \item $u_1(0,t) = 0 \quad \forall t > 0, u_1 (x,0) = f(x) \quad \forall x > 0$.
\end{itemize}
\end{ProofBox}
\begin{ProofBox}
    $(P_2)$. To treat problem $(P_2)$, we start introducing the function
\begin{equation*}
    \Gamma_x(x,t) = \partial_x \Gamma(x,t) = -\frac{1}{4 \sqrt{\pi}} \frac{x}{t^\frac{3}{2}} e{- \frac{x^2}{4t}}, \quad (x,t) \in \mR \times (0,+\infty)
\end{equation*}
Let us define, for $(x,t) \in (0,+\infty)^2$,
\begin{align*}
    u_2(x,t) &= -2 \int_0^t \Gamma_x(x,t-s) g(s) ds \\
    &= \frac{1}{2 \sqrt{\pi}} \int_0^t \frac{x}{(t-s)^\frac{3}{2}} e^{-\frac{x^2}{4 (t-s)}}g(s) ds 
\end{align*}
We are going to prove that $u_2$ solves (\ref{P2} - P2). This will be done through several steps.
\end{ProofBox}
\begin{itemize}
    \item \underline{Step 1}
    \begin{PropBox}
        \begin{Proposition}
            \begin{equation*}
                \partial_x u_2(x,t) = -2 \int_0^t \partial_x \Gamma_x(x,t-s) g(s) ds 
            \end{equation*}
        \end{Proposition}
    \end{PropBox}
    \begin{ProofBox}
    \begin{proof}
       We fix $x > 0, t  > 0$ and pick small $h$'s:
       \begin{align*}
           \frac{u_2(x+h,t) - u_2(x,t)}{h} &= -2 \int_0^t\frac{\Gamma_x(x+h,t-s) - \Gamma_x (x,t-s)}{h}g(s)ds \\
           &= -2 \int_0^t \partial_x \Gamma_x(x+\theta, t-s) g(s) ds 
       \end{align*}
       where $\theta = \theta(h)$ and $|\theta| \leq |h|$. Hence, we have:
       \begin{align*}
           & \Big|\frac{u_2(x+h,t)-u_2(x,t)}{h} - (-2) \int_0^t\partial_x\Gamma_x (x,t-s)g(s)ds \Big| \\
           &= 2 \Big|\int_0^t \Bigg(\partial_x \Gamma_x(x+\theta, t-s) - \partial_x \Gamma_x(x, t-s)\Bigg) g(s) ds \Big| \\
           &\leq 2 M |\theta| \int_0^t |\partial_{xx} \Gamma_x(x + \theta_1, t-s)| ds 
       \end{align*}
       where $\theta_1 = \theta_1(h)$ and $|\theta_1| \leq |\theta| \leq |h|$. \\
       We know that $\partial_{xx} \Gamma(x,t)$ is a finite sum of terms like 
       \begin{equation*}
           a \frac{x^k}{t^\frac{m}{2}} e^{-\frac{x^2}{4t}}
       \end{equation*}
       Hence, $\partial_{xx} \Gamma_x(x+\theta_1,t-s)$ is a finite sum of terms like
       \begin{equation*}
           a \frac{(x+\theta_1)^k}{(t-s)^\frac{m}{2}}e^{-\frac{(x+\theta_1)^2}{4 (t-s)}}
       \end{equation*}
       As $x > 0$, we can pick $h$ small enough such that 
       \begin{equation*}
           0 < \frac{x}{2} < x + \theta_1 < 2x 
       \end{equation*}
       so that 
       \begin{equation*}
           \Big|a \frac{(x+\theta_1)^k}{(t-s)^\frac{m}{2}}e^{-\frac{(x+\theta_1)^2}{4 (t-s)}} \Big| \leq |a| 2^k x^k \frac{e^{-\frac{x^2}{16(t-s)}}}{(t-s)^\frac{m}{2}}
       \end{equation*}
       The right-hand side is independent of $h$ and, as a function of $s$, is bounded on $[0,t)$. Hence
       \begin{equation*}
           \int_0^t \Big|a \frac{(x+\theta_1)^k}{(t-s)^\frac{m}{2}}e^{-\frac{(x+\theta_1)^2}{4 (t-s)}} \Big| ds \leq c
       \end{equation*}
       where $c$ is a constant independent of $h$, so that also
       \begin{equation*}
           \int_0^t |\partial_{xx} \Gamma_x(x+\theta_1,t-s)|ds \leq c'
       \end{equation*}
       where $c'$ is a constant independent of $h$. \\
       We then obtain 
       \begin{equation*}
           \lim_{h \rightarrow 0} 2 M |\theta| \int_0^t |\partial_{xx} \Gamma_x(x+\theta_1,t-s)| ds = 0
       \end{equation*}
       Hence
       \begin{equation*}
           \lim_{h \rightarrow 0} \Big| \frac{u_2 (x+h,t)-u_2(x,t)}{h} - (-2) \int_0^t \partial_x \Gamma_x(x,t-s) g(s) ds \Big| = 0
       \end{equation*}
       That is 
       \begin{equation*}
           \lim_{h \rightarrow 0} \Big| \frac{u_2 (x+h,t)-u_2(x,t)}{h} = -2 \int_0^t \partial_x \Gamma_x(x,t-s) g(s) ds
       \end{equation*}
       or 
       \begin{equation*}
           \partial_x u_2(x,t) = -2 \int_0^t \partial_x \Gamma_x(x,t-s) g(s) ds
       \end{equation*}
    \end{proof}
\end{ProofBox}
\item \underline{Step 2}
\begin{PropBox}
    \begin{Proposition}
        \begin{equation*}
            \partial_{xx} u_2(x,t) = -2 \int_0^t \partial_{xx} \Gamma_x(x,t-s) g(s) ds
        \end{equation*}
    \end{Proposition}
\end{PropBox}
\begin{ProofBox}
        \begin{proof}
            The argument is exactly the same as in Step 1, and we skip it. 
        \end{proof}
    \end{ProofBox}
\item \underline{Step 3}
\begin{PropBox}
    \begin{Proposition}
        \begin{equation*}
            \partial_t u_2(x,t) = -2 \int_0^t \partial_t \Gamma_x(x,t-s) g(s) ds 
        \end{equation*}
    \end{Proposition}
    \end{PropBox}
    
\section{Lecture 11}

    \begin{ProofBox}
        \begin{proof}
            The argument is a little more complicated than the previous one. Fix, as before, $x > 0, t > 0$ and pick small $h$'s. 
            \begin{align*}
                \frac{u_2(x,t+h)-u_2(x,t)}{h} &= -\frac{2}{h} \Bigg[\int_0^{t+h} \Gamma-x(x,t+h-s)g(s) ds - \int_0^t \Gamma_x(x,t-s) g(s) ds \Bigg]\\
                &= -\frac{2}{h} \Bigg[ \int_0^t \Bigg(\Gamma_x(x,t+h-s)-\Gamma_x(x,t-s)\Bigg) g(s) + \int_t^{t+h} \Gamma_x(x,t+h-s) g(s) ds \Bigg]\\
                &= -2 [I_1 + I_2]
            \end{align*}
            where 
            \begin{equation*}
            \begin{split}
                 I_1 &= I_1(h) = \int_0^t \frac{\Gamma_x(x,t+h-s) - \Gamma_x(x,t-s)}{h} g(s) ds \\
                 I_2&= I_2 (h) = \frac{1}{h}\int_t^{t+h} \Gamma_x(x,t+h-s) g(s) ds 
            \end{split}
            \end{equation*}
            $I_1$ can be studied as the previous ones:
            \begin{equation*}
                I_1 = \int_0^t \partial_x \Gamma(x,t+\theta -s) g(s) ds 
            \end{equation*}
            where $\theta = \theta(h)$ and $|\theta| \leq |h|$, so that 
            \begin{align*}
                \Big|I_1(h) - \int_0^t \partial_t \Gamma_x(x,t-s) g(s) ds \Big| &= \Big|\int_0^t \Bigg(\partial_t \Gamma_x (x,t+\theta-s) - \partial_t \Gamma_x(x,t-s)\Bigg) g(s) ds \Big|\\
                &\leq M |\theta| \int_0^t |\partial_{tt} \Gamma_x (x,t+\theta_1 - s)| ds 
            \end{align*}
            where $\theta_1 = \theta_1(h)$ and $|\theta_1| \leq |\theta| \leq |h|$. \\
            Let us recall that $\Gamma$ and all its derivatives are $0$ on $(0,+\infty) \times (-\infty,0]$. \\
            On the other hand, $\partial_{tt} \Gamma(x,t)$ is a finite sum of terms as 
            \begin{equation*}
                a \frac{x^k}{t^\frac{m}{2}}e^{-\frac{x^2}{4t}}
            \end{equation*}
            Hence, as a function of $t$ (and fixed $x$) is bounded in $(0,+\infty)$. 
            As a consequence, for fixed positive $x$, the function 
            \begin{equation}
                t \mapsto \partial_{tt}\Gamma_x(x,t)
            \end{equation}
            is bounded on $\mR$. Hence
            \begin{equation*}
                \int_0^t |\partial_{tt}\Gamma_x(x,t+\theta_1-s)|ds \leq c \cdot t 
            \end{equation*}
            and
            \begin{equation*}
                M \theta| \int_0^t |\partial_{tt}\Gamma_x(x,t+\theta_1 - s)| ds \leq c t M |\theta| \rightarrow 0
            \end{equation*}
            as $h \rightarrow 0$. Hence
            \begin{equation*}
                \lim_{h \rightarrow 0} I_1(h) = \int_0^t \partial_t \Gamma_x(x,t-s) g(s) ds 
            \end{equation*}
            Let us now deal with $I_2$: the claim is that 
            \begin{equation*}
                \lim_{h \rightarrow 0} I_2(h) = 0
            \end{equation*}
            Notice that, if $h < 0$, then when $s \in (t+h,t)$ is is $t+h-s < 0$. hence 
            \begin{equation*}
                \Gamma_x(x,t+h-s) = 0
            \end{equation*}
            and 
            \begin{equation*}
                \int_t^{t+h} \Gamma_x(x,t+h-s) g(s) = 0
            \end{equation*}
            that is $I_2(h) = 0$ if $h < 0$.
            Hence, we have to prove the claim for $h > 0$, that is, we have to prove
            \begin{equation*}
                \lim_{h \rightarrow 0^+} I_2(h) = 0
            \end{equation*}
            Let us write:
            \begin{align*}
                I_2 &= \frac{1}{h} \int_t^{t+h} \Gamma_x(x,t+h-s)g(s) ds \\
                &= \frac{1}{h} \int_0^h \Gamma_x(x,s) g(x,t+h-s) ds
            \end{align*}
            so that 
            \begin{equation*}
                |I_2| \leq \frac{1}{h} M \int_0^h |\Gamma_x(x,s)| ds = \frac{1}{h} M \frac{x}{4 \sqrt{\pi}} \int_0^h \frac{1}{s^\frac{3}{2}}e^{-\frac{x^2}{4s}}ds
            \end{equation*}
            We have
            \begin{equation*}
                \frac{d}{ds} \Bigg(\frac{1}{s^\frac{3}{2}}e^{-\frac{x^2}{4s}}\Bigg) = \frac{1}{s^\frac{7}{2}} e^{-\frac{x^2}{4s}} (-\frac{3}{2}s + \frac{x^2}{4})
            \end{equation*}
            so this derivative is positive in $(0,h)$ for small $h$'s. \\
            Hence, in $(0,h)$
            \begin{equation*}
                \frac{1}{s^\frac{3}{2} e^{-\frac{x^2}{4s}}} \leq \frac{1}{h^\frac{3}{2}}e^{-\frac{x^2}{4h}}
            \end{equation*}
            and
            \begin{align*}
                \frac{1}{h} M \frac{x}{4 \sqrt{\pi}} \int_0^h \frac{1}{s^\frac{3}{2}} e^{-\frac{x^2}{4s}} ds & \leq \frac{1}{h} M \frac{x}{4 \sqrt{\pi}} \int_0^h \frac{1}{h^\frac{3}{2}} e^{-\frac{x^2}{4h}} \\
                &= \frac{Mx}{4 \sqrt{\pi}} \frac{1}{h^\frac{3}{2}}e^{-\frac{x^2}{4h}} \rightarrow 0 \quad h \rightarrow 0^+
            \end{align*}
            hence 
            \begin{equation*}
                \lim_{h \rightarrow 0^+} I_2(h) = 0
            \end{equation*}
            and the claim is proved. \\
            To conclude the proof of Step 3, we write:
            \begin{align*}
                \partial_t u_2(x,t) &= \lim_{h \rightarrow 0^+} \frac{u_2(x,t+h)-u_2(x,t)}{h}\\
                &= (-2) \lim_{h \rightarrow 0^+} (I_1(h) + I_2(h)) \\
                &= -2 \int_0^t \partial_t \Gamma_x(x,t-s) g(s) ds  
            \end{align*}
        \end{proof}
    \end{ProofBox}
    \item \underline{Step 4}
    \begin{PropBox}
        \begin{Proposition}
            \begin{equation*}
                (\partial_t - \partial_{xx})u_2(x,t) = 0 \quad (x > 0, t > 0)
            \end{equation*}
        \end{Proposition}
    \end{PropBox}
    \begin{ProofBox}
        \begin{proof}
            \begin{align*}
                \partial_t u_2(x,t) - \partial_{xx}u_2(x,t) &= -2 \int_0^t \partial_t \Gamma_x(x,t-s) g(s) ds - (-2) \int_0^t \partial_{xx} \Gamma_x(x,t-s)g(s) ds \\
                &= -2 \int_0^t (\partial_t - \partial_{xx})\Gamma_x(x,t-s) g(s) ds = 0
            \end{align*}
        \end{proof}
    \end{ProofBox}
    \item \underline{Step 5}
    \begin{PropBox}
        \begin{Proposition}
            \begin{equation*}
                \lim_{(x,t) \ra (x_0,0^+)} u_2(x,t) = 0 \quad (x_0 > 0)
            \end{equation*}
        \end{Proposition}
    \end{PropBox}
    \begin{ProofBox}
        \begin{proof}
            As $x_0 > 0$, we can assume $0 < c_1 < x$. Hence
            \begin{equation*}
                |u_2(x,t)| \leq c_2 \int_0^t \frac{1}{(t-s)^\frac{3}{2}} e^{-\frac{c_1^2}{(t-s) \cdot 4}}ds
            \end{equation*}
            But the function $t \mapsto \frac{1}{t^\frac{3}{2}} e^{-\frac{c^2_1}{4t}}$ is bounded in $(0,+\infty)$, hence
            \begin{equation*}
                |u_2(x,t)| \leq c_2 \int_0^t c_3 ds = c_2 c_3 t
            \end{equation*}
            so that $|u_2(x,t)| \ra 0$ as $ t \ra 0^+$ (and $x \ra x_0$). 
        \end{proof}
    \end{ProofBox}
    \item \underline{Step 6}
    \begin{PropBox}
        \begin{Proposition}
            \begin{equation*}
                \lim_{(x,t) \ra (0^+, t_0)} u_2(x,t) = g(t_0) \quad (t_0 > 0)
            \end{equation*}
        \end{Proposition}
    \end{PropBox}
    \begin{ProofBox}
        \begin{proof}
            We have
            \begin{equation*}
                u_2(x,t) = \frac{1}{2\sqrt{\pi}} \int_0^t \frac{x}{(t-s)^\frac{3}{2}} e^{-\frac{x^2}{4(t-s)}}g(s) ds 
            \end{equation*}
            We set $\sigma=\frac{x}{2 \sqrt{t-s}}$ so that 
            \begin{equation*}
                s = -\frac{1}{\sigma^2} \frac{x^2}{4}+t \hspace{2 cm} ds = \frac{1}{\sigma^3} \frac{x^2}{2} d\sigma
            \end{equation*}
            and 
            \begin{equation*}
                \frac{1}{(t-s)^\frac{3}{2}} = 8 \frac{\sigma^3}{x^3}
            \end{equation*}
            We then get 
            \begin{align*}
                u_2(x,t) &= \frac{1}{2 \sqrt{\pi}} \int_{\frac{x}{2 \sqrt{t}}}^{+\infty} x 8 \frac{\sigma^3}{x^3} e^{-\sigma^2} \frac{1}{\sigma^3} \frac{x^2}{2}g(-\frac{1}{\sigma^2}\frac{x^2}{4} + t) d\sigma\\
                &= \frac{1}{\sqrt{\pi}}\int_0^{+\infty} e^{-\sigma^2} g(-\frac{1}{\sigma^2}\frac{x^2}{4}+t) \chi(\sigma) d\sigma
            \end{align*}
            where the characteristic function of the interval $[\frac{x}{2 \sqrt{t}}, +\infty)$
            \begin{equation*}
            \chi(\sigma) = \chi_{x,t}(\sigma) =
                \begin{cases}
                    1 & \quad \sigma > \frac{x}{2 \sqrt{t}}\\
                    0 & \quad \sigma \in (0,\frac{x}{2 \sqrt{t}}]
                \end{cases}
            \end{equation*}
            We have 
            \begin{equation*}
                \begin{split}
                    &|e^{-\sigma^2} g(-\frac{1}{\sigma^2}\frac{x^2}{4}+t) \chi(\sigma)| \leq M e^{-\sigma^2} \in L^1(0,+\infty)\\
                    & e^{-\sigma^2} g(-\frac{1}{\sigma^2}\frac{x^2}{4}+t) \chi_{x,t}(\sigma)\ra e^{-\sigma^2}g(t_0)
                \end{split}
            \end{equation*}
            as $(x,t) \ra (0^+,t_0)$, with fixed value for $\sigma$. \\
            Since we have pointwise convergence, dominated by an $L^1$ function, we can apply the Dominated Convergence Theorem. \\
            By the Dominated Convergence Theorem, we obtain
            \begin{equation*}
                \frac{2}{\sqrt{\pi}} \int_0^{+\infty} e^{-\sigma^2} g(-\frac{1}{\sigma^2}\frac{x^2}{4}+t) \chi(\sigma) d\sigma \ra \frac{2}{\sqrt{\pi}} \int_0^{+\infty} e^{-\sigma^2}g(t_0) d\sigma = g(t_0) \frac{2}{\sqrt{\pi}} \int_0^{+\infty} e^{-\sigma^2}d\sigma = g(t_0)
            \end{equation*}
            and this proves the thesis. 
        \end{proof}
    \end{ProofBox}
\end{itemize}


\subsection{Maximum Principle and Uniqueness}
\begin{DefBox}
    \begin{Def}
        \begin{itemize}
            \item A domain $\Omega \subseteq \mR^n$ is an open connected subset of $\mR^n$.
            \item Let $\Omega$ be a bounded domain in $\mR^n$ and $T>0$. We define the space-time cylinder $Q_T$ as follows
            \begin{equation*}
                Q_T = \Omega \times (0,T) \subseteq \mR^{n+1}
            \end{equation*}
            and its parabolic boundary 
            \begin{equation*}
                \delta_pQ_T = [\Bar{\Omega} \times \{0\}] \cup [\delta \Omega \times (0,T]]
            \end{equation*}
            \item Let $A \subseteq \mR^{n+1}$ be an open set. We define $\mathcal{C}^{2,1}(A)$ the set of functions $u = u(x,t): A \ra \mR$ such that all the derivatives 
            \begin{equation*}
                \{u_t, D_{x_i}u, D_{x_ix_j}u\} \quad (i,j = 1,\ldots,n) 
            \end{equation*}
            exist and are continuous (together with $u$). 
            \item A function $w \in \mathcal{C}^{2,1}(Q_T)$ is a \emph{subsolution} of the diffusion equation if 
            \begin{equation*}
                w_t - \Delta w \leq 0 \quad \text{ in } Q_T
            \end{equation*}
            It is a \emph{supersolution} if
            \begin{equation*}
                w_t - \Delta w \geq 0 \quad \text{ in } Q_T
            \end{equation*}
        \end{itemize}
    \end{Def}
\end{DefBox}
\begin{ThBox}
    \begin{Th}[Maximum Principle for the Diffusion Equation]
    Let $Q_T$ be as before, and $u \in \mathcal{C}^{2,1}(Q_T) \cap \mathcal{C}^0(\Bar{Q}_T)$ be a subsolution of the diffusion equation in $Q_T$. Then
    \begin{equation*}
        \max_{\Bar{Q}_T} u= \max_{\delta_p Q_T} u 
    \end{equation*}
    \end{Th}
\end{ThBox}
Before the proof, some remarks.
\begin{remark}
    \begin{itemize}
        \item $\Bar{Q}_T$ and $\delta_p Q_T$ are compact and $u \in \mathcal{C}^0(Q_T)$, so $
        \max_{\Bar{Q}_T}u$ and $\max_{\delta_p Q_T} u$ indeed exist. 
        \item It is $\delta_p Q_T\subseteq \Bar{Q}_T$, so of course
        \begin{equation*}
            \max_{\Bar{Q}_T} u \geq \max_{\delta_p Q_T} u 
        \end{equation*}
        \item In the proof of the theorem, we will use the following obvious fact:
        \begin{PropBox}
            \begin{Proposition}
                If $\max_{\Bar{Q}_T} u = u(x_0,t_0)$ and $(x_0,t_0) \in Q_T$, then $u_t(x_0,t_0) = 0$ and
                \begin{equation*}
                    \frac{\partial^2}{\partial x_i^2} (x_0,t_0) \leq 0
                \end{equation*}
                Hence $\Delta u(x_0,t_0) \leq 0$ and 
                \begin{equation*}
                    (u_t-\Delta u)(x_0,t_0) \geq 0
                \end{equation*}
            \end{Proposition}
        \end{PropBox}
    \end{itemize}
\end{remark}

\begin{ProofBox}
    \begin{proof}
        Fix $\epsilon \in (0,T)$ and define $u_\epsilon=u-\epsilon t$. We have
        \begin{equation*}
            (\partial_t - \Delta) u_\epsilon =(\partial_t - \Delta) u - \epsilon \leq -\epsilon < 0 \quad\text{ in } Q_T
        \end{equation*}
        Let $M_\epsilon = \max_{\Bar{Q_{T-\epsilon}}} u_\epsilon = u_\epsilon (x_\epsilon, t_\epsilon)$. We claim that 
        \begin{equation*}
            (x_\epsilon, t_\epsilon) \in \partial_p Q_{T-\epsilon}
        \end{equation*}
        It cannot be $(x_\epsilon, t_\epsilon) \in Q_{T-\epsilon}$: if this happens, then $\partial_t - \Delta) u_\epsilon(x_\epsilon, t_\epsilon) \geq 0$, which is a contradiction. \\
        But it cannot be $t_\epsilon = T-\epsilon$ and $x_\epsilon \in \Omega$: if this happens, then again $\Delta u_\epsilon (x_\epsilon, t_\epsilon) \leq 0$, $\frac{\partial u_\epsilon}{\partial t}(x_\epsilon, t_\epsilon) \geq 0$, so again $(\partial_t - \Delta)u_\epsilon(x_\epsilon, t_\epsilon) \geq 0$, which is a contradiction. \\
        We have proved the claim. We then have
        \begin{equation*}
            \max_{\Bar{Q_{T-\epsilon}}} u_\epsilon = \max_{\partial_pQ_{T-\epsilon}} u_\epsilon  \leq \max_{\partial_pQ_T u_\epsilon}
        \end{equation*}
        because $\partial_p Q_{T-\epsilon} \subseteq \partial_p Q_T$. Now, we notice that by definition, it is
        \begin{equation*}
            u_\epsilon \leq u = u_\epsilon + \epsilon t
        \end{equation*}
        Hence
        \begin{equation*}
            \max_{Q_{T-\epsilon}} u_\epsilon \leq \max_{\Bar{Q_{T-\epsilon}}} u_\epsilon + \epsilon T \leq \max_{\partial_pQ_{T-\epsilon}} u_\epsilon + \epsilon T \leq \max_{\partial_pQ_{T-\epsilon}} u + \epsilon T
        \end{equation*}
        Now, assume 
        \begin{equation*}
            \max_{\Bar{Q_T}}u = u(x_0,t_0)
        \end{equation*}
        We have three possibilities:
        \begin{enumerate}
            \item $(x_0,t_0) \in \partial_p Q_T$: in this case 
            \begin{equation*}
                \max_{\Bar{Q}_T} u \leq \max_{\partial_p Q_T} u
            \end{equation*}
            and we have done.
            \item $(x_0,t_0) \in Q_T$: in this case $t_0 < T$ and for all $\epsilon \in (0,T-t_0)$ we have $(x_0,t_0) \in Q_{T-\epsilon}$. Hence
            \begin{equation*}
                u(x_0,t_0) \leq \max_{\Bar{Q}_{T-\epsilon}} u \leq \max_{\partial_p Q_T} u + \epsilon T
            \end{equation*}
            As this holds for all $\epsilon \in (0,T-t_0)$, we let $\epsilon \ra 0^+$ and we obtain $u(x_0,t_0) \leq \max_{\partial_t Q_T}u$, that is 
            \begin{equation*}
                \max_{\Bar{Q}_T} u\leq \max_{\partial_pQ_T}u
            \end{equation*}
            \item $t_0 = T$ and $x_0 \in \Omega$. In this case we tale $\epsilon \in (0,T)$ and we consider $(x_0, T-\epsilon) \in \Bar{Q_{T-\epsilon}}$. We know that 
            \begin{equation*}
                u(x_0, T-\epsilon) \leq \max_{\Bar{Q}_{T-\epsilon}} u \leq \max_{\partial_p Q_T} u + \epsilon T
            \end{equation*}
            We let $\epsilon \ra 0^+$. As $u \in \mathcal{C}(\Bar{Q}_t)$, we have 
            \begin{equation*}
                \lim_{\epsilon \ra 0^+} u(x_0,T-\epsilon) = u(x_0,T) = u(x_0,t_0)
            \end{equation*}
            Hence
            \begin{equation*}
                \max_{\Bar{Q}_T} u = u(x_0,T) = \lim_{\epsilon \ra 0^+} u(x_0, T-\epsilon) \leq \max_{\partial_t Q_T}
            \end{equation*}
            So, in any case we obtain
            \begin{equation*}
                \max_{\Bar{Q}_T} u \leq \max_{\partial_p Q_T} u
            \end{equation*}
        \end{enumerate}
    \end{proof}
\end{ProofBox}
There is a version of this theorem for supersolution.
\begin{ThBox}
    \begin{Th}
        Let $Q_T$ be as before, and $u \in \mathcal{C}^{2,1}(Q_T) \cap \mathcal{C}^0(\Bar{Q}_T)$ be a supersolution of the diffusion equation in $Q_T$. Then
        \begin{equation*}
            \min_{\Bar{Q}_T} = \min_{\partial_t Q_T} u
        \end{equation*}
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        It is easy to adapt the previous proof to this case, or also to set $v=-u$, to notice that $v$ is a subsolution and to apply the previous theorem. (Homework)
    \end{proof}
\end{ProofBox}
Let us see some relevant corollaries of the previous theorems. 
\begin{PropBox}
    \begin{Cor}[Corollary 1]
        \label{cor 1}
        $Q_T$ as before, $u \in \mathcal{C}^{2,1}(Q_T) \cap \mathcal{C}^0(\Bar{Q}_T)$ satisfies 
        \begin{equation*}
            (\partial_t - \Delta) u = 0 \quad \text{ in } Q_T
        \end{equation*}
        Hence
        \begin{equation*}
        \min_{\partial_p Q_T} \leq u(x,t) \leq \max_{\partial_p Q_T} u \quad \forall (x,t) \in \Bar{Q}_T
        \end{equation*}
    \end{Cor}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
        A solution of $(\partial_t - Delta) u = 0$ is both a subsolution and a supersolution.
    \end{proof}
\end{ProofBox}
\begin{PropBox}
    \begin{Cor}[Corollary 2 - Uniqueness]
    \label{cor 2}
        $Q_T$ as before; assume $f \in \mathcal{C}^0 (Q_T), g \in \mathcal{C}^0(\partial_p Q_T)$. Consider the problem
        \begin{equation*}
        (PBP)
            \begin{cases}
                \partial_t u - \Delta u &= f \quad \text{ in } Q_T\\
                u&=g \quad \text{ on } \partial_p Q_T
            \end{cases}
        \end{equation*}
        Then, (PBP) has at most one solution $u \in \mathcal{C}^{2,1}(Q_T) \cap \mathcal{C}^0(\Bar{Q}_T)$.
    \end{Cor}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
        Assume that we have two solutions $u_1, u_2$. Define $v = u_2 - u_1$. Hence $v$ is a solution of the problem
        \begin{equation*}
            \begin{cases}
                \partial_t v - \Delta v &= 0 \quad \text{ on } Q_T \\
                v &= 0 \quad \text{ in }  \partial_p Q_T
            \end{cases}
        \end{equation*}
        Applying \ref{cor 1} we get 
        \begin{equation*}
            \min_{ \partial_p Q_T} v \leq v(x,t) \leq \max_{ \partial_p Q_T} v \quad \text{ in } Q_T
        \end{equation*}
        that is 
        \begin{equation*}
            0 \leq v(x,t) \leq 0 \quad \forall (x,t) \in Q_T
        \end{equation*}
        that is 
        \begin{equation*}
            v(x,t) = 0
        \end{equation*}
        Hence,
        \begin{equation*}
            u_1(x,t) = u_2(x,t), \quad \forall (x,t) \in Q_T
        \end{equation*}
    \end{proof}
\end{ProofBox}
\begin{PropBox}
    \begin{Cor}[Corollary 3 - Comparison and Stability]
        $Q_T$ as before. Assume $f_1,f_2 \in \mathcal{C}^0(Q_T)$, bounded in $Q_T$. Assume that $u_1,u_2 \in \mathcal{C}^{2,1}(Q_T) \cap \mathcal{C}^0(\Bar{Q}_T)$ are solutions of 
        \begin{equation*}
            (\partial_t - \Delta) u_1 = f_1 \quad (\partial_t - \Delta)u_2 = f_2 \quad \text{ in } Q_T
        \end{equation*}
        Then, 
        \begin{itemize}
            \item if $u_1 \geq u_2$ on $ \partial_p Q_T$ and $f_1 \geq f_2$ in $Q_T$. Then $u_1 \geq u_2$ in all $Q_T$.
            \item The following holds
            \begin{equation*}
                \max_{\Bar{Q}_T} |u_1 - u_2| \leq \max_{ \partial_p Q_T} |u_1 - u_2 | + T \sup_{Q_T} |f_1-f_2|
            \end{equation*}
        \end{itemize}
    \end{Cor}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
        \begin{itemize}
            \item Define $w=u_1-u_2$. Then, we have
            \begin{equation*}
                (\partial_t - \Delta)w = (\partial_t - \Delta) u_1-(\partial_t -\Delta)u_2 = f_1 - f_2 \geq 0 \quad \text{ in }Q_T
            \end{equation*}
            Hence
            \begin{equation*}
                \min_{\Bar{Q}_T} w = \min_{ \partial_p Q_T} w \geq 0
            \end{equation*}
            because 
            \begin{equation*}
                u_1 \geq u_2 \quad \text{ on } \partial_p Q_T
            \end{equation*}
            So, $u_1 \geq u_2 $ in all $\Bar{Q}_T$.
            \item Let $M=\sup_{Q_T} |f_1 - f_2|$. Define $w=u_1-u_2 - Mt$. Hence
            \begin{equation*}
                (\partial_t - \Delta) w = (\partial_t - \Delta)u_1- (\partial_t - \Delta) u_2 - M = f_1 - f_2 - M \leq |f_1 - f_2| - M \leq 0
            \end{equation*}
            This implies 
            \begin{equation*}
                \max_{\Bar{Q}_T} w = \max_{ \partial_p Q_T} w \leq \max_{ \partial_p Q_T} (u_1 - u_2) \leq \max_{ \partial_p Q_T} |u_1-u_2|
            \end{equation*}
            Now, we have
            \begin{equation*}
                u_1 - u_2 = w + Mt \leq w + MT
            \end{equation*}
            Hence, 
            \begin{equation*}
                \max_{\Bar{Q}_T}(u_1- u_2) \leq \max_{\Bar{Q}_T} w + MT \leq \max_{ \partial_p Q_T} |u_1 - u_2| + MT
            \end{equation*}
            that is, in all $\Bar{Q}_T$ it holds
            \begin{equation*}
                u_1-u_2 \leq \max_{ \partial_p Q_T} |u_1-u_2| + \sup_{Q_T} |f_1-f_2| T
            \end{equation*}
            But the argument holds, in the same way, for the function $w = u_2 - u_1 - Mt$, so we obtain
            \begin{equation*}
                u_2-u_1 \leq \max_{ \partial_p Q_T} |u_1-u_2| + \sup_{Q_T} |f_1-f_2| T
            \end{equation*}
            in all $\Bar{Q}_T$, hence in all $\Bar{Q}_T$, it holds
            \begin{equation*}
                |u_1-u_2| \leq \max_{ \partial_p Q_T} |u_1-u_2| + T \sup_{Q_T} |f_1-f_2|
            \end{equation*}
            that is 
            \begin{equation*}
                \max_{\Bar{Q}_T}  |u_1-u_2| \leq \max_{ \partial_p Q_T} |u_1 - u_2| + T \sup_{Q_T} |f_1-f_2|
            \end{equation*}
            which is the thesis. 
        \end{itemize}
    \end{proof}
\end{ProofBox}
We have proved maximum principle for bounded domains. \\
We want now to prove a maximum principle for unbounded domains, in particular, for a Cauchy problem. From the maximum principle, we will obtain an uniqueness result for such problems. As we have already proven an existence result, in the end we obtain an existence and uniqueness result. \\
For $T > 0$, define $D_T = \mR^n \times (0,T), D_0 = \mR^n \times \{0\}$. Of course, $D_{T_J} D_0 \subseteq \mR^{n+1}$. 
\begin{ThBox}
    \begin{Th}[Maximum Principle for the Cauchy problem]
        Let $T > 0, g \in \mathcal{C}^0(D_0) \cap L^\infty(D_0)$, and $u \in \mathcal{C}^{2,1}(D_T) \cap \mathcal{C}^0(\Bar{D}_T)$ be a solution of the problem
        \begin{equation*}
            \begin{cases}
                \partial_t u - \Delta u &= 0 \quad \text{ in } D_T \\
                u &= g \quad \text{ in } D_0
            \end{cases}
        \end{equation*}
        Assume that $u$ satisfies the estimate
        \begin{equation*}
            u(x,t) \leq A e^{a |x|^2} \quad \forall (x,t) \in \Bar{D}_T
        \end{equation*}
        where $A,a > 0$. Then
        \begin{equation*}
            \sup_{\Bar{D}_T}u = \sup_{\mR^n} g
        \end{equation*}
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        We first assume $4aT < 1$, so we can fix $\epsilon > 0$ such that $4a(T+\epsilon) < 1$. \\
        We now fix $y \in \mR^n, \mu > 0$ and we define 
        \begin{equation*}
            v(x,t) = u(x,t) - \frac{\mu}{(T+\epsilon - t)^\frac{n}{2}}e^{-\frac{|x-y|^2}{4(T+\epsilon-t)}}
        \end{equation*}
        We know that 
        \begin{equation*}
            \partial_t - \Delta) v = 0 \quad \text{ in } D_T
        \end{equation*}
        For any fixed $r > 0$ we set 
        \begin{equation*}
            Q_T = B(y,r) \times (0,T) \subseteq Q_T
        \end{equation*}
        hence, we have $v \in \mathcal{C}^{2,1}(Q_T) \cap \mathcal{C}^0(\Bar{Q}_T)$ and 
        \begin{equation*}
            \max_{\Bar{Q}_T} v = \max_{ \partial_p Q_T} v
        \end{equation*}
        We now check what happens on $ \partial_p Q_T$: when $t= 0$, we have
        \begin{equation*}
            v(x,0) = u(x,0) -\frac{\mu}{(T+\epsilon)^\frac{n}{2}} e^{-\frac{|x-y|^2}{4(T+\epsilon)}}\leq u(x,0) = g(x) \leq \sup_{\mR^n}g
        \end{equation*}
        Next we have to check the case $|x-y|=r$ and \begin{equation*}
            v(x,t) = u(x,t) - \frac{\mu}{(T+\epsilon - t)^\frac{n}{2}} e^{-\frac{|x-y|^2}{4(T+\epsilon - t)}} \leq A e^{a |x|^2} - \frac{\mu}{(T+\epsilon - t)^\frac{n}{2}} e^{-\frac{|x-y|^2}{4(T+\epsilon - t)}} 
        \end{equation*}
        from $|x-y|=r$ we deduce 
        \begin{equation*}
            |x| = |x-y+y| \leq |x-y| + |y| = r + |y|
        \end{equation*}
        Hence,
        \begin{equation*}
            |x|^2 \leq (|y|+r)^2
        \end{equation*}
        and
        \begin{equation*}
            v(x,t) \leq Ae^{a (|y| +r)^2} - \frac{\mu}{(T+\epsilon - t)^\frac{n}{2}} e^{-\frac{|x-y|^2}{4(T+\epsilon - t)}} \leq A e^{a(|y|+r)^2} - \frac{\mu}{(T+\epsilon - t)^\frac{n}{2}} e^{-\frac{|x-y|^2}{4(T+\epsilon - t)}}
        \end{equation*}
        as $\frac{1}{4(T+\epsilon)} > a$, we have 
        \begin{equation*}
            \frac{1}{4(T+\epsilon)} = a + \gamma, \quad \exists \gamma > 0
        \end{equation*}
        Hence,
        \begin{equation*}
            v(x,t) \leq A e^{a r^2 + 2 a |y| r + a |y|^2} - \frac{\mu}{(T+\epsilon)^\frac{n}{2}} e^{ar^2+\gamma r^2} = e^{a r^2 + \gamma r^2} (-\frac{\mu}{(T+\epsilon )^\frac{n}{2}} + A e^{-\gamma r^2 + 2 a |y| r + a |y|^2}) = \sigma(r)
        \end{equation*}
        If $\gamma, T, \epsilon, A, a, \mu$ are fixed and $r \ra +\infty$, we obtain $\sigma(r) \ra -\infty$, so we can choose $r$ large enough such that $\sigma(r) \leq \sup_{\mR^n} g$. Hence, for this choice of $r$, we get
        \begin{equation*}
            \max_{\partial_p Q_T} v \leq \sup_{\mR^n}g
        \end{equation*}
        Hence
        \begin{equation*}
            \max_{\Bar{Q}_T} v \leq \sup_{\mR^n g}
        \end{equation*}
        In particular, we have
        \begin{equation*}
            v(y,t) \leq \sup_{\mR^n}g \quad \forall t \in [0,T]
        \end{equation*}
        This means 
        \begin{equation*}
            u(y,t) - \frac{\mu e^{\frac{|x-y|^2}{4(T+\epsilon -t)}}}{(T+\epsilon -t)^\frac{n}{2}} \leq \sup_{\mR^n} g \quad \forall t \in [0, T]
        \end{equation*}
        The inequality holds for any $\mu > 0$, so we can let $\mu \ra 0^+$ and we get 
        \begin{equation*}
            u(y,t) \leq \sup_{\mR^n} g \quad \forall t \in [0,T]
        \end{equation*}
        This holds for all $y \in \mR^n$, so we have
        \begin{equation*}
            u(y,t) \leq \sup_{\mR^n} g \quad \forall (y,t) \in \Bar{D}_T
        \end{equation*}
        Hence
        \begin{equation*}
            \sup_{\Bar{D}_T} u \leq \sup_{\mR^n} g
        \end{equation*}
        which gives the thesis. \\
        We have proved the thesis assuming $T < \frac{1}{4a}$. If we drop this hypothesis, we argue as follows:\\
        Set $T_1 = \frac{1}{8a}$. Applying the previous result to $D_{T_1}$, we get 
        \begin{equation*}
            \sup_{\Bar{D}_{T_1}} u \leq \sup_{\mR^n}g
        \end{equation*}
        and in partiuclar 
        \begin{equation*}
            \sup_{x \in \mR^n} u(x,t+T_1)
        \end{equation*}
        Hence $v$ satisfies
        \begin{equation*}
            \begin{cases}
                (\partial_t - \Delta) v &= 0 \quad \text{ in } D_{T_1}\\
                v(x,0) &= u(x,T_1)
            \end{cases}
        \end{equation*}
        Hence
        \begin{equation*}
            \sup_{\Bar{D}_{T_1}} v \leq \sup_{\mR^n} u(x,T_1) \leq \sup_{\mR^n} g
        \end{equation*}
        So, we have 
        \begin{equation*}
            u(x,t) \leq \sup_{\mR^n} g
        \end{equation*}
        and
        \begin{equation*}
            u(x,t+T_1) \leq \sup_{\mR^n} g
        \end{equation*}
        for $t \in [0,T_1]$, which means 
        \begin{equation*}
            u(x,t) \leq \sup_{\mR^n} g \quad \forall x \in \mR^n, \forall t \in [0,2T_1]
        \end{equation*}
        that is 
        \begin{equation*}
             \sup_{\Bar{D}_{2 T_1}} u \leq \sup_{\mR^n} g
        \end{equation*}
        Assume $n T_1 \leq T < (n+1) T_1, \exists n \geq 2$. \\
        Hence, we can repeat the previous argument for all intervals $[0,T_1], [T_1, 2T_1], \ldots, [(n-1) T_1, nT_1]$ and a last time for the interval $[nT_1,T]$, and we end with
        \begin{equation*}
            \sup_{\Bar{D}_T} u \leq \sup_{\mR^n} g
        \end{equation*}
    \end{proof}
\end{ProofBox}
\begin{PropBox}
    \begin{Cor}[Corollary 1]
        Let $T > 0, g \in \mathcal{C}^0(D_0) \cap L^\infty(D_0), u \in \mathcal{C}^{2,1}(D_T) \cap \mathcal{C}^0(\Bar{D}_T)$ satisfying the same equation of the theorem, and the estimate
        \begin{equation*}
            u(x,t) \geq - A e^{a |x|^2} \quad \forall (x,t) \in \Bar{D}_T
        \end{equation*}
        where $A,a > 0$. Then
        \begin{equation*}
            \inf_{\Bar{D}_T} u = \inf_{\mR^n} g
        \end{equation*}
    \end{Cor}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
        Set $w=-u$ and apply the theorem to $w$.
    \end{proof}
\end{ProofBox}
\begin{PropBox}
    \begin{Cor}[Corollary 2 - Uniqueness]
        Assume $T > 0, g \in \mathcal{C}^0(\mR^n) \cap L^\infty(\mR^n), f \in \mathcal{C}^0(\mR^n \times [0,T])$. Then, there is at most one solution of the problem
        \begin{equation*}
        (PP)
            \begin{cases}
                \partial_t u - \Delta u &= f \quad \text{ in } D_T \\
                u &= g \quad \text{ in } D_0
            \end{cases}
        \end{equation*}
        satisfying the growth estimate
        \begin{equation*}
            |u(x,t)| \leq A e^{a |x|^2} \quad (x,t) \in \Bar{D}_T
        \end{equation*}
        for $A,a > 0$. 
    \end{Cor}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
        If $u_1,u_2$ are two solutions of (PP) satisfying the growth estimate, then the function $ w = u_1 - u_2$ satisfies 
        \begin{equation*}
            \begin{cases}
                \partial_t u - \Delta w &= 0 \quad \text{ in } D_T \\
                w &= 0 \quad \text{ in } D_0
            \end{cases}
        \end{equation*}
        and the growth estimate
        \begin{equation*}
            |w(x,t)| \leq 2 A e^{a |x|^2} \quad \text{ in } \Bar{D}_T
        \end{equation*}
        Hence
        \begin{equation*}
            \sup_{\Bar{D}_T} w \leq 0
        \end{equation*}
        that is, $w \leq 0$ in $\Bar{D}_T$, so $u_1 \leq u_2$ in $\Bar{D}_T$. But the same argument can be repeated for the function $- w = u_2 - u_1$, and we get $u_2 \leq u_1$. Hence $u_1=u_2$ in $\Bar{D}_T$. 
    \end{proof}
\end{ProofBox}
\begin{remark}
    If we drop the growth hypothesis, the uniqueness result does not hold anymore. \\
    It hase been proben that the problem
    \begin{equation*}
        \begin{cases}
            (\partial_t - \Delta) u &= 0 \quad \text{ in } \mR^n \times (0,+\infty) \\
            u&= 0 \quad \text{ on } \mR^n \times \{0\}
        \end{cases}
    \end{equation*}
    has solutions different from $u(x,t) = 0$, which is obviously a solution. These solutions grow more than exponentially as $|x| \ra +\infty$. 
\end{remark}
\begin{PropBox}
    \begin{Cor}[Corollary 3 - Comparison and Stability]
        Let $T > 0, g_1, g_2 \in \mathcal{C}^0(D_0), f \in \mathcal{C}^0(D_T)$, and let $u_1,u_2 \in \mathcal{C}^{2,1}(D_T) \cap \mathcal{C}^0(\Bar{D}_T)$ satisfy the following 
        \begin{equation*}
            \begin{cases}
                (\partial_t - \Delta) u_1 &= f \quad \text{ in } D_T\\
                u_1\ &= g_1 \quad \text{ on } D_0
            \end{cases}
            \hspace{2 cm}
            \begin{cases}
                (\partial_t - \Delta) u_2 &= f \quad \text{ in } D_T\\
                u_2\ &= g_2 \quad \text{ on } D_0
            \end{cases}
        \end{equation*}
        Assume 
        \begin{equation*}
            |u_i(x,t)| \leq A e^{a |x|^2}, \forall (x,t) \in D_T, (A,a > 0).
        \end{equation*}
        Then
        \begin{enumerate}
            \item If $g_1 \geq g_2$ on $D_0$, then $u_1 \geq u_2$ in $D_T$.
            \item $\sup_{D_T} |u_1 - u_2| \leq \sup_{D_0} |g_1-g_2|$
        \end{enumerate}
    \end{Cor}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
        \begin{enumerate}
            \item Set $w = u_1-u_2$, then $w$ satisfies 
            \begin{equation*}
                \begin{cases}
                    (\partial_t - \Delta) w &= 0 \quad \text{ in } D_T\\
                w &= g_1 - g_2 \quad \text{ on } D_0
                \end{cases}
            \end{equation*}
            and $|w(t,x)| \leq 2Ae^{a |x|^2}$. Hence we have
            \begin{equation*}
                \inf_{D_T} w \geq \inf_{D_0}(g_1-g_2) \geq 0
            \end{equation*}
            that is $u_1 \geq u_2$ in $D_T$. 
            \item Set $w = u_1 - u_2$. Applying maximum principle, we get 
            \begin{equation*}
                \sup_{D_T}w \leq \sup_{D_0} (g_1 - g_2) \leq \sup_{D_0} |g_1-g_2|
            \end{equation*}
            Repeating the argument for $-w = u_2 - u_1$, we get 
            \begin{equation*}
                \sup_{D_T}(-w) \leq \sup_{D_0} |g_1-g_2|
            \end{equation*}
            Hence
            \begin{equation*}
                u_1(x,t) - u_2(x,t) \leq \sup_{D_0} |g_1-g_2|
            \end{equation*}
            and
            \begin{equation*}
                u_2(x,t) - u_1(x,t) \leq \sup_{D_0} |g_1-g_2| \quad \forall (x,t) \in D_T
            \end{equation*}
            Hence
            \begin{equation*}
                |u_1(x,t) - u_2(x,t)| \leq \sup_{D_0} |g_0-g_2| \quad \forall (x,t) \in D_T
            \end{equation*}
            that is 
            \begin{equation*}
                \sup_{D_T}|u_1-u_2| \leq \sup_{D_0} |g_0-g_2|
            \end{equation*}
        \end{enumerate}
    \end{proof}
\end{ProofBox}
\begin{remark}
    In the hypothesis $g \in \mathcal{C}^0(\mR^n), g$ bounded, we have proved that the function
    \begin{equation*}
    u(x,t) = 
        \begin{cases}
            &\frac{1}{(4 \pi t) ^ \frac{n}{2}} \int_{\mR^n} e^{-\frac{|x-y|^2}{4t}} g(y) dy \quad (t > 0)\\
            &g(x) \quad (t= 0)
        \end{cases}
    \end{equation*}
    satisfies $u \in \\{C}^{2,1}(D_T) \cap \mathcal{C}^0(\Bar{D}_T) \forall T > 0$, and satisfies 
    \begin{equation*}
    (IPP)
        \begin{cases}
            (\partial_t - \Delta) u &= 0 \quad \text{ in } \mR^n \times (0,+\infty)\\
            u&=g \quad \text{ on } \mR^n \times \{0\} 
        \end{cases}
    \end{equation*}
    We also have, if $|g(y)| \leq M \quad \forall y \in \mR^n$
    \begin{align*}
        |u(x,t)| &\leq \frac{1}{(4 \pi t)^\frac{n}{2}} \int_{\mR^n} e^{-\frac{|x-y|^2}{4t}} |g(y)| dy \\
        & \leq M \frac{1}{(4 \pi t)^\frac{n}{2}} \int_{\mR^n} e^{-\frac{|x-y|^2}{4t}} dy = M
    \end{align*}
    Hence, $u$ satisfies a growth hypothesis, and we can conclude that $u$ is the unique solution of (IPP) satisfying the growth hypothesis of the uniqueness result. 
\end{remark}
\begin{PropBox}
    \begin{Cor}[Corollary 4 - A Summary on Cauchy problem]
    Assume $g \in \mathcal{C}^0(\mR^n) \cap L^\infty(\mR^n)$, and consider the problem
    \begin{equation*}
    (CPP)
        \begin{cases}
            (\partial_t - \Delta) u&=0 \quad \text{in } \mR^n \times (0,+\infty)\\
            u&g \quad \text{ on } \mR^n \times \{0\}
        \end{cases}
    \end{equation*}
    We have the following result 
    \begin{enumerate}
        \item The function defined by 
        \begin{equation*}
        u(x,t) = 
            \begin{cases}
                \int_{\mR^n} \Gamma(x-y,t) g(y) dy & t > 0\\
                g(x) & t= 0
            \end{cases}
        \end{equation*}
        satisfies $v \in \mathcal{C}^{2,1}(\mR^n \times (0,+\infty)) \cap \mathcal{C}^0(\mR^n \times [0,+\infty))$ and solves (CPP). Also, $u \in L^\infty(\mR^n \times [0,+\infty))$.
        \item If $v$ is another solution of (CPP) satisfying $v \in \mathcal{C}^{2,1}(\mR^n \times (0,+\infty)) \cap \mathcal{C}^0(\mR^n \times [0,+\infty))$ and $|v(x,t)| \leq A e^{a |x|^2}$ in $\mR^n \times [0,+\infty) (A,a > 0)$. Then $u = v$ in $D_T \forall T > 0$. Hence $u=v$ in $\mR^n \times [0,+\infty)$. 
        \item If $g_1 \in \mathcal{C}^0(\mR^n) \cap L^\infty(\mR^n)$ and $u_1$ is the solution of (CPP) with initial datum $g_1$ and $|u_1(x,t)| \leq A e^{a |x|^2}$, then 
        \begin{equation*}
            \sup_{\mR^n \times (0,+\infty)} |u-u_1| \leq \sup_{\mR^n} |g-g_1|
        \end{equation*}
    \end{enumerate}
    \end{Cor}
\end{PropBox}
Notice that the last result can be obtained by the general stability theorem but also with direct calculations:
\begin{align*}
    |u(x,t) - u_1(x,t)| &= |\int_{\mR^n} \Gamma(x-y,t) g(y) dy - \int_{\mR^n} \Gamma(x-y,t)g_1(y) dy| \\
    &\leq \int_{\mR^n} \Gamma(x-y,t) |g(y)-g_1(y)| dy \\
    &\leq \sup_{\mR^n} |g-g_1| \int_{\mR^n} \Gamma(x-y,t) dy \\
    &= \sup_{\mR^n} |g-g_1|
\end{align*}



%%% Slide 5
\subsection{Laplace's Equation}
\begin{align}
    \label{LE1}
    \Delta u&=0 \hspace{2.5 cm} \text{Laplace}\\
    \label{LE2}
    -\Delta u&=f \hspace{2.5 cm} \text{Poisson}
\end{align}
+ some kind of boundary conditions. \\
$\mathcal{C}^2$-solution of \eqref{LE1} are called \emph{harmonic functions}.\\
\subsection{Fundamental Solution}
We start by looking for \emph{radial} harmonic functions in $ \mathbb{R}^n$. Set $u(x)=v(|x|)$, then, for $x \neq 0$, 
\begin{equation*}
    \begin{split}
        \frac{\partial u}{\partial x_i}&= v' (|x|)\frac{x_i}{|x|} \\
        \frac{\partial^2 u}{\partial x_i^2}&= v'' (|x|)\frac{x_i^2}{|x|^2} + v' (|x|)\frac{1}{|x|}-v' (|x|)\frac{x_i^2}{|x|^3}  
    \end{split}
\end{equation*}
hence 
\begin{equation*}
    \Delta u = v''(|x|) +(n-1) \frac{1}{|x|}v'(|x|)
\end{equation*}
Setting $r$ the variable for $v$, we have to solve the ODE
\begin{equation*}
    v''(r)+ \frac{n-1}{r}v'(r)=0
\end{equation*}
By usual separation techniques we find 
\begin{equation*}
    v'(r)= \frac{a}{r^{n-1}}\hspace{2 cm} \textbf{ for some constant } a \in \mathbb{R}
\end{equation*}
for $r>0$ we obtain 

\[
v(R) =\begin{cases}
    b \log r + c \hspace{1 cm} & (n=2)\\
    \frac{b}{r^{n-2}} + c \hspace{1 cm} & (n\geq 3)
\end{cases}
\]
where $b,c \in \mR$.\\
The fundamental solution of Laplace's equation is obtained by a particular choice of constants $b,c$.
\begin{DefBox}
    \begin{Def}
        The function
        \begin{equation*}
            \Phi (x) = \begin{cases}
                - \frac{1}{2\pi} \log|x|  \hspace{2 cm} & (n=2) \\
                 \frac{1}{n(n-2)W_n} \frac{1}{|x|^{n-2}}  \hspace{2 cm} & (n\geq 3) 
            \end{cases}
        \end{equation*}
        is the \textbf{fundamental solution of Laplace's equation}.
    \end{Def}
\end{DefBox}
\begin{itemize}
    \item $W_n$ is the Lebesgue measure of the unit ball in $\mR^n$.
    \item $\Phi$ is defined in $\mR^n \setminus \{0\}$, and $\mathcal{C}^\infty$ there.
    \item It is easy to see that there are constants $c > 0$ such that
    \begin{equation*}
        |D_i \Phi(x)| \leq \frac{c}{|x|^{n-1}}, |D_{ij} \Phi(x)| |leq \frac{c}{|x|^n} \quad \forall x \neq 0, \quad \forall i,j = 1,\ldots,n.
    \end{equation*}
    As in the case of the heat equation, the fundamental solution gives a way to build solutions of Poisson's equation \eqref{LE2}.
    \item notice: $\phi$, $D_i\Phi \in L^1_{loc}(\mR^n)$.
\end{itemize}

\begin{ThBox}
    \begin{Th}[Solutions of Poisson's Equation]
Assume $f \in C_0^2(\mathbb{R}^n)$. Define
\[
u (x) = \int_{\mathbb{R}^n} \varphi(x-y)f(y) \, dy.
\]
Then:
\begin{itemize}
    \item[(i)] $u \in \mathcal{C}^2(\mR^n)$
    \item[(ii)] $-\Delta u = f$ in $\mathbb{R}^n$.
\end{itemize}
\end{Th}
\end{ThBox}
%%venti pagine di dimostrazione ahahhaha 


%Baddy ha saltato slide 8. 



\begin{ThBox}
    \begin{Th}
        Assume $f \in \mathcal{C}^2 _0(\mR^n)$ and $u(x)= \int_{\mR^n }\Phi (x-y)f(y) dy$. Then
        \begin{equation*}
        \lim_{|x| \ra +\infty} u(x) = 0
        \end{equation*}
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        
    \end{proof}
\end{ProofBox}
We summarize what we have obtained in the following statement.
\begin{PropBox}
    \begin{Cor}
        Assume $f \in \mathcal{C}^2_0(\mR^n)$. Then, the function
        \begin{equation*}
            u(x)=\int_{\mR^n} \Phi(x-y)f(y) dy 
        \end{equation*}
        satisfies $u \in \mathcal{C}^2(\mR^n)$ and solves the following problem
        \begin{equation*}
            \begin{cases}
                -\Delta u &= f \quad \text{ in } \mR^n\\
                \lim_{|x| \ra +\infty} u(x) &= 0
            \end{cases}
        \end{equation*}
    \end{Cor}
\end{PropBox}
\subsection{The Point of View of Distribution Theory}
\begin{equation*}
    \varphi (x)= \frac{1}{n(n-2) W_n}\frac{1}{|x|^{n-2}} \hspace{1 cm} \in L_{loc}^1(\mR^n),
\end{equation*}
hence we can define the distribution with $\varphi$:
\begin{equation*}
    T_{\Phi}(h)= \int_{\mR^n} \Phi (y) h(y) \, dy 
\end{equation*}
Hence we know that $\Delta T_\phi$ is defined as a distribution
\begin{equation*}
    \Delta T_\phi(h) = T_\phi (\Delta h) = \int_{\mR^n} \phi(y) \Delta h(y) dy
\end{equation*}
As $\phi$ is radial, by the change of variables $z = -y$ we get
\begin{align*}
    \int_{\mR^n} \Phi(y) \Delta h(y)dy &= \int_{\mR^n} \Phi(-z) \Delta h(-z) dz \\
    &= \int_{\mR^n} \Phi(z) \Delta h(-z) dz \\
    &= \Delta T_\phi(h)
\end{align*}
But we have proved that 
\begin{equation*}
    \int_{\mR^n} \Phi(y) \Delta h(x-y)dy = -h(x) \quad \forall x \in \mR^n
\end{equation*}
Hence
\begin{equation*}
    \int_{\mR^n} \Phi(z) \Delta h(-z)dz = \int_{\mR^n} \Phi(z) \Delta h(0-z)dz = -h(0) = - \delta (h)
\end{equation*}
So that 
\begin{equation*}
    \Delta T_\phi(h) = -\delta(h)
\end{equation*}
or 
\begin{equation*}
    -\Delta T_\phi(h) = \delta(h) \quad \forall h \in \mathcal{C}^\infty_0 (\mR^n)
\end{equation*}
Hence
\begin{equation*}
    -\Delta T_\phi = \delta
\end{equation*}
and $T_\phi$ (or $\phi$) is the fundamental solution of $-\Delta$ in the sens of distribution theory. \\
Hence, from distribution theory, we know that $\T_\phi \* h$ satisfies 
\begin{equation*}
    -\Delta (T_\phi \* h) = h \quad \forall h \in \mathcal{C}^\infty_0 (\mR^n)
\end{equation*}
By definition, we have that 
\begin{equation*}
    T_\phi \* h(x) = T_\phi(h(x-\cdot)) = \int_{\mR^n} \Phi(y) h(x-y) dy
\end{equation*}
which is the solution we have obtained in the main theorem (for $h \in \mathcal{C}^\infty_0(\mR^n)$). 
\subsection{Mean value formulas}
Harmonic functions satisfy the important mean value formulas, which in turn generate remarkable consequences. 
\begin{remark}[Notations]
    \begin{itemize}
        \item if $A,B \subseteq \mR^n$ are two open sets, $A$ bounded, by $A \subset \subset B$ we mean $\Bar{A} \subseteq B$
        \item if $B = B(x,r)$ is a ball and $u: \Bar{B} \ra \mR$ a function such that the integrals 
        \begin{equation*}
            \int_B u(x) dx 
        \end{equation*}
        and 
        \begin{equation*}
            \int_{\delta B} u(x) d\sigma_x 
        \end{equation*}
        are well defined, then
        \begin{equation*}
            \cancel{\int}_B u(x) dx = \frac{1}{|B|} \int_B u(x) dx = \frac{1}{r^n w_n} \int_B u(x) dx 
        \end{equation*}
        \begin{equation*}
            \cancel{\int}_{\delta B} u(x) d\sigma_x = \frac{1}{|\delta B|} \int_{\delta B} u(x) d\sigma_x = \frac{1}{nr^{n-1} w_n} \int_{\delta B} u(x) d\sigma_x 
        \end{equation*}
    \end{itemize}
\end{remark}
\begin{ThBox}
    \begin{Th}[Mean-Value Formulas]
        Let $\Omega \subseteq \mR^n$ be an open set, $u \in \mathcal{C}^2(\Omega)$ harmonic in $\Omega$. Pick $x \in \Omega$ and $r > 0$ such that $B(x,r) \subset \subset \Omega$. Then
        \begin{equation*}
            u(x)= \cancel{\int}_{\delta B(x,r)} u(y) d\sigma_y = \cancel{\int}_{B(x,r)} u(y) dy
        \end{equation*}
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        
    \end{proof}
\end{ProofBox}
\begin{ThBox}
    \begin{Th}[Converse to Mean-Value Property]
        Assume that $u \in \mathcal{C}^2(\Omega)$ and satisfies 
        \begin{equation*}
            u(x) = \cancel{\int}_{\delta B(x,r)} u(y) d\sigma_y
        \end{equation*}
        for each $x \in \Omega, r > 0$ such that $B(x,r) \subset \subset \Omega$. Then, $u$ is harmonic. 
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        
    \end{proof}
\end{ProofBox}
\begin{ThBox}
    \begin{Th}[Strong Maximum Principle]
        Assume $\Omega$ is an open, bounded, connected subset of $\mR^n$. Assume $u \in \mathcal{C}^2(\Omega) \cap \mathcal{C}^0(\Bar{\Omega})$ and $\Delta u = 0$ in $\Omega$. Let $M = \max_{\Bar{\Omega}} u$. Assume there is $x_0 \in \Omega$ such that $u(x_0) = M$. Then, $u(x) = M \quad \forall x \in \Omega$. 
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        
    \end{proof}
\end{ProofBox}
Replacing $u$ with $-u$, it is easy to prove that the same statement holds with "min" replacing "max", as follows:
\begin{PropBox}
    \begin{Cor}[Strong Maximum Principle]
        Assume $\Omega$ is an open, bounded, connected subset of $\mR^n$. Assume $u \in \mathcal{C}^2(\Omega) \cap \mathcal{C}^0(\Bar{\Omega})$ and $\Delta u = 0$ in $\Omega$. Let $M = \min_{\Bar{\Omega}} u$. Assume there is $x_0 \in \Omega$ such that $u(x_0) = M$. Then, $u(x) = M \quad \forall x \in \Omega$. 
    \end{Cor}
\end{PropBox}
\begin{ProofBox}
    \begin{proof}
        Apply the Strong Maximum Principle to $-u$. 
    \end{proof}
\end{ProofBox}
\begin{remark}
    Strong Maximum/Minimum Principle implies that 
    \begin{equation*}
        \max_{\Bar{\Omega}} u = \max_{\delta \Omega} u, \quad \min_{\Bar{\Omega}} u = \min_{\delta \Omega} u
    \end{equation*}
    These statements are called "Weak Maximum/Minimum Principles". \\
    All these principles imply, roughly speaking, that if we control the values of $u$ on $\delta \Omega$, then we control its values in all $\Omega$. 
\end{remark}
\begin{ThBox}
    \begin{Th}
        Assume $\Omega$ is an open, bounded, connected subset of $\mR^n$. Assume $f \in \mathcal{C}(\Omega), g \in \mathcal{C}(\delta \Omega)$ and let $u_1, u_2 \in \mathcal{C}^2(\Omega) \cap \mathcal{C}^0(\Bar{\Omega})$ be solutions of 
        \begin{equation*}
            \begin{cases}
                - \Delta u &= f \quad \text{ in } \Omega\\
                 u &= g \quad \text{ on } \delta \Omega
            \end{cases}
        \end{equation*}
        Then, $u_1 = u_2$ in $\Bar{\Omega}$. 
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        
    \end{proof}
\end{ProofBox}
\subsection{Mollifier, Convolution, Smoothing}
\begin{DefBox}
    \begin{Def}
        \begin{itemize}
            \item If $x \in \mR^n$ and $A \subseteq \mR^n, A \neq \emptyset$, we define
            \begin{equation*}
                dist(x,A) = \inf\{|x-y| \mid y \in A\}
            \end{equation*}
            \item If $\Omega \subseteq \mR^n, \Omega \neq \mR^n$ is an open set and $\epsilon > 0$, we define 
            \begin{equation*}
                \Omega_\epsilon = \{x \in \Omega \mid dist(x,\delta \Omega) > \epsilon\}
            \end{equation*}
            \item If $\Omega = \mR^n$, then $\Omega_\epsilon = \Omega = \mR^n$. 
            \item We define $\phi \in \mathcal{C}^\infty(\mR)$ by 
            \begin{equation*}
                \phi(z) = e^{\frac{1}{z-1}} \quad \text{ if } z < 1, \quad \phi(z) = 0 \quad \text{ if } z \geq 1
            \end{equation*}
            \item We define $\eta \in \mathcal{C}^\infty_0(\mR^n)$ by
            \begin{equation*}
                \eta(x) = c \phi(|x|^2)
            \end{equation*}
            where $c > 0$ is such that 
            \begin{equation*}
                \int_{\mR^n} \eta(x) dx = 1
            \end{equation*}
            \item $\forall \epsilon > 0$, we define $\eta_\epsilon \in \mathcal{C}^\infty_0(\mR^n)$ by 
            \begin{equation*}
                \eta_\epsilon(x) = \frac{1}{\epsilon^n} \eta(\frac{x}{\epsilon})
            \end{equation*}
            The function $\eta_\epsilon$ satisfy
            \begin{equation*}
                \int_{\mR^n} \eta_\epsilon(x) = 1, \quad supp \eta_\epsilon \subseteq B(0,\epsilon)
            \end{equation*}
            \item If $\Omega \subseteq \mR^n$ is an open set and $f \in \mathcal{C}^0(\Omega)$, we define, for $\epsilon > 0$ and $x \in \Omega_\epsilon$:
            \begin{align*}
                f^\epsilon(x) &= \eta_\epsilon \* f(x) \\
                &= \int_\Omega \eta_\epsilon(x-y) f(y) dy \\
                &= \int_{B(0,\epsilon)} \eta_\epsilon(y) f(x-y) dy
            \end{align*}
        \end{itemize}
    \end{Def}
\end{DefBox}
\begin{ThBox}
    \begin{Th}
        Assume $f, \Omega, \epsilon, f^\epsilon$ as in the previous definition. Then
        \begin{itemize}
            \item [i)] $ f^ \epsilon \in \mathcal{C}^\infty(\Omega_\epsilon)$
            \item [ii)] $f^\epsilon(x) \ra f(x)$ as $\epsilon \ra 0^+, \quad \forall x \in \Omega$
            \item [iii)] $f^ \epsilon \ra f$ uniformly on compact subsets of $\Omega$. 
        \end{itemize}
    \end{Th}
\end{ThBox}
\begin{ThBox}
    \begin{Th}[Smoothness]
        Let $\Omega \subseteq \mR^n$ be open, $u \in \mathcal{C}^0(\Omega)$ and assume that $u$ satisfies the mean-value property for each ball $B(x,r) \subset \subset \Omega$. Then $u \in \mathcal{C}^\infty(\Omega)$. 
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        
    \end{proof}
\end{ProofBox}
\begin{PropBox}
    \begin{Cor}
        Let $\Omega \subseteq \mR^n$ be open, $u \in \mathcal{C}^0(\Omega)$. The following two statements are equivalent:
        \begin{itemize}
            \item [i)] $u$ satisfies mean-value property in $\Omega$.
            \item [ii)] $u \in \mathcal{C}^\infty(\Omega)$ and $\Delta u= 0$. Furthermore, (i) and (ii) imply
            \item [iii)] $u \in \mathcal{C}^\infty(\Omega)$
            \item [iv)] All the derivatives of $u$ are harmonic. 
        \end{itemize}
    \end{Cor}
\end{PropBox}
\begin{ThBox}
    \begin{Lemma}[Estimates on Derivatives]
        Assume $\Omega \subseteq \mR^n$ open, $u$ harmonic in $\Omega$. Then, there is a constant $c > 0$ such that, for all balls $B(x_0,r) \subset \subset \Omega$, it holds
        \begin{equation*}
            |D_iu(x_0)| \leq \frac{c}{r^{n+1}}\int_{B(x_0,r)}|u| dx \quad (i=1,\ldots,n)
        \end{equation*}
    \end{Lemma}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        
    \end{proof}
\end{ProofBox}
\begin{remark}
    It is possible to prove that similar estimates hodl for \emph{all} derivatives of an harmonic functions. From this, one proves that an harmonic function is not only $\mathcal{C}^\infty$ but even analytical. \\
    We will not deal with such results in these lectures. The interested student can look at the book of Evans. 
\end{remark}
\begin{ThBox}
    \begin{Th}[Liouville]
        Suppose $u: \mR^n \ra \mR$ is harmonic and bounded. Then, $u$ is constant. 
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        
    \end{proof}
\end{ProofBox}
\begin{ThBox}
    \begin{Th}[Representation Formula]
        Let $f \in \mathcal{C}^2_0(\mR^n), n \geq 3$. Define 
        \begin{equation*}
            u(x)=\int_{\mR^n} \Phi(x-y) f(y) dy
        \end{equation*}
        Assume that $v \in \mathcal{C}^2(\mR^n)$ is a bounded solution of $- \Delta v = f$ in $\mR^n$. Then, $\exists c \in \mR $ such that 
        \begin{equation*}
            v(x) = u(x) + c \quad \forall x \in \mR^n
        \end{equation*}
    \end{Th}
\end{ThBox}
\begin{ProofBox}
    \begin{proof}
        
    \end{proof}
\end{ProofBox}
\begin{PropBox}
    \begin{Cor}
        If $f \in \mathcal{C}^2_0(\mR^n)$ and $n \geq 3$, the function 
        \begin{equation*}
            u(x) = \int_{\mR^n} \Phi(x-y) f(y) dy
        \end{equation*}
        is the unique $\mathcal{C}^2$ solution of the problem
        \begin{equation*}
            \begin{cases}
                -\Delta u & f \quad \text{ in } \mR^n\\
                u(x) \ra 0 & \quad \text{ as } |x| \ra +\infty
            \end{cases}
        \end{equation*}
    \end{Cor}
\end{PropBox}