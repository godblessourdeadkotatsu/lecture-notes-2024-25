% !TeX spellcheck = en_US
\documentclass[12pt]{report}
\usepackage{paccopsde}
	\makeindex
\begin{document}
	\title{Partial Stochastic Differential Equations}
	\author{Kotatsu}
	\date{\small TOALDO VAFFANCULOOOOOOOOOO}
	\maketitle
	\pagenumbering{Roman}
	\begin{preface}
		These are the notes of the Stochastic Processes course for the Academic Year 2025-2026 with Professors Toaldo and Badiale.\par
		I took these notes personally and I integrated the (many) unclear parts and passages using online resources and occasionally the fucking shitty books that this course has. \par
		\vskip1.2cm
		
		\hfill Kotatsu
		\vskip1.2cm
		Check the source code for this and other notes in my GitHub repo $\to$ \href{https://github.com/godblessourdeadkotatsu/lecture-notes-2024-25/tree/main/PSDE}{\faGithubSquare}
	\end{preface}
	\clearpage
	\tableofcontents
	\pagenumbering{arabic}
\chapter{The Itô integral}	
\section{Introduction}
Imagine we have a random phenomenon whose realization is 
\begin{equation*}
	\seqtm{X}.
\end{equation*}
Imagine the infinitesimal increment 
\begin{equation*}
	\seqtdt{X}.
\end{equation*}
This is proportional to the time integral
\begin{equation*}
	\seqtdt{X}=X_{t}=b\dt
\end{equation*}
which is just a deterministic quantity proportional to the time increment $\dt$. Now add a source of randomness like a noise \rv{} $Z$:
\begin{equation*}
	b\dt+\sigma Z.
\end{equation*}
When we model something random we usually have a deterministic to which we add some noise. We can see $Z$ as the total contribution of $N$ (small) sources of randomness:
\begin{equation*}
	Z=\sum_{i=1}^{N}\frac{X_{i}}{N}.
\end{equation*}
If the $X_{i}$ are i.i.d. then $Z$ is gaussian with $Z\distnorm{0,1}$. We can imagine that $Z$ increases with time (the more time it passes, the more the ``noise'' can influence the process) so we can think of $Z$ as 
\begin{equation*}
	Z\distnorm{0,\dt}.
\end{equation*}
We now have 
\begin{equation*}
	\dif X_{t}=b\dt+\sigma Z
\end{equation*}
which is already a stochastic differential equation (SDE) in the way we usually write it. We can think of $Z$ as the increment of a \bwm{}:
\begin{equation*}
	Z=B_{t+\dt}-B_{t}\implies \dif X_{t}=b\dt+\sigma\dbt.
\end{equation*}
We could imagine that the variation of $X$ and the effect of the noise are dependent on the current position of the process $X_{t}$:
\begin{equation*}
	\dxt=b(X_{t})\dt+\sigma(X_{t})\dbt.
\end{equation*}
In this scenario, to know the future evolution of the process we just have to know the position of the process in the current time: this is indeed the differential equation of a \emph{Markovian process}.
We have to remember that 
\begin{equation*}
	\dxt=b(X_{t})\dt+\sigma(X_{t})\dbt
\end{equation*}
is just a symbol. If this was ``literal'' then it would mean
\begin{equation*}
	\frac{\dif}{\dt}X_{t}=b(X_{t})+\sigma(X_{t})\frac{\dif}{\dt}B_{t}
\end{equation*}
but this \emph{doesn't make any fucking sense} because we know that the \bwm{} is not differentiable in any point. To get around this a common trick is to write the integral equation
\begin{equation*}
	X_{t}-X_{0}=\ubracketthin{\int_{0}^{t}b(X_{s})\ds}_{\claptext{Normal Riemann integral}}+\int_{0}^{t}\sigma(X_{s})\dif B_{s}.
\end{equation*}
The $\int_{0}^{t}\sigma(X_{s})\dif B_{s}$ part could look like a normal Riemann - Stieltjes integral (the ones of the kind $\int_{A}f(x)\dif g(x)$ where we ``weigh'' each piece of a function with another) but the problem is that we cannot use a single trajectory of a \bwm{} for this. \bwm{} has infinite variation and therefore the sums could diverge. The tools we have for analysis are not general enough for the class of functions we are trying to study. To understand the object
\begin{equation*}
	\int_{0}^{t}B_{s}\dbs
\end{equation*}
we need a new notion of integral. Once we know what this is then our SDE is just a normal differential equation system, whose solution turns out to be a process called \emph{Itô diffusion}. We will see that these diffusions help us solve PDSE in general. All \ito{} diffusions can be written as
\begin{equation*}
	\frac{\partial}{\partial t}q(x,t)=G_{x}q(x,t)\qquad q(x,0)=u(x)
\end{equation*}
where $G_{x}$ is a linear operator and where 
\begin{equation*}
	\evs^{x}u(X_{t}) 
\end{equation*}
is the solution. This is a huge class of functions called \textit{parabolic equations} and they allow us to use probabilistic methods also for general analytic equations.
\section{Some facts about \bwm{} construction}
We already know how to construct a trajectory (that is a probability space) that satisfies all the conditions for a \bwm{} trajectory by computing the increments. It is a probability space $(\Omega,\A,\pr)$ of the kind
\begin{equation*}
	\left(\mathcal{C}_{0}(I),\B,P_{\omega}\right)
\end{equation*}
with $I=[0,\infty)$. Which \sa{} can we construct on a function space and which probability measure $P_{\omega}$ is such that the collection
\begin{equation*}
	B(\omega)=\seqttm{B_{t}(\omega)}
\end{equation*}
is a \bwm? There are a lot of ways to construct a \bwm{} but just one can be extended to Markov processes and this is the \emph{canonical construction of a \bwm}. \par
Let $I=[0,\infty)$: for each $t\in I$ let $(E_{t},\E_{t})$ be a measurable space. The collection ${\left(X_{t}\right)}$ can be viewed as a function $I\ni t\mapsto X_{t}\in E_{t}$. In most case we simply have $(E_{t},\E_{t})=(\R,\B(\R))$. The product space 
\begin{equation*}
	F=\bigtimes_{t\in I}E_{t}
\end{equation*}
Is the set of such $X$; that is, the set of all real valued functions from $I$ to $E_{t}$. The correspondent \sa{} is 
\begin{equation*}
	\bigotimes_{t\in I}\Sigma_{t}=\sigma(\text{measurable rectangle}).
\end{equation*}
\begin{definition}
	A rectangle in $F$ is a subset
	\begin{equation*}
		\left\{X\in F:X_{t}\in A_{t}\qquad\every t\in I\right\}
	\end{equation*}
	and this is the subset of functions $A_{t}$ that differs from $E_{t}$ only in a finite number of positions $t\in I$.
\end{definition}
A cylinder set is obtained by restricting finitely many coordinates to measurable subset $A_{t}$
and leaving all other coordinates unrestricted.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{img/screenshot001}
	\caption{All the functions that are in area 1 at time 1 and in area 2 at time 2 are included in the rectangle}
	\label{fig:screenshot001}
\end{figure}
A random process is a sequence of \rv s but it can also be seen as a one \rv{} measurable on the product space. If $I=[0,\infty)$ then the product space is $\R^{(0,\infty)}$. So we now need to explicitly build a process that is a random process.
\begin{theorem}
	For $t_{1},\ldots,t_{n}\geq0$ with $n\in\N$ and $t_{j}\neq t_{i}$ we assume that 
	\begin{equation*}
		P_{t_{1},\ldots,t_{n}}(\cdot)
	\end{equation*}
	is a family of probability measures for any choice of $n$ and it is on $\left(\R^{n},\B(\R^{n})\right)$.
The Kolmogorov consistency conditions tell us that this family has a random process having this as a distribution. The conditions are:
\begin{enumerate}
	\item $P_{t_{1},\ldots,t_{n}}(A_{1}\times\ldots\times A_{n})=P_{t_{\sigma(1)},\ldots,t_{\sigma(n)}}(A_{\sigma(1)},\ldots,A_{\sigma(n)})$, which means that the distribution is invariant to permutations;
	\item $P_{t_{1},\ldots,t_{n}}(A_{1},\ldots,A_{n-1}\times \R)=P_{t_{1},\ldots,t_{n-1}}(A_{1}\times\ldots\times A_{n-1})$ (integrating the marginal).
\end{enumerate}
If we can specify our family such that $t_{1}<t_{2}<\ldots<t_{n}$ then there exists a unique probability measure $\mu$ on the product space $(\R^{I},\B^{I}(\R))$ such that the canonical process
\begin{equation*}
	X_{t}(\omega)=\omega,\qquad X=\seqtm{X}
\end{equation*}
has finite dimensional distribution $P_{t_{1},\ldots,t_{n}}$. 
\end{theorem}
Now take $t_{1}<t_{1}<\ldots<t_{n}$ and 
\begin{equation*}
	T_{t_{1},\ldots,t_{n}}\distnorm{0,\mathbf{C}}
\end{equation*}
with covariance matrix
\begin{equation*}
	\mathbf{C}_{ij}=t_{i}\wedge t_{j}.
\end{equation*}
This is just the finite dimensional \bwm{} distribution. The idea behind all of this thing is that once we choose the canonical process, finding a probability measure $P$ on $(\Omega,\B^{I}(\R))$ automatically determines the finite-dimensional distribution of this process. Vice versa, if we start from a family of finite-dimensional distribution that satisfy the Kolmogorov extension theorem conditions, then we can define a measure $P$ such that the canonical process has those particular distributions.
\begin{corollary}
	If $\seqtm{X}$ is a process then there exists a unique measure $\mu$ on the product space such that the canonical process satisfies $X_{0}=0$, then $X_{t}$ has stationary independent and Gaussian increments.
\end{corollary}
Product \sa{} is generated by measurable rectangles, typically by union or intersection; but that is just the combination of a countable number of information and we need to understand what to do when the information is uncountable. We care about this because to create a probability measure on our event space we need some kind of \sa{} and the product \sa{} is the \emph{smallest \sa{}} that makes all projection of the canonical process measurable.
\begin{theorem}
	For every $\Gamma\subset \B^{I}(\R)$ there exists an \textit{at most} countable set $S\subset I$ such that 
	\begin{equation*}
		v\in\Gamma,w\in\R^{I}\;\text{and}\;v|_{S}=w|_{S}\implies w\in\Gamma.
	\end{equation*}
\end{theorem}
This tells us that the product \sa{} is too small: for any set in the \sa{} (countably generated) we can find a set of indices which depends on $\Gamma$ and it is countable. If we pick a function  $v$ in $\Gamma$ and we choose another function $w$ in $\B^{I}(\R)$ coinciding with $v$ on $S$ then it must belong to $\Gamma$ as well... but $w$ may be not continuous! This means that the set of continuous functions $C_{0}(I)$ is \underline{not} in $\B^{I}(\R)$. This is an issue because it implies that the set $C_{0}(I)$ is not measurable with respect to the product \sa{} so we cannot compute a probability measure on it.
\begin{equation*}
	\mathcal{C}_{0}([0,\infty))\notin\B^{[0,\infty)}.
\end{equation*}
Define the following metric between functions:
\begin{equation*}
	\rho(v,w):=\sum_{n\geq1}2^{-n}\min\left\{\sup_{0\leq t\leq n}|v(t)-w(t)|,1\right\}
\end{equation*}
so that if $\rho(v_{n},v)\to0$ then $v_{n}\to v$. This also gives us uniformity on a compact set. This metric space is separable, which means that there exists a dense numerable subset in this space. Here $\B(\mathcal{C}_{0}(I))$ is the smallest that contains
\begin{equation*}
	B_{\rho}(\mu,r):=\left\{v\in\mathcal{C}_{0}(I):\rho(\mu,v)< r\right\}
\end{equation*}
which is the open Borel ball.
\begin{lemma}
	We have
	\begin{equation*}
		\B^{I}(\R)\cap \mathcal{C}_{0}(I)=\B(\mathcal{C}_{0}(I)).
	\end{equation*}
\end{lemma}
This means that the Borel \sa{} on $\mathcal{C}_{0}(I)$ induced by $\rho$ is equal to the trace \sa{} obtained by restricting the product \sa{} $\B^{I}(\R)$ to the set $\mathcal{C}_{0}(I)$.
\begin{definition}
	If $(E,\E)$ is a measurable space and $F\subset E$ then the trace \sa{} on $F$ is
	\begin{equation*}
		\Sigma_{F}:=\left\{A\cap F:A\in\E\right\}.
	\end{equation*}
\end{definition}
\begin{theorem}
	Let $X=\seqtm{X}$ be a $d$-dimensional process on $(\Omega,\A,\pr)$. If there are constants $c,\alpha,\beta$ all $>0$ such that
	\begin{equation*}
		\ev{\left|X_{t}-X_{s}\right|^{\alpha}}\leq c|t-s|^{1+\beta}
	\end{equation*}
	then $X_{t}$ has a version with \emph{only continuous paths}.
\end{theorem}
\begin{revise}
	The version of a \rv{} is a \rv{} $\Xbar$ such that
	\begin{equation*}
		\pr(X_{t}=\Xbar_{t})=1\qquad\every t
	\end{equation*}
\end{revise}
In particular, for \bwm{} we have that
\begin{equation*}
	\ev{\left|X_{t}-X_{s}\right|^{4}}=\ev{\left|X_{t-s}\right|^{4}}=\ubracketthin{|t-s|^{2}}_{\sigma^{4}}\ubracketthin{\ev{\left|X_{t}\right|^{4}}}_{3=c}
\end{equation*}
so there exists a version with continuous paths. Let's denote it as
\begin{equation*}
	\Xbar=\seqtm{\Xbar}\qquad\Xbar(\omega)\in\mathcal{C}_{0}(I).
\end{equation*}
Now take a measure $\mu$ and define
\begin{equation*}
	P(A):=\mu\left(\left\{\omega\in\R^{I}:\Xbar(\omega)\in A\right\}\right).
\end{equation*}
This is the image measure of $\mu$ under $\Xbar$:
\begin{equation*}
	P=\mu\circ\Xbar^{-1}.
\end{equation*}
By Kolmogorov extension theorem we get
\begin{equation*}
	\left(\R^{I},\B^{I}(\R),\mu\right)\xrightarrow{\Xbar}\left(\mathcal{C}_{0}(I),\B(C_{0}(I)),P\right)
\end{equation*}
where $P$ is the image measure of $\mu$ called \emph{Wiener measure}. So by construction $\left(\mathcal{C}_{0}(I),\B(C_{0}(I)),P\right)$ is a probability space where the canonical process is \bwm{} with continuous paths.
\section{More facts about \bwm{} trajectories}
Take $f:[0,\infty]\mapsto\R^{d}$. Ideally, $f(t)=B(t,\omega)$. We want to measure the oscillations off the interval $[a,b]$. Set 
\begin{equation*}
	a=t_{0}<t_{1}<\ldots<t_{n}=b
\end{equation*}
and call $[a,b]=\Pi$.
\begin{definition}
	Let $f:[0,\infty)\to\R^{d}$ and let $\seqn{\Pi}$ with $n\in\N$ be a sequence of a partition of $[0,t]$ with mesh $\left|\Pi_{n}\right|\to0$, Then 
	\begin{equation*}
		\vari_{p}(f,t):=\lim_{n\to\infty}S^{\Pi_{n}}_{p}(f,t)
	\end{equation*}
	is called \emph{$p$-variation of $f$ along $\Pi$}.
\end{definition}
 We denote the following quantity
\begin{equation*}
	\sum_{t_{j}\leq\Pi}\left|f(t_{j})-ft_{j-1})\right|^{P}=S^{\Pi}_{p}(f,[a,b])
\end{equation*}
as the \emph{$p$-variation sum} of $f$ on $[a,b]$ along the partition path.
\begin{definition}
	Let $f:[0,\infty)\to\R^{d}$ and $\Pi:=\left\{
	0=t_{0}<t_{1}<\ldots<t_{n}=t\right\}$ with mesh $|\Pi|:=\max_{j}|t_{j}-t_{j-1}|$. for $p>0$ we call
	\begin{align*}
		S_{p}^{\Pi}(f,t)&=\sum_{t_{j}\leq\Pi}\left|f(t_{j})-f(t_{j-1})\right|^{p}\\
		&=\sum_{j=1}^{n}\left|f(t_{j})-f(t_{j-1})\right|^{p}
	\end{align*}
	the \emph{$p$-variation sum of f along $\Pi$} and we call
	\begin{equation*}
		\vari_{p}(f,t)=\sup\left\{S^{\Pi}_{p}(f,t):\Pi\text{ is a partition of }[0,t]\right\}
	\end{equation*}
	the \emph{strong $p$-variation}.
\end{definition}
The difference with the weak $p$-variation is that the strong $p$-variation is for any partition and not just along a sequence. We have that
\begin{equation*}
	S_{2}^{\Pi_{n}}(B,t)\xrightarrow{L^{2}}t
\end{equation*}
and there exists a subsequence $\seqkk{\Pi_{n_{k}}}$ of partitions such that
\begin{equation*}
	S^{\Pi_{n_{k}}}_{2}(B,t)\convas t
\end{equation*}
and there exists a sequence $\seqn{\Pi}$ such that
\begin{equation*}
	S^{\Pi_{n}}:S_{2}^{\Pi_{n}}(B,t)\convas t \qquad\text{if }\sum_{n}|\Pi_{n}|\leq\infty.
\end{equation*}
\begin{proposition}
	For any $t>0$ and $p\leq2$ we have
	\begin{equation*}
		\vari_{p}(B,t)=+\infty.
	\end{equation*}
\end{proposition}
\begin{fancyproof}
	We prove for $p<2$. Choose $p=2-\delta$ for $\delta>0$ and let $\seqn{\Pi}$ be a sequence of partitions such that $|\Pi_{n}|\to0$. Then, if we suppose that $\vari_{p}(B,t)<\infty$, we expect $\sum_{t_{j}\in\Pi_{n}}\left(B_{t_{j}}-B_{t_{j-1}}^{2}\right)\leq\vari_{p}(B,t)<\infty$. We can write
	\begin{align*}
		\sum_{t_{j}\in\Pi_{n}}\left(B_{t_{j}}-B_{t_{j-1}}^{2}\right)&=\sum_{t_{j}\in\Pi_{n}}\left(B_{t_{j}}-B_{t_{j-1}}\right)^{\delta}\left(B_{t_{j}}-B_{t_{j-1}}\right)^{2-\delta}\\
		&\leq\max_{t_{j}\in\Pi_{n}}\left|B_{t_{j}}-B_{t_{j-1}}\right|^{\delta}\sum_{t_{j}\in\Pi_{n}}\left|B_{t_{j}}-B_{t_{j-1}}\right|^{2-\delta}\\
		&\leq\max_{t_{j}\in\Pi_{0}}\ubracketthin{\left|B_{t_{j}}-B_{t_{j-1}}\right|^{\delta}}_{\to0}\vari_{p}(B,t)=0
	\end{align*}
	but this is a contradiction because we know that 
	\begin{equation*}
		\sum_{t_{j}\in\Pi_{n}}\left|B_{t_{j}}-B_{t_{j-1}}\right|\xrightarrow{|\Pi|\to0}=t
	\end{equation*}
	that is, the quadratic variation goes to $t$ and not 0!
\end{fancyproof}
\begin{theorem}
	Let $\seqtm{B}$ be a 1-dimensional \bwm. If $\seqn{\Pi}$ is a sequence of partitions on $[0,t]$ with $\left|\Pi_{n}\right|\to0$ then the quadratic variation is such that $$\vari_{2}(B,t)=L^{2}-\lim_{n}S_{n}^{\Pi_{n}}(B,t).$$
\end{theorem}
\begin{fancyproof}
	We actually mean to prove 
	\begin{equation*}
		\ev{\left(S^{\Pi}_{2}(B,t)-t\right)^{2}}\xrightarrow{|\Pi|\to0}0.
	\end{equation*}
	Set $\Pi=\left\{0=t_{0}<t_{1}<\ldots<t_{n}\right\}$. Since $\ev{S_{2}^{\Pi}(B,t)}=t$, we have
	\begin{align*}
		\ev{\left(S^{\Pi}_{2}(B,t)-t\right)^{2}}&=\ev{\left(S^{\Pi}_{2}(B,t)-\ev{S_{2}^{\Pi}(B,t)}\right)^{2}}\\
		&=\var\left(S_{2}^{\Pi}(B,t)\right).
	\end{align*}
	We have
	\begin{align*}
		\ev{S^{\Pi}_{2}(B,t)}&=\sum_{j=1}^{n}\ev{\left(B_{t_{j}}-B_{t_{j-1}}\right)^{2}}\\
		&=\sum_{j=1}^{n}(t_{j}-t_{j-1})=t.
	\end{align*}
	Moreover,
	\begin{align*}
		\ubracketthin{\var\left(\sum_{j=1}^{n}(B_{t_{j}}-B_{t_{j-1}})^{2}\right)}_{\var\left(S_{2}^{\Pi}(B,t)\right)}&=\sum_{j=1}^{n}\ev{\left((B_{t_{j}}-B_{t_{j-1}})^{2}-(t_{j}-t_{j-1})\right)^{2}}\\
		&\text{by indep. increments}\\
		&=\sum_{j=1}^{n}\ev{\left(B^{2}_{t_{j}-t_{j-1}}-(t_{j}-t_{j-1})\right)^{2}}\\
		&=\sum_{j=1}^{n}(t_{j}-t_{j-1})^{2}\ubracketthin{\ev{\left(B_{t}^{2}-1\right)}}_{c}\\
		&\text{by scaling property}\\
		&=c\sum_{j=1}^{n}\ubracketthin{(t_{j}-t_{j-1})}_{\leq|\Pi|}(t_{j}-t_{j-1})\\
		&\leq c|\Pi|t\xrightarrow{|\Pi|\to0}0
	\end{align*}
\end{fancyproof}
\end{document} 


%THIS IS THE DARK AGE OF LOVE   $\cdot$